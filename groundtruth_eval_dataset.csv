question,contexts,ground_truth,evolution_type,metadata,episode_done
What challenges did the team encounter with obtaining a clean height map in environments with low ceilings?,"['3.3\nIssues\nAlthough our planner performed very well during the Finals, it still encountered some issues related to other\nparts of the navigation stack.\n3.3.1\nHeight Map Spikes\nAs mentioned in Section 2.4.3, obtaining a clean height map in environments with low ceilings was challenging,\nand we tuned our ceiling point ﬁlter to also work with inclines and stairs, which exacerbated the issue. This\nslowed our progress quite a bit in the cave section, where the ceiling was especially low. Unfortunately, we\nreached these sections with our explorer robots, which recorded many ceiling points very close to the robot,\ndue to their dome lidar conﬁguration (see Figure 2(a)+(b)).\nAlthough this slowed our progress, we never got stuck. We demonstrate this behavior with a narrow cave\nopening, which is immediately followed by an incline, shown in Figure 12. Even when the robot was less than\na meter from the opening (Figure 12\n), the height map showed a straight wall. Our planner could therefore\nonly plan right up to the halucinated wall, which then shifts forward slightly (Figure 12\n) and allows the\nrobot to plan a bit farther. In Figure 12\n, the slope aligns with the distance-dependent height threshold\nof our ceiling point ﬁlter and we can plan forward farther. Once we are fully on the incline (Figure 12\n),\nthe negative slope at the rear of the robot causes the fake wall to reappear right behind it. Nonetheless, the\nrobot was able to plan up the slope and back down fully autonomous.\n3.3.2\nPath Follower\nPlanners rely on their paths being tracked accurately. The path follower is therefore an important component\nof the navigation stack. Our pure-pursuit path follower generally performed well, tracking paths precisely\nin narrow spaces and moving swiftly in more open spaces. However, during data analysis for this work, we\ndiscovered that there was a non-constant delay between the planner publishing a path, and the path follower\nstarting to track it, which we observed to be up to 500 ms. The top right of Figure 13 shows an example of\nthe diﬀerence in robot position between the plan being published and being followed. This, in combination\n3\n5\n4\n1\n6\n2\n0s\nOnboard Image\nPath at time:\nPath Following Started\n1s\n3s\n5s\n25s\n30s\nTime Series\nComputed\nFigure 13: A 500 ms time delay between computing a path and starting to follow it lead to imperfect path following.\nAs a result, one robot brieﬂy got stuck on a narrow scaﬀolding pole in the urban section.\nThe robot initially\nplanned to pass the pole on the right but\nreplanned to use the shorter path on the left.\nSince the robot already\nmoved, the next path went right again which got the robot stuck on the pole.\nThe pole got removed from the map\nas it was now too close to be perceived and, consequently, paths through the pole were planned until\nthe robot\ndrifted to the left and\nﬁnally got unstuck.\nwith height map issues and the unsmooth nature of sampling-based planning, caused one explorer to get']",Obtaining a clean height map in environments with low ceilings was challenging.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does the Ceiling Point Filter address the issue of low ceilings in underground spaces during SubT?,"['ure 6\n. It is implemented using image erosion on the foothold score layer of the height map, which reduces\nthe steppable map region by a safety margin. This has the additional beneﬁt of also removing small isolated\nsteppable patches from the map, which could only be overcome by solving a stepping-stone problem. To\navoid unnecessarily inﬂating small obstacles like rails, which the robot can easily step over, we do not inﬂate\nunsteppable regions below a certain size. In practice this is done by performing an image dilation of smaller\nradius, before doing the erosion.\nThis safety threshold was crucial on the Subway Station of the Finals circuit, as shown in Section 3.2.2.\n2.4.3\nCeiling Point Filter\nWe use a 2.5D height map representation for planning. While this is suﬃcient for ground robot navigation\nin most environments, it can be problematic in the tight underground spaces encountered during SubT. Low\nceilings mean that they are frequently observed by the depth sensors, which causes spikes in the height map,\nas shown in Figure 2(b). However, we cannot simply discard all points above a ﬁxed height, since this would\neither prevent us from planning up slopes or from passing underneath low overhangs.\nWe therefore use a rising height threshold to ﬁlter points (Miki et al., 2022b), shown in Figure 6\n. It\nﬁlters points just above robot height close to the robot, and linearly increases the height threshold up to a\nmaximum at larger distances. This setup caused map spikes in parts of the course with low ceilings which\nslowed us down, but these crucially never stopped us from exploring. It allowed us to pass underneath very\nlow overhangs, and to plan up slopes, even when encountered together, as detailed in Section 3.3.1.\n3\nExperimental Results\nArtPlanner was deployed on all four ANYmal-C ground robots of team CERBERUS during all runs of the\nSubT Finals. It used a height map of size 8 m×8 m with a 4 cm resolution. We only cover results related\nto the navigation planner presented in this work. For further details on the general performance we refer to\nour overview article (Tranzatto et al., 2022a).\nWe deployed all four ground robots during the Prize Run, which were directed by the supervisor to explore\ndiﬀerent areas of the course. All ground robots successfully made it to the end of the competition and\nwe did not observe a single path planning or locomotion failure, which could have been provoked by bad\npath planning. Our planner was active for 90 minutes between all robots which accounts for 88.94% of all\nrobot motion. We gracefully navigated the narrow doorways and small rooms in the Urban section, passed\nthrough the Tunnel section with obscuring fog, and made it through the narrowest and roughest part of the\nCave section. The only case where ArtPlanner did not follow the exploration path over traversable terrain\nhappened at the stairs leading to the subway station. This resulted from our operational decision to use']",The Ceiling Point Filter addresses the issue of low ceilings in underground spaces during SubT by using a rising height threshold to filter points. It filters points just above robot height close to the robot and linearly increases the height threshold up to a maximum at larger distances. This allows the robot to pass underneath low overhangs and plan up slopes.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
"What is the focus of the research in the article ""A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges""?","['Hudson, N., et al. (2022). A survey on terrain traversability analysis for autonomous ground vehicles:\nMethods, sensors, and challenges. Field Robotics, 2(1):1567–1627.\nBradley, D. M., Chang, J. K., Silver, D., Powers, M., Herman, H., Rander, P., and Stentz, A. (2015). Scene\nunderstanding for a high-mobility walking robot. In IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 1144–1151. IEEE.\nChao, C., Hongbiao, Z., Howie, C., and Ji, Z. (2021). Tare: A hierarchical framework for eﬃciently exploring\ncomplex 3d environments.\nIn Robotics: Science and Systems XVII, Virtual. Robotics: Science and\nSystems Foundation.\nChavez-Garcia, R. O., Guzzi, J., Gambardella, L. M., and Giusti, A. (2017). Image classiﬁcation for ground\ntraversability estimation in robotics. In Int. Conf. on Advanced Concepts for Intelligent Vision Systems,\npages 325–336. Springer.\nDang, T., Mascarich, F., Khattak, S., Papachristos, C., and Alexis, K. (2019). Graph-based path planning\nfor autonomous robotic exploration in subterranean environments. In 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages 3105–3112. IEEE.\nFan, D. D., Otsu, K., Kubo, Y., Dixit, A., Burdick, J., and Agha-Mohammadi, A.-A. (2021). Step: Stochastic\ntraversability evaluation and planning for risk-aware oﬀ-road navigation. Robotics: Science and Systems.\nFernbach, P., Tonneau, S., Del Prete, A., and Ta¨\nıx, M. (2017). A kinodynamic steering-method for legged\nmulti-contact locomotion. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Sys-\ntems (IROS), pages 3701–3707. IEEE.\nFrey, J., Hoeller, D., Khattak, S., and Hutter, M. (2022). Locomotion policy guided traversability learning\nusing volumetric representations of complex environments. arXiv preprint arXiv:2203.15854.\nGuzzi, J., Chavez-Garcia, R. O., Nava, M., Gambardella, L. M., and Giusti, A. (2020). Path planning with\nlocal motion estimations. IEEE Robotics and Automation Letters, 5(2):2586–2593.\nHarper, M. Y., Nicholson, J. V., Collins, E. G., Pusey, J., and Clark, J. E. (2019). Energy eﬃcient navigation\nfor running legged robots. In 2019 International Conference on Robotics and Automation (ICRA), pages\n6770–6776. IEEE.\nHauser, K. (2015).\nLazy collision checking in asymptotically-optimal motion planning.\nIn 2015 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 2951–2957. IEEE.\nHines, T., Stepanas, K., Talbot, F., Sa, I., Lewis, J., Hernandez, E., Kottege, N., and Hudson, N. (2021).\nVirtual surfaces and attitude aware planning and behaviours for negative obstacle navigation. IEEE\nRobotics and Automation Letters, 6(2):4048–4055.\nHirose, N., Sadeghian, A., V´\nazquez, M., Goebel, P., and Savarese, S. (2018). Gonet: A semi-supervised\ndeep learning approach for traversability estimation. In 2018 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 3044–3051. IEEE.']","The focus of the research in the article ""A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges"" is terrain traversability analysis for autonomous ground vehicles.",simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does reachability checking prevent the planner from planning over risky areas?,"['even though the actual terrain was too rough and rocky to overcome. Using just reachability checking, our\nsampler found individual valid poses on the slope and connected them, as shown in Figure 9(a). Here, risk\npruning identiﬁed that moving on this slope was too risky and removed these edges, as shown in Figure 9(b).\nWithout risk pruning the robot would have tried to scale this slope, which would in the best case have lead\nto lost time, and in the worst case to loss of this robot.\nFigure 7\n+\nshow that the cost function generally lead to safer paths, which kept a safe distance from\n(a) Without risk pruning\n(b) With risk pruning\nFigure 9: Sampled valid poses shown in red with blue graph edges connecting them. Using reachability checking,\nvalid robot poses are still found on a steep and rocky incline in the cave section. Pruning graph edges based on\nmotion risk prevents the planning graph from spanning this risky area.\n(a) Onboard Image\n(b) Exploration Path\n(c) No Safety Threshold\n(d) With Safety Threshold\nEdge\nEdge\nEdge\nEdge\nFigure 10: (a) The SubT Station platform had a sharp edge with a signiﬁcant drop. (b) GBPlanner2 was tuned to be\noptimistic and planned over the edge of the SubT Station platform. (c) Without a foothold safety margin the robot\nwould have stepped onto and possibly over the platform edge. (d) With foothold safety margin the ﬁnal path pose is\na safe distance from the platform edge. Yellow cubes represent our lidar map to indicate the actual location of the\nedge. The ANYmal model in (c)+(d) does not indicate the current robot pose, but rather is placed at the ﬁnal pose\nof the path to better show how close the robot would have stepped to the edge.\nobstacles. This can be attributed to the risk term cr in the cost function. The time cost ct had negligible\nimpact compared to the shortest path of the No Motion Cost planner, since the terrain present during the\nFinals was uniform enough such that the shortest path generally was also the fastest. However, ct was\nnecessary to condition the planning problem, since a pure risk cost would have lead to large detours to\nachieve minor risk improvements.\n3.2.2\nSafety Threshold Analysis\nThe safety threshold was introduced to handle negative obstacles. One carrier robot reached the Subway\nStation in autonomous exploration mode during the ﬁrst Preliminary Run of the Finals. The Subway Station\nhad a sharp drop with a wall a few meters behind, pictured in Figure 10(a). As discussed in Section 1.5, the\nexploration planner was tuned to be optimistic, and planned to explore into the free space above the train\ntracks (Figure 10(b)). With the wall visible behind the platform, both image inpainting and virtual surfaces\nwould have simply created ﬂat ground or a gentle slope such that we could not rely on motion cost for safety.\nReachability checking generally prevents the planner from planning over the edge, however, without a safety']",Reachability checking prevents the planner from planning over risky areas by identifying valid robot poses on the slope and connecting them. It ensures that the planner does not plan paths that involve moving on risky terrain.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
What are some common approaches used in navigation planning for mobile robots?,"['map while providing a limit on the sampling duration. We then validate all graph edges at once by applying\na locomotion risk threshold, leveraging massively parallel execution of a cost prediction network on GPU.\nThis allows for consistent and fast planning times. The resulting paths are collision-free and can be followed\nsafely by the used locomotion controller.\nWhen robotic algorithms are deployed on hardware they have to deal with the imperfections of the real\nworld and interact with other, possibly ﬂawed parts of a robot software and hardware stack. It is therefore\nbeneﬁcial to apply certain heuristics and make tweaks to part of the stack. These “small tricks” are often\nglossed over but can be essential to making a system robust in practice. For us, this was the case when\ncomputing the height map, which has the biggest impact on the outcome of planning decisions. We detail\nthree important components of our height map processing pipeline and show why each of them played an\nimportant role during the DARPA Subterranean Challenge (SubT) Finals.\nWe extensively evaluated ArtPlanner during SubT and present results gathered on four legged robots during\nthe Finals, for ArtPlanner and GBPlanner2, the planner used for exploration (Kulkarni et al., 2022). We\nprovide detailed analysis of the challenges faced by ArtPlanner, and how it managed to overcome these\nadverse conditions.\nIn addition, we compare our method to other state-of-the-art planners on the data\ngathered during the Finals and show why other methods would not have been robust enough for SubT.\nFinally, we will open-source the code of our method upon acceptance of this work.\n1.2\nRelated Work\nNavigation planning for mobile robots is a vast ﬁeld of research with a manifold of diﬀerent approaches.\nMost navigation approaches for mobile robots use a geometric environment representation as their basis for\nplanning (Wermelinger et al., 2016; Kr¨\nusi et al., 2017; Chavez-Garcia et al., 2017; Oleynikova et al., 2017).\nThey use various diﬀerent terrain representations for planning, most commonly 2.5D height maps (Wer-\nmelinger et al., 2016; Chavez-Garcia et al., 2017), point clouds (Kr¨\nusi et al., 2017) or truncated signed-\ndistance ﬁeld (TSDF)s (Oleynikova et al., 2017). Because planning in full 3D representations is currently\ncomputationally prohibited (Tonneau et al., 2018), we chose to work with 2.5D height maps as the en-\nvironment representation. Most planning approaches compute a single geometric traversability value per\nterrain patch (Wermelinger et al., 2016; Kr¨\nusi et al., 2017) as measure for how easily the terrain can be\ntraversed, irrespective of robot orientation. Thereby, they neglect the much higher mobility which legged\nrobots provide due to their ability to step over obstacles. An overview of diﬀerent traversability analysis\napproaches can be found in a recent survey article (Borges et al., 2022). While we have argued in previous']","Most navigation approaches for mobile robots use a geometric environment representation as their basis for planning. They use various different terrain representations for planning, most commonly 2.5D height maps, point clouds, or truncated signed-distance fields. Planning approaches compute a single geometric traversability value per terrain patch as a measure for how easily the terrain can be traversed, irrespective of robot orientation. An overview of different traversability analysis approaches can be found in a recent survey article.",simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does the ArtPlanner navigation planner ensure safe paths in unknown environments?,"['and the EU Horizon 2020 research and innovation programme under grant agreements No 780883, No 852044 and No 101016970.\nIt has been conducted as part of ANYmal Research, a community to advance legged robotics\n1http://github.com/leggedrobotics/art_planner\narXiv:2303.01420v1  [cs.RO]  2 Mar 2023\nFigure 1: Team CERBERUS won the DARPA Subterranean Challenge Finals with four ANYmal quadrupeds deployed\nduring the Prize Run. The navigation planner presented in this work guided all four robots safely during the hour-long\nmission.\nis enforced for the reachability volumes, to ensure that the robot is able to make environment-contact with\nits legs. While this approach can ﬁnd all theoretically feasible poses, basic reachability does not account\nfor dynamics of the robot and in practice most locomotion controllers do not use the full range of motion.\nThis issue can somewhat be alleviated by restricting geometry which is considered safe for contacts, and\nheuristics on the range of motion used by a speciﬁc locomotion controller. This mostly produces feasible\npaths, but the cost of walking over challenging terrain and the risk of stepping close to obstacles and edges\nis not considered (Wellhausen and Hutter, 2021).\nIn this work, we consider the path planning problem for legged robots on a robo-centric height map, which\nis of ﬁxed size, centered around the robot.\nSince we are interested in a navigation planner which can\nwork in possibly unknown environments, it will use information from an onboard mapping pipeline which\nis continuously updated as the robot moves. We therefore require a fast update rate for our planner to\nkeep up with map updates. Building a planning graph incrementally by maintaining the graph between\nplanning queries can speed up planning in static environments where most map updates will not invalidate\nthe planning graph. However, this approach necessitates graph maintenance between map updates, which\nadds heuristics and algorithmic complexity to identify updated graph elements. The extreme challenges\nposed by the DARPA Subterranean Challenge (SubT), like smoke, and dynamic obstacles, necessitate a\nmore robust solution.\n1.1\nContributions\nIn this work, we therefore introduce ArtPlanner, a navigation planner for legged robots which uses geometric\nreachability checking to ﬁnd valid poses and a learned motion cost to ﬁnd optimal paths which are safe and\npractically feasible. This combination is enabled by a novel graph construction method: It creates a new\nplanning graph every time the map is updated. To keep computation time low, we lazily sample candidate\npose vertices until we either reach a limit of number of poses, number of edges in the graph, or sampling\ntime. Because we plan on a robo-centric map of limited size, this allows us to densely sample the planning\nmap while providing a limit on the sampling duration. We then validate all graph edges at once by applying']",The ArtPlanner navigation planner ensures safe paths in unknown environments by using geometric reachability checking to find valid poses and a learned motion cost to find optimal paths that are safe and practically feasible. It creates a new planning graph every time the map is updated and lazily samples candidate pose vertices. All graph edges are validated at once by applying a reachability volume enforcement to ensure that the robot can make environment-contact with its legs.,reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
"How is the height map representation used in planning, particularly in relation to image erosion and dilation?","['ure 6\n. It is implemented using image erosion on the foothold score layer of the height map, which reduces\nthe steppable map region by a safety margin. This has the additional beneﬁt of also removing small isolated\nsteppable patches from the map, which could only be overcome by solving a stepping-stone problem. To\navoid unnecessarily inﬂating small obstacles like rails, which the robot can easily step over, we do not inﬂate\nunsteppable regions below a certain size. In practice this is done by performing an image dilation of smaller\nradius, before doing the erosion.\nThis safety threshold was crucial on the Subway Station of the Finals circuit, as shown in Section 3.2.2.\n2.4.3\nCeiling Point Filter\nWe use a 2.5D height map representation for planning. While this is suﬃcient for ground robot navigation\nin most environments, it can be problematic in the tight underground spaces encountered during SubT. Low\nceilings mean that they are frequently observed by the depth sensors, which causes spikes in the height map,\nas shown in Figure 2(b). However, we cannot simply discard all points above a ﬁxed height, since this would\neither prevent us from planning up slopes or from passing underneath low overhangs.\nWe therefore use a rising height threshold to ﬁlter points (Miki et al., 2022b), shown in Figure 6\n. It\nﬁlters points just above robot height close to the robot, and linearly increases the height threshold up to a\nmaximum at larger distances. This setup caused map spikes in parts of the course with low ceilings which\nslowed us down, but these crucially never stopped us from exploring. It allowed us to pass underneath very\nlow overhangs, and to plan up slopes, even when encountered together, as detailed in Section 3.3.1.\n3\nExperimental Results\nArtPlanner was deployed on all four ANYmal-C ground robots of team CERBERUS during all runs of the\nSubT Finals. It used a height map of size 8 m×8 m with a 4 cm resolution. We only cover results related\nto the navigation planner presented in this work. For further details on the general performance we refer to\nour overview article (Tranzatto et al., 2022a).\nWe deployed all four ground robots during the Prize Run, which were directed by the supervisor to explore\ndiﬀerent areas of the course. All ground robots successfully made it to the end of the competition and\nwe did not observe a single path planning or locomotion failure, which could have been provoked by bad\npath planning. Our planner was active for 90 minutes between all robots which accounts for 88.94% of all\nrobot motion. We gracefully navigated the narrow doorways and small rooms in the Urban section, passed\nthrough the Tunnel section with obscuring fog, and made it through the narrowest and roughest part of the\nCave section. The only case where ArtPlanner did not follow the exploration path over traversable terrain\nhappened at the stairs leading to the subway station. This resulted from our operational decision to use']","The height map representation is used in planning by implementing image erosion on the foothold score layer of the height map. This reduces the steppable map region by a safety margin and removes small isolated steppable patches. Additionally, image dilation of smaller radius is performed to avoid inflating small obstacles like rails. This process helps in planning navigation in tight underground spaces encountered during SubT.",reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
What challenges did the ceiling point filter face in low ceiling environments and how did it impact the robot's progress?,"['3.3\nIssues\nAlthough our planner performed very well during the Finals, it still encountered some issues related to other\nparts of the navigation stack.\n3.3.1\nHeight Map Spikes\nAs mentioned in Section 2.4.3, obtaining a clean height map in environments with low ceilings was challenging,\nand we tuned our ceiling point ﬁlter to also work with inclines and stairs, which exacerbated the issue. This\nslowed our progress quite a bit in the cave section, where the ceiling was especially low. Unfortunately, we\nreached these sections with our explorer robots, which recorded many ceiling points very close to the robot,\ndue to their dome lidar conﬁguration (see Figure 2(a)+(b)).\nAlthough this slowed our progress, we never got stuck. We demonstrate this behavior with a narrow cave\nopening, which is immediately followed by an incline, shown in Figure 12. Even when the robot was less than\na meter from the opening (Figure 12\n), the height map showed a straight wall. Our planner could therefore\nonly plan right up to the halucinated wall, which then shifts forward slightly (Figure 12\n) and allows the\nrobot to plan a bit farther. In Figure 12\n, the slope aligns with the distance-dependent height threshold\nof our ceiling point ﬁlter and we can plan forward farther. Once we are fully on the incline (Figure 12\n),\nthe negative slope at the rear of the robot causes the fake wall to reappear right behind it. Nonetheless, the\nrobot was able to plan up the slope and back down fully autonomous.\n3.3.2\nPath Follower\nPlanners rely on their paths being tracked accurately. The path follower is therefore an important component\nof the navigation stack. Our pure-pursuit path follower generally performed well, tracking paths precisely\nin narrow spaces and moving swiftly in more open spaces. However, during data analysis for this work, we\ndiscovered that there was a non-constant delay between the planner publishing a path, and the path follower\nstarting to track it, which we observed to be up to 500 ms. The top right of Figure 13 shows an example of\nthe diﬀerence in robot position between the plan being published and being followed. This, in combination\n3\n5\n4\n1\n6\n2\n0s\nOnboard Image\nPath at time:\nPath Following Started\n1s\n3s\n5s\n25s\n30s\nTime Series\nComputed\nFigure 13: A 500 ms time delay between computing a path and starting to follow it lead to imperfect path following.\nAs a result, one robot brieﬂy got stuck on a narrow scaﬀolding pole in the urban section.\nThe robot initially\nplanned to pass the pole on the right but\nreplanned to use the shorter path on the left.\nSince the robot already\nmoved, the next path went right again which got the robot stuck on the pole.\nThe pole got removed from the map\nas it was now too close to be perceived and, consequently, paths through the pole were planned until\nthe robot\ndrifted to the left and\nﬁnally got unstuck.\nwith height map issues and the unsmooth nature of sampling-based planning, caused one explorer to get']","Obtaining a clean height map in environments with low ceilings was challenging for the ceiling point filter. The filter was tuned to work with inclines and stairs, which exacerbated the issue. This slowed the robot's progress in the cave section, where the ceiling was especially low. However, the robot never got stuck and was able to navigate through narrow cave openings and inclines.",multi_context,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
"What methods are used for outdoor robot navigation, considering deep learning for traversability estimation and terrain analysis for autonomous ground vehicles?","['deep learning approach for traversability estimation. In 2018 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 3044–3051. IEEE.\nHutter, M., Gehring, C., Jud, D., Lauber, A., Bellicoso, C. D., Tsounis, V., Hwangbo, J., Bodie, K.,\nFankhauser, P., Bloesch, M., et al. (2016). Anymal-a highly mobile and dynamic quadrupedal robot. In\nIROS, pages 38–44. IEEE.\nKim, D., Sun, J., Oh, S. M., Rehg, J. M., and Bobick, A. F. (2006). Traversability classiﬁcation using\nunsupervised on-line visual learning for outdoor robot navigation. In IEEE International Conference\non Robotics and Automation (ICRA), pages 518–525. IEEE.\nKr¨\nusi, P., Furgale, P., Bosse, M., and Siegwart, R. (2017).\nDriving on point clouds: Motion planning,\ntrajectory optimization, and terrain assessment in generic nonplanar environments. Journal of Field\nRobotics, 34(5):940–984.\nKulkarni, M., Dharmadhikari, M., Tranzatto, M., Zimmermann, S., Reijgwart, V., De Petris, P., Nguyen, H.,\nKhedekar, N., Papachristos, C., Ott, L., Siegwart, R., Hutter, M., and Alexis, K. (2022). Autonomous\nteamed exploration of subterranean environments using legged and aerial robots. In IEEE International\nConference on Robotics and Automation (ICRA).\nLin, Y.-C. and Berenson, D. (2017). Humanoid navigation in uneven terrain using learned estimates of\ntraversability. In 2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids),\npages 9–16. IEEE.\nMiki, T., Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., and Hutter, M. (2022a).\nLearning robust\nperceptive locomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822.\nMiki, T., Wellhausen, L., Grandia, R., Jenelten, F., Homberger, T., and Hutter, M. (2022b). Elevation\nmapping for locomotion and navigation using gpu. arXiv preprint arXiv:2204.12876.\nNorby, J. and Johnson, A. M. (2020). Fast global motion planning for dynamic legged robots. 2020 IEEE\nInternational Conference on Intelligent Robots and Systems (IROS).\nOhradzansky, M. T., Rush, E. R., Riley, D. G., Mills, A. B., Ahmad, S., McGuire, S., Biggie, H., Harlow,\nK., Miles, M. J., Frew, E. W., Heckman, C., and Humbert, J. S. (2021).\nMulti-agent autonomy:\nAdvancements and challenges in subterranean exploration. arXiv preprint arXiv:2110.04390.\nOleynikova, H., Taylor, Z., Fehr, M., Siegwart, R., and Nieto, J. (2017). Voxblox: Incremental 3d euclidean\nsigned distance ﬁelds for on-board mav planning. In IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 1366–1373. IEEE.\nOtsu, K., Ono, M., Fuchs, T. J., Baldwin, I., and Kubota, T. (2016). Autonomous terrain classiﬁcation with\nco-and self-training approach. IEEE Robotics and Automation Letters, 1(2):814–819.\nReid, W., Fitch, R., G¨\nokto˘\ngan, A. H., and Sukkarieh, S. (2020). Sampling-based hierarchical motion plan-\nning for a reconﬁgurable wheel-on-leg planetary analogue exploration rover. Journal of Field Robotics,\n37(5):786–811.'
 'Hudson, N., et al. (2022). A survey on terrain traversability analysis for autonomous ground vehicles:\nMethods, sensors, and challenges. Field Robotics, 2(1):1567–1627.\nBradley, D. M., Chang, J. K., Silver, D., Powers, M., Herman, H., Rander, P., and Stentz, A. (2015). Scene\nunderstanding for a high-mobility walking robot. In IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 1144–1151. IEEE.\nChao, C., Hongbiao, Z., Howie, C., and Ji, Z. (2021). Tare: A hierarchical framework for eﬃciently exploring\ncomplex 3d environments.\nIn Robotics: Science and Systems XVII, Virtual. Robotics: Science and\nSystems Foundation.\nChavez-Garcia, R. O., Guzzi, J., Gambardella, L. M., and Giusti, A. (2017). Image classiﬁcation for ground\ntraversability estimation in robotics. In Int. Conf. on Advanced Concepts for Intelligent Vision Systems,\npages 325–336. Springer.\nDang, T., Mascarich, F., Khattak, S., Papachristos, C., and Alexis, K. (2019). Graph-based path planning\nfor autonomous robotic exploration in subterranean environments. In 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages 3105–3112. IEEE.\nFan, D. D., Otsu, K., Kubo, Y., Dixit, A., Burdick, J., and Agha-Mohammadi, A.-A. (2021). Step: Stochastic\ntraversability evaluation and planning for risk-aware oﬀ-road navigation. Robotics: Science and Systems.\nFernbach, P., Tonneau, S., Del Prete, A., and Ta¨\nıx, M. (2017). A kinodynamic steering-method for legged\nmulti-contact locomotion. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Sys-\ntems (IROS), pages 3701–3707. IEEE.\nFrey, J., Hoeller, D., Khattak, S., and Hutter, M. (2022). Locomotion policy guided traversability learning\nusing volumetric representations of complex environments. arXiv preprint arXiv:2203.15854.\nGuzzi, J., Chavez-Garcia, R. O., Nava, M., Gambardella, L. M., and Giusti, A. (2020). Path planning with\nlocal motion estimations. IEEE Robotics and Automation Letters, 5(2):2586–2593.\nHarper, M. Y., Nicholson, J. V., Collins, E. G., Pusey, J., and Clark, J. E. (2019). Energy eﬃcient navigation\nfor running legged robots. In 2019 International Conference on Robotics and Automation (ICRA), pages\n6770–6776. IEEE.\nHauser, K. (2015).\nLazy collision checking in asymptotically-optimal motion planning.\nIn 2015 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 2951–2957. IEEE.\nHines, T., Stepanas, K., Talbot, F., Sa, I., Lewis, J., Hernandez, E., Kottege, N., and Hudson, N. (2021).\nVirtual surfaces and attitude aware planning and behaviours for negative obstacle navigation. IEEE\nRobotics and Automation Letters, 6(2):4048–4055.\nHirose, N., Sadeghian, A., V´\nazquez, M., Goebel, P., and Savarese, S. (2018). Gonet: A semi-supervised\ndeep learning approach for traversability estimation. In 2018 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 3044–3051. IEEE.']","The methods used for outdoor robot navigation, considering deep learning for traversability estimation and terrain analysis for autonomous ground vehicles, include unsupervised on-line visual learning, terrain classification with co-and self-training approach, image classification for ground traversability estimation, and stochastic traversability evaluation and planning for risk-aware off-road navigation.",multi_context,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}
 {'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How can a modular design approach be implemented in robotics to enhance versatility and adaptability?,"['making it challenging to assess the feasibility of a task in\na limited scenario. Therefore, a valuable research direction\nis to explore methods for transferring model training across\ndifferent scenarios while maintaining their accuracy in the\noriginal training environments.\n6.3. Unify Format of Modal\nCurrently, many models are utilizing LLM as the robot’s\nbrain, and text-type data is typically the input that LLM\naccepts. However, for agents reliant on multi-modal per-\nception, efficiently handling diverse input formats poses a\nsignificant challenge. To address this issue, a VLA model\nhas been proposed [9], which uniformly converts visual\nand natural language multi-modal inputs into multi-modal\nsentences for processing, and outputs actions in the same for-\nmat. In other words, multi-modal statements are employed to\nharmonize input and output. Nevertheless, there is currently\nno unified processing for other modalities such as touch and\nsmell. It is anticipated that unified multi-modal models like\nVLA will gain popularity in the future.\n6.4. Modular Components\nAs previously discussed, the field of robotics currently\nlacks a unified approach to robot design, with varying opin-\nions on the matter. We believe that there should be a mod-\nular design method, wherein each part of the robot can be\nswapped out like a machine, just like in Figure 4(c), allowing\nfor greater versatility and adaptability8. To achieve this,\nwe must first establish unified specifications for the various\nmodules of the robot. For instance, a robot can be composed\nof a head, torso, upper limbs, and lower limbs, with the upper\nlimbs and lower limbs being interchangeable based on the\ntask at hand. Among them, the upper limbs and lower limbs\ncan be replaced according to specific tasks. When we need\nto cook, we can use our upper limbs as a shovel, and when\n8https://www.agibot.com\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 14 of 19\nLarge Language Models for Robotics: A Survey\nwe need to deal with weeds in the yard, we can use our lower\nlimbs as a weeder.\n6.5. Autonomous Perception\nOur current research focuses on developing robots that\ncan interact with humans using natural language instruc-\ntions. In many cases, we study how humans issue instruc-\ntions and how robots can decompose abstract tasks into\nspecific sub-tasks for execution [1]. However, we also hope\nthat robots can perceive and respond autonomously to handle\nour current needs. For instance, if our cup falls to the ground\nand breaks, an agent should be able to perceive the situation\nthrough hearing and vision, and then autonomously handle\nthe glass fragments for us. Autonomous perception requires\nthe robot to have common sense, which is a capability that\ncan be integrated into robots based on LLM as the brain.\nResearch on robots’ autonomous perception capabilities is\ncrucial for improving our quality of life in the future.\n7. Conclusions\nIn this survey, we summarized the methods and tech-'
 'Research on robots’ autonomous perception capabilities is\ncrucial for improving our quality of life in the future.\n7. Conclusions\nIn this survey, we summarized the methods and tech-\nnologies currently used for large models in robots. First, we\nreviewed some basic concepts of large language models and\ncommon large models. We explain what improvements will\nbe brought to robots by using large models as brains. We\nalso introduce the representative LLM-based robot models\nproposed in recent years, such as LM-Nav [117], PaLM-\nSayCan [1], PaLM-E [34], etc. Next, we divide the robot into\nfour modules: perception, decision-making, control, and in-\nteraction. For each module, we discuss the relevant technolo-\ngies and their functions, including the perception module’s\nability to process the robot’s input from the surroundings;\nthe decision-making module’s capacity to understand human\ninstructions and plan; the control module’s role in processing\noutput actions; the interaction module’s ability to interact\nwith the environment. We also explore the potential applica-\ntion scenarios of current robots based on LLMs and discuss\nthe challenges, such as training, safety, shape, deployment,\nand long-term task performance. Finally, we consider the\nsocial and ethical implications of post-intelligent robots and\ntheir potential impact on human society.\nAs LLMs continue to evolve, robots may become in-\ncreasingly intelligent and capable of processing instructions\nand tasks more efficiently. With advancements in hardware,\nrobots may eventually become reliable assistants for hu-\nmans, as depicted in science fiction movies. However, we\nmust also be mindful of their potential impact on society and\naddress any concerns proactively. Embodied intelligence is\na new paradigm for the development of intelligent science\nand is of great significance in leading the development of\nthe future. LLM-based robotics represent a potential path\nto embodied intelligence. We hope this survey can provide\nsome inspiration to the community and facilitate research in\nrelated fields.\nAcknowledgment\nThis research was supported in part by the National\nNatural Science Foundation of China (Nos. 62002136 and\n62272196), the Natural Science Foundation of Guangdong\nProvince (No. 2022A1515011861), Engineering Research\nCenter of Trustworthy AI, Ministry of Education (Jinan Uni-\nversity), and Guangdong Key Laboratory of Data Security\nand Privacy Preserving.\nReferences\n[1] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David,\nB., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al., 2022.\nDo as i can, not as i say: Grounding language in robotic affordances,\nin: The Conference on Robot Learning, pp. 287–318.\n[2] Alam, A., 2022. Social robots in education for long-term human-\nrobot interaction: socially supportive behaviour of robotic tutor for\ncreating robo-tangible learning environment in a guided discovery\nlearning interaction. ECS Transactions 107, 12389.']","A modular design approach in robotics can be implemented by allowing each part of the robot to be swapped out like a machine, enhancing versatility and adaptability. For example, the robot can be composed of interchangeable head, torso, upper limbs, and lower limbs, which can be replaced based on the specific task at hand.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}
 {'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
How can large models and robots be applied in agriculture and farm mechanization?,"['Robots can analyze medical images, patient data, and\nclinical records, aiding in disease detection, surgical\nplanning, and personalized therapy. They can also pro-\nvide physical assistance and rehabilitation exercises\nfor mobility-impaired patients.\n• Environmental monitoring and exploration. Large\nmodels can be combined with robot platforms for\nmonitoring and exploration in various environments,\nsuch as oceans, forests, and disaster sites. These robots\ncan analyze sensor data, satellite imagery, and other\nenvironmental data to monitor pollution levels, detect\nnatural disasters, and explore uncharted territories.\n• Agriculture and farm mechanization. Large models\nand robots can be applied in agriculture and farm\nmechanization, optimizing crop management, mon-\nitoring plant health, and automating labor-intensive\ntasks. Robots equipped with sensors and cameras can\ncollect data from farmlands, and analyze soil condi-\ntions, climate changes, and crop requirements, provid-\ning farmers with decision support to enhance agricul-\ntural productivity and sustainability.\n• Education and learning assistance. Large models\nand robots can provide personalized tutoring and\nlearning support in the field of education. Robots\ncan interact with students, and then offer personal-\nized learning materials and guidance based on their\nabilities and needs [2]. Leveraging the semantic un-\nderstanding and knowledge reasoning capabilities of\nlarge models, robots can answer questions, explain\nconcepts, and help students deepen their understand-\ning of knowledge.\nIn summary, the combination of large models and robotics\nholds tremendous potential across various domains, in-\ncluding autonomous navigation, speech interaction, visual\nperception, human-robot collaboration, industrial automa-\ntion, healthcare, environmental monitoring, agriculture, and\neducation. It can bring convenience and innovation to human\nlife and work.\n5. Challenges\n5.1. Datasets\nIn the realm of Web 3.0 [39], big data [123], AI-\nGenerated Content (AIGC) [140], and machine learning,\ncollecting datasets has always been a challenge. Currently,\ntraining LLMs require vast amounts of data to support their\ncapabilities, particularly high-quality datasets that consume\nconsiderable resources. In the field of robotics, collecting\ndatasets is even more difficult. While LLM like ChatGPT re-\nlies on text data for pre-training [14], VLM uses a combina-\ntion of text and image data [99]. Robotics, however, requires\na combination of both, with the addition of multimodal\ndata, such as text, images, and touch, to serve as the robot’s\nsensory input. These diverse datasets need to be processed\nin a unified format [34], allowing the robot’s brain to plan\nand divide tasks effectively. Unfortunately, there is a lack\nof ready-made, multi-modal datasets, and collecting them\nrequires a significant time investment. Moreover, policy\ncontrol is necessary, which includes the interaction between']","Large models and robots can be applied in agriculture and farm mechanization by optimizing crop management, monitoring plant health, and automating labor-intensive tasks. Robots equipped with sensors and cameras can collect data from farmlands, analyze soil conditions, climate changes, and crop requirements, providing farmers with decision support to enhance agricultural productivity and sustainability.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
How can LLMs assist robots in knowledge acquisition and reasoning?,"['• Task execution. LLMs assist robots in performing\nvarious tasks by understanding and generating natural\nlanguage instructions. Robots can navigate, manipu-\nlate objects, and execute specific actions based on user\nlanguage commands [126]. This opens up broader\npossibilities for robot applications in everyday life.\n• Knowledge acquisition and reasoning. LLMs pos-\nsess powerful information retrieval and reasoning ca-\npabilities, which can help robots acquire and process\nrich knowledge. Robots can interact with language\nmodels to obtain real-time and accurate information,\nthereby improving their decision-making ability and\nintelligence.\n• Flexibility and adaptability. The flexibility of LLMs\nenables robots to adapt to different tasks and envi-\nronments. Through interaction with language mod-\nels, robots can make flexible adjustments and self-\nadaptation based on specific circumstances, better\nmeeting user needs [52].\n• Learning and improvement. LLMs enable contin-\nuous learning and improvement through interaction\nwith users. By analyzing and understanding user feed-\nback, robots can enhance their performance and profi-\nciency. This learning and improvement capability al-\nlows robots to gradually adapt to user personalities and\npreferences, providing more personalized services.\n• Multimodal interaction. LLMs also support multi-\nmodal interaction, enabling robots to process different\nforms of inputs such as speech, images, and text simul-\ntaneously. This multimodal capability [141] allows\nrobots to comprehensively understand user needs and\nprovide richer interaction experiences.\n• Education and entertainment. LLMs offer poten-\ntial applications for education and entertainment pur-\nposes in robotics. Robots can provide educational\ncontent, answer questions, or engage in games and\nentertainment activities through interaction with lan-\nguage models. This has significant implications for\nchildren’s education, language learning, and the en-\ntertainment industry.\n• Emotional interaction. The application of LLMs\nenhances the emotional interaction capabilities of\nrobots. By generating emotionally responsive outputs,\nrobots can establish closer and more meaningful\nrelationships with users. This emotional interaction\nis valuable in fields such as care robots, emotional\nsupport, and psychotherapy.\n• Collaboration and cooperation. LLMs enable robots\nto collaborate and cooperate better with humans.\nRobots can jointly solve problems, formulate plans,\nand execute tasks through interaction with language\nmodels [126]. This collaboration and cooperation\nability is significant for industrial automation, team\ncollaboration, and human-robot coexistence.\n• Innovation and exploration. The application of LLMs\nstimulates innovation and exploration in the field of\nrobotics. Through interaction with language mod-\nels, robots can possess higher-level intelligence and\ncomprehension abilities, opening up new avenues for\nresearch and development in robotics.']","LLMs possess powerful information retrieval and reasoning capabilities, which can help robots acquire and process rich knowledge. Robots can interact with language models to obtain real-time and accurate information, thereby improving their decision-making ability and intelligence.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What issues are associated with traditional geometric approaches in navigation?,"['approaches can be found in a recent survey article (Borges et al., 2022). While we have argued in previous\nwork (Wellhausen et al., 2019; Wellhausen et al., 2020) that purely geometric approaches are not suﬃcient\nfor navigation in natural outdoor environments, approaches relying on semantic information exhibit the\nsame issues as traditional geometric approaches. They, either implicitly through semantic segmentation of\nthe environment (Rothrock et al., 2016; Bradley et al., 2015; Otsu et al., 2016; Valada et al., 2017), or\nexplicitly (Kim et al., 2006; Barnes et al., 2017; Hirose et al., 2018) predict a traversability label. How-\never, we can instead reinterpret traversability labels as foothold feasibility labels and use them to enhance\ngeometric planning. While full kino-dynamic planning over long horizons would be the most general and\naccurate planning method, applying these methods in real-time is not tractable on current computational\nhardware (Tonneau et al., 2018; Fernbach et al., 2017; Winkler et al., 2018). Dynamic planning using a\nreduced robot model has recently shown promising results (Norby and Johnson, 2020) but has not been\nevaluated in deployment scenarios. Other work on navigation planning speciﬁcally for legged robots either\nonly considers cases of obstacle avoidance on ﬂat terrain (Zhao et al., 2018; Harper et al., 2019) or does\nadditional contact planning, which pushes computational complexity past the real-time mark (Belter et al.,\n2019; Lin and Berenson, 2017; Reid et al., 2020). Approaches which learn traversability (Chavez-Garcia\net al., 2017) or motion cost (Guzzi et al., 2020; Yang et al., 2021a) are powerful, but are either too slow due\nto the sequential querying of neural networks during sampling-based planning (Guzzi et al., 2020) or struggle\nin tight spaces where precise motion checking is necessary (Yang et al., 2021a). In previous work (Wellhausen\nand Hutter, 2021), we have used reachability planning with a learned foothold score to achieve real-time\nperformance for legged navigation planning. While resulting paths were generally feasible, the employed\nshortest path cost did not suﬃciently account for locomotion risk on challenging terrain or close to obstacles.\nFor further related work of other navigation planners used during SubT, please refer to Section 1.3.1.\nArtPlanner uses a reachability-based robot representation (Tonneau et al., 2015) and learned foothold\nscores (Wellhausen and Hutter, 2021) with batched motion cost computation (Yang et al., 2021a). This\nis the ﬁrst work which combines geometric collision checking and learned motion costs in a navigation\nplanner for legged robots.\n1.3\nDARPA Subterranean Challenge\nThe DARPA Subterranean Challenge (SubT) was a robotics challenge initiated by the Defense Advanced Re-\nsearch Projects Agency (DARPA) in 2018. Its goal was to expedite development of robotic systems to rapidly']","Traditional geometric approaches in navigation exhibit the same issues as approaches relying on semantic information. They either implicitly through semantic segmentation of the environment or explicitly predict a traversability label. However, these approaches do not take into account foothold feasibility or locomotion risk on challenging terrain or close to obstacles.",simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How is image erosion used in the implementation of the height map in the navigation planner?,"['ure 6\n. It is implemented using image erosion on the foothold score layer of the height map, which reduces\nthe steppable map region by a safety margin. This has the additional beneﬁt of also removing small isolated\nsteppable patches from the map, which could only be overcome by solving a stepping-stone problem. To\navoid unnecessarily inﬂating small obstacles like rails, which the robot can easily step over, we do not inﬂate\nunsteppable regions below a certain size. In practice this is done by performing an image dilation of smaller\nradius, before doing the erosion.\nThis safety threshold was crucial on the Subway Station of the Finals circuit, as shown in Section 3.2.2.\n2.4.3\nCeiling Point Filter\nWe use a 2.5D height map representation for planning. While this is suﬃcient for ground robot navigation\nin most environments, it can be problematic in the tight underground spaces encountered during SubT. Low\nceilings mean that they are frequently observed by the depth sensors, which causes spikes in the height map,\nas shown in Figure 2(b). However, we cannot simply discard all points above a ﬁxed height, since this would\neither prevent us from planning up slopes or from passing underneath low overhangs.\nWe therefore use a rising height threshold to ﬁlter points (Miki et al., 2022b), shown in Figure 6\n. It\nﬁlters points just above robot height close to the robot, and linearly increases the height threshold up to a\nmaximum at larger distances. This setup caused map spikes in parts of the course with low ceilings which\nslowed us down, but these crucially never stopped us from exploring. It allowed us to pass underneath very\nlow overhangs, and to plan up slopes, even when encountered together, as detailed in Section 3.3.1.\n3\nExperimental Results\nArtPlanner was deployed on all four ANYmal-C ground robots of team CERBERUS during all runs of the\nSubT Finals. It used a height map of size 8 m×8 m with a 4 cm resolution. We only cover results related\nto the navigation planner presented in this work. For further details on the general performance we refer to\nour overview article (Tranzatto et al., 2022a).\nWe deployed all four ground robots during the Prize Run, which were directed by the supervisor to explore\ndiﬀerent areas of the course. All ground robots successfully made it to the end of the competition and\nwe did not observe a single path planning or locomotion failure, which could have been provoked by bad\npath planning. Our planner was active for 90 minutes between all robots which accounts for 88.94% of all\nrobot motion. We gracefully navigated the narrow doorways and small rooms in the Urban section, passed\nthrough the Tunnel section with obscuring fog, and made it through the narrowest and roughest part of the\nCave section. The only case where ArtPlanner did not follow the exploration path over traversable terrain\nhappened at the stairs leading to the subway station. This resulted from our operational decision to use']",Image erosion is used in the implementation of the height map in the navigation planner to reduce the steppable map region by a safety margin. It also removes small isolated steppable patches from the map. This is done by performing an image dilation of smaller radius before doing the erosion.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does the affordance function contribute to the value function in RL?,"['current state after parsing a natural language command,\na temporal-difference-based (TD) reinforcement learning\napproach can be employed. This method learns a value\nfunction to evaluate whether the skill is executable or not [1].\nThe value function is derived from the corresponding affor-\ndance function of reinforcement learning [42]. Additionally,\nLM-Nav [117] utilizes a self-supervised learning method\nto enhance the parsing of free-form language instructions\nleveraging pre-trained VLM in a large number of previous\nenvironments. To address the challenges of long-term tasks,\nhierarchical reinforcement learning (HRL) [55] can be em-\nployed, where higher-level policies play a role in setting\nobjectives for lower-level protocols to execute [90, 132]. The\nprocess of mapping natural language and observations into\nrobot actions can also be viewed as a sequence modeling\nproblem [9, 10, 29]. Transformer-based robot control, such\nas the Behavior Transformer [113], focuses on learning\ndemonstrations that correspond to each task. Gato [104]\nsuggests training a model on large datasets including robotic\nand non-robotic.\n3.4. Interaction\nInteraction serves as a fundamental module that enables\nrobots to engage and interact with both the environment\nand humans. To enhance robots’ ability to interact in the\nphysical world, they are often trained extensively. While\nsome researchers utilize artificial intelligence to interact in\nvirtual environments, such as games or simulations, ulti-\nmately, these models must be transferred to the real world.\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 10 of 19\nLarge Language Models for Robotics: A Survey\nHowever, the accuracy of these models tends to be lower in\nreal-world settings compared to simulated environments.\n3.4.1. Game\nTraditional game developers manually write over a\ndozen character behaviors (including class methods and at-\ntributes) for the implementation of a game in the Valentine’s\nDay party’s specific game environment. Almost all of these\nbehaviors are fixed sets, making the process very cumber-\nsome with poor scalability. In games, LLMs have been used\nto create interactive novels and text adventure games [17].\nLLMs are increasingly utilized for planning robotic tasks\ndue to their capacity to generate and decompose sequences\nof actions. In GA [95], they created a computer program\nthat can mimic the behavior of human beings, called the\nGenerative Agents. It extends the LLM by using natural\nlanguage to store complete records of the intelligentsia’s\nexperiences. Synthesizing accumulated memories and re-\nflecting upon them at higher levels over time, the system can\ndynamically retrieve these memories to plan and guide its\nbehavior. Agent characters engage in comprehensive verbal\nexchanges utilizing authentic human language. They possess\nknowledge of other intelligent entities within their vicinity,\nand the generative agent framework dictates whether they']",The value function in reinforcement learning is derived from the corresponding affordance function. The affordance function contributes to the value function by providing information on whether a skill is executable or not.,reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"What are the benefits and challenges of integrating language-based human-robot interaction in robotics, and how can it enhance robot's language capabilities?","['exchanges utilizing authentic human language. They possess\nknowledge of other intelligent entities within their vicinity,\nand the generative agent framework dictates whether they\nproceed to interact or initiate a dialogue. These intelligent\nagents’ characters can exhibit quite realistic personal behav-\nior and social interactions. For example, when someone tells\none of the agents that they have a desire to organize and host\na festive gathering to celebrate Valentine’s Day, these agents\nwill spontaneously invite others to attend, meet each other,\ndate, and be on time for the party together. This innovative\narchitecture empowers generative agents with the ability to\nretain, recall, contemplate, engage with fellow agents, and\nstrategize amidst ever-changing circumstances.\n3.4.2. Language-based human-robot interaction\nThere are GUI (Graphical User Interface) and LUI (Lan-\nguage User Interface) for human-robot interaction. GUI\nrefers to a computer-operated user interface that is graphi-\ncally displayed and uses an interactive device to manage the\ninteraction with the system. Unlike GUI, LUI can directly\nuse natural human language for human-robot interaction,\nand the most representative LUI product is ChatGPT. Tradi-\ntionally, the task of simulating human-robot interaction us-\ning natural language has proven to be difficult due to the con-\nstraints imposed on users by rigid instructions, or the need\nfor intricate algorithms to manage numerous probability dis-\ntributions related to actions and target objects[4]. However,\nit is not easy to translate instructions into commands that\nrobots can understand in the real world, and traditionally,\nfixed collections of desired actions and directives have been\nused to enable robots to understand human language. How-\never, this can significantly limit the robot’s flexibility and has\nlimited generalizability across different hardware platforms.\nThe LAnguage Trajectory TransformEr [16] introduces a\nversatile language-driven framework that empowers users\nto customize and adapt the overall trajectories of robots.\nThe approach leverages pre-trained language models (e.g.,\nBERT [31] and CLIP [99]) to encode the user’s intention\nand target objects directly from unrestricted text inputs and\nscene images. It combines geometric features produced by a\nnetwork of transformer encoders and generates the trajectory\nusing a transformer decoder, eliminating the requirement for\nprior task-related or robot-specific information.\nConsidering the vagueness and ambiguity of natural\nlanguage, from the point of view of human-robot interaction,\nrobots should enhance the initiative of interaction in the\nfuture, that is to say, let the robot actively ask the user\nquestions through the large language model. If the robot feels\nthat the user’s words are problematic and is not sure what\nthey mean, it should ask you back what you mean or whether\nyou mean what you say.\n4. Applications of LLMs in Robotics'
 'guage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology\n[66, 32] has created a demand for more intelligent and\nnatural human-machine interaction. Combining LLMs with\nrobots can provide robots with stronger natural language\nunderstanding and generation capabilities, enabling more\nintelligent and human-like conversations and interactions.\nApplying LLMs to the field of robotics has important\nresearch significance and practical value. Firstly, LLMs can\nsignificantly enhance a robot’s natural language understand-\ning and generation capabilities. Traditional robot dialogue\nsystems often require manual rules and template writing,\n∗Corresponding author\nflzeng1@gmail.com (F. Zeng); wsgan001@gmail.com (W. Gan);\nyonghengwwang@gmail.com (Y. Wang); tliuning@jnu.edu.cn (N. Liu);\npsyu@uic.edu (P.S. Yu)\nORCID(s):\nmaking it difficult to handle complex natural language in-\nputs. LLMs, on the other hand, can better understand and\ngenerate natural language by learning from massive text\ncorpora, enabling robots to have more intelligent and natural\nconversation abilities. Secondly, LLMs can provide more\ndiverse conversation content and personalized interaction\nexperiences. Through interaction with LLMs, robots can\ngenerate varied responses and personalize interactions based\non user preferences and needs. This helps improve user\nsatisfaction and interactions. In addition, the combination of\nLLMs and robots contributes to the advancement of artificial\nintelligence and robotics technology, laying the foundation\nfor future intelligent robots (or called smart robots).\nCurrently, many research teams and companies have\nbegun exploring the application of LLMs in the field of\nrobotics. Some research focuses on using LLMs for natural\nlanguage understanding in robots. By using pre-trained\nlanguage models [152], robots can better understand user\nintentions and needs [34, 117]. Other research focuses\non using LLMs for natural language generation in robots.\nRobots can generate fluent and coherent natural language\nresponses through interaction with language models. Fur-\nthermore, some research explores how to combine LLMs\nwith other technologies, such as knowledge graphs and senti-\nment analysis, to further enhance robot dialogue capabilities\nand user experiences. From multiple perspectives, LLMs-\nbased robotics is one of the most promising paths to achieve\nembodied intelligence in the future.\nAlthough the combination of LLMs and robots has many\npotential advantages, it also faces challenges and issues [50,\n86]. Firstly, training and deploying LLMs require substantial\ncomputing resources and data, which can be challenging for\nresource-limited robot platforms [7]. Secondly, LLMs may\ngenerate inaccurate, unreasonable, or even harmful content\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 1 of 19\narXiv:2311.07226v1  [cs.RO]  13 Nov 2023']","Applying LLMs to the field of robotics has important research significance and practical value. Firstly, LLMs can significantly enhance a robot’s natural language understanding and generation capabilities. Traditional robot dialogue systems often require manual rules and template writing, making it difficult to handle complex natural language inputs. LLMs, on the other hand, can better understand and generate natural language by learning from massive text corpora, enabling robots to have more intelligent and natural conversation abilities. Secondly, LLMs can provide more diverse conversation content and personalized interaction experiences. Through interaction with LLMs, robots can generate varied responses and personalize interactions based on user preferences and needs. This helps improve user satisfaction and interactions. In addition, the combination of LLMs and robots contributes to the advancement of artificial intelligence and robotics technology, laying the foundation for future intelligent robots (or called smart robots). Although the combination of LLMs and robots has many potential advantages, it also faces challenges and issues. Firstly, training and deploying LLMs require substantial computing resources and data, which can be challenging for resource-limited robot platforms. Secondly, LLMs may generate inaccurate, unreasonable, or even harmful content.",multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}
 {'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"How do large language models contribute to robotics' dexterity intelligence and their applications in control, perception, decision-making, and path planning?","['Large Language Models for Robotics: A Survey\nFanlong Zenga, Wensheng Gana,∗, Yongheng Wanga, Ning Liua and Philip S. Yub\naSchool of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China\nbDepartment of Computer Science, University of Illinois Chicago, Chicago, USA\nA R T I C L E I N F O\nKeywords:\nlarge language models\nrobotics\ncontrol and interaction\ndecision-making\nembodied intelligence\nA B S T R A C T\nThe human ability to learn, generalize, and control complex manipulation tasks through multi-\nmodality feedback suggests a unique capability, which we refer to as dexterity intelligence. Under-\nstanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field of robotics have\ngarnered increasing attention. LLMs possess the ability to process and generate natural language,\nfacilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of\nrobotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot\ninteraction, and autonomy. Therefore, this comprehensive review aims to summarize the applications\nof LLMs in robotics, delving into their impact and contributions to key areas such as robot control,\nperception, decision-making, and path planning. We first provide an overview of the background and\ndevelopment of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and\nrecent advancements in robotics models based on LLMs. We then delve into the various techniques\nused in the model, including those employed in perception, decision-making, control, and interaction.\nFinally, we explore the applications of LLMs in robotics and some potential challenges they may face\nin the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics\nis one of the promising but challenging paths to achieve this.\n1. Introduction\nHumans possess exceptional proficiency in executing\nintricate and dexterous manipulation skills by integrating\ntactile, visual, and other sensory inputs. Research in the\nfield of robotics aspires to imbue robots with comparable\nmanipulation intelligence. Although recent advancements\nin robotics and machine learning have yielded promising\nresults in visual mitigation and exploration learning for robot\nmanipulation, there remains much to be accomplished in this\narea. Large language models (LLMs), such as BERT [31],\nRoberta [79], GPT-3 [27], GPT-4 [110], have emerged as\nsignificant research achievements in the field of artificial\nintelligence (AI) in recent years. Through deep learning\ntechniques [76], LLMs can be trained on massive text cor-\npora, enabling them to generate high-quality natural lan-\nguage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology']","Large language models (LLMs) contribute to robotics' dexterity intelligence by enabling efficient interaction and collaboration with robots through the processing and generation of natural language. LLMs have applications in control, perception, decision-making, and path planning in robotics.",reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
How can large models and robots be applied in agriculture and farm mechanization?,"['Robots can analyze medical images, patient data, and\nclinical records, aiding in disease detection, surgical\nplanning, and personalized therapy. They can also pro-\nvide physical assistance and rehabilitation exercises\nfor mobility-impaired patients.\n• Environmental monitoring and exploration. Large\nmodels can be combined with robot platforms for\nmonitoring and exploration in various environments,\nsuch as oceans, forests, and disaster sites. These robots\ncan analyze sensor data, satellite imagery, and other\nenvironmental data to monitor pollution levels, detect\nnatural disasters, and explore uncharted territories.\n• Agriculture and farm mechanization. Large models\nand robots can be applied in agriculture and farm\nmechanization, optimizing crop management, mon-\nitoring plant health, and automating labor-intensive\ntasks. Robots equipped with sensors and cameras can\ncollect data from farmlands, and analyze soil condi-\ntions, climate changes, and crop requirements, provid-\ning farmers with decision support to enhance agricul-\ntural productivity and sustainability.\n• Education and learning assistance. Large models\nand robots can provide personalized tutoring and\nlearning support in the field of education. Robots\ncan interact with students, and then offer personal-\nized learning materials and guidance based on their\nabilities and needs [2]. Leveraging the semantic un-\nderstanding and knowledge reasoning capabilities of\nlarge models, robots can answer questions, explain\nconcepts, and help students deepen their understand-\ning of knowledge.\nIn summary, the combination of large models and robotics\nholds tremendous potential across various domains, in-\ncluding autonomous navigation, speech interaction, visual\nperception, human-robot collaboration, industrial automa-\ntion, healthcare, environmental monitoring, agriculture, and\neducation. It can bring convenience and innovation to human\nlife and work.\n5. Challenges\n5.1. Datasets\nIn the realm of Web 3.0 [39], big data [123], AI-\nGenerated Content (AIGC) [140], and machine learning,\ncollecting datasets has always been a challenge. Currently,\ntraining LLMs require vast amounts of data to support their\ncapabilities, particularly high-quality datasets that consume\nconsiderable resources. In the field of robotics, collecting\ndatasets is even more difficult. While LLM like ChatGPT re-\nlies on text data for pre-training [14], VLM uses a combina-\ntion of text and image data [99]. Robotics, however, requires\na combination of both, with the addition of multimodal\ndata, such as text, images, and touch, to serve as the robot’s\nsensory input. These diverse datasets need to be processed\nin a unified format [34], allowing the robot’s brain to plan\nand divide tasks effectively. Unfortunately, there is a lack\nof ready-made, multi-modal datasets, and collecting them\nrequires a significant time investment. Moreover, policy\ncontrol is necessary, which includes the interaction between']","Large models and robots can be applied in agriculture and farm mechanization by optimizing crop management, monitoring plant health, and automating labor-intensive tasks. Robots equipped with sensors and cameras can collect data from farmlands, analyze soil conditions, climate changes, and crop requirements, providing farmers with decision support to enhance agricultural productivity and sustainability.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is the DARPA Subterranean Challenge?,"['ArtPlanner:\nRobust Legged Robot Navigation in the Field\nLorenz Wellhausen\nRobotic Systems Lab\nETH Z¨\nurich\nSwitzerland\nMarco Hutter\nRobotic Systems Lab\nETH Z¨\nurich\nSwitzerland∗\nAbstract\nDue to the highly complex environment present during the DARPA Subterranean Challenge,\nall six funded teams relied on legged robots as part of their robotic team. Their unique loco-\nmotion skills of being able to step over obstacles require special considerations for navigation\nplanning. In this work, we present and examine ArtPlanner, the navigation planner used by\nteam CERBERUS during the Finals. It is based on a sampling-based method that deter-\nmines valid poses with a reachability abstraction and uses learned foothold scores to restrict\nareas considered safe for stepping. The resulting planning graph is assigned learned motion\ncosts by a neural network trained in simulation to minimize traversal time and limit the\nrisk of failure. Our method1 achieves real-time performance with a bounded computation\ntime. We present extensive experimental results gathered during the Finals event of the\nDARPA Subterranean Challenge, where this method contributed to team CERBERUS win-\nning the competition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.\n1\nIntroduction\nNavigation planning for legged robots has distinct challenges which are not present for other types of robots.\nWhile ﬂying robots attempt to avoid any contact with the environment, ground robots by deﬁnition require\ncontact with the ground to locomote. Compared to other types of ground robots, which have a constant\ncontact patch with the ground, legged robots can overcome obstacles by lifting their legs. Most traditional\nnavigation planning approaches assume a single traversability value for any given terrain patch, which they\ncheck against the footprint of the robot (Wermelinger et al., 2016). These approaches are limiting for legged\nrobots because their ability to change their footprint and choose contact locations with the environment\ndeliberately is not accounted for. Deﬁning a traversability value for such a highly articulated systems is\nextremely challenging due to the high dimensionality of the problem.\nTo ﬁnd a stable conﬁguration for a legged robot, we need to ﬁnd a base pose where the terrain is within\nthe range of motion of each limb. We call this reachability checking, where a robot is represented as one\ncollision volume for its torso, and one reachability volume for each of its limbs (Tonneau et al., 2015). When\nchecking the feasability of a given robot pose, the torso volume is expected to be collision-free, while collision\n∗This work was supported by the Swiss National Science Foundation (SNF) through the NCCR Robotics and project 188596,\nand the EU Horizon 2020 research and innovation programme under grant agreements No 780883, No 852044 and No 101016970.']",The DARPA Subterranean Challenge is a highly complex competition that involves navigating in a subterranean environment. All six funded teams in the competition relied on legged robots as part of their robotic team.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
What is the title and source of the paper that discusses deep imitation learning for complex manipulation tasks?,"['arXiv preprint,\narXiv:2306.13549 .\n[150] Zaremba, W., Sutskever, I., Vinyals, O., 2014.\nRecurrent neural\nnetwork regularization. arXiv preprint arXiv:1409.2329 .\n[151] Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J.S., Cao, J., Farhadi,\nA., Choi, Y., 2021. MERLOT: Multimodal neural script knowledge\nmodels. Advances in Neural Information Processing Systems 34,\n23634–23651.\n[152] Zeng, F., Gan, W., Wang, Y., Yu, P.S., 2023. Distributed training of\nlarge language models, in: The 29th IEEE International Conference\non Parallel and Distributed Systems, IEEE. pp. 1–8.\n[153] Zhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg, K.,\nAbbeel, P., 2018. Deep imitation learning for complex manipula-\ntion tasks from virtual reality teleoperation, in: IEEE International\nConference on Robotics and Automation, IEEE. pp. 5628–5635.\n[154] Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X.,\nSchuurmans, D., Cui, C., Bousquet, O., Le, Q., et al., 2023. Least-\nto-most prompting enables complex reasoning in large language\nmodels. The International Conference on Learning Representations\n.\n[155] Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J., Gao, J., 2020.\nUnified vision-language pre-training for image captioning and vqa,\nin: AAAI Conference on Artificial Intelligence, pp. 13041–13049.\n[156] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M., 2023a. MiniGPT-\n4: Enhancing vision-language understanding with advanced large\nlanguage models. arXiv preprint, arXiv:2304.10592 .\n[157] Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L.,\nFarhadi, A., 2017. Target-driven visual navigation in indoor scenes\nusing deep reinforcement learning, in: The IEEE International Con-\nference on Robotics and Automation, IEEE. pp. 3357–3364.\n[158] Zhu, Z., Lin, K., Jain, A.K., Zhou, J., 2023b.\nTransfer learning\nin deep reinforcement learning: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence 45, 13344–13362.\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 19 of 19']","Deep imitation learning for complex manipulation tasks from virtual reality teleoperation, in: IEEE International Conference on Robotics and Automation, IEEE.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"What is the topic of the paper ""Formalizing properties of agents""?","['robot transfer, in: IEEE International Conference on Robotics and\nAutomation, IEEE. pp. 2169–2176.\n[31] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018.\nBERT:\nPre-training of deep bidirectional transformers for language under-\nstanding, in: The Conference of the North American Chapter of the\nAssociation for Computational Linguistics, ACL. pp. 4171–4186.\n[32] Dorigo, M., Theraulaz, G., Trianni, V., 2021. Swarm robotics: Past,\npresent, and future [point of view]. Proceedings of the IEEE 109,\n1152–1165.\n[33] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai,\nX., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,\nS., et al., 2021. An image is worth 16x16 words: Transformers for\nimage recognition at scale. International Conference on Learning\nRepresentations .\n[34] Driess, D., Xia, F., Sajjadi, M.S.e.a., 2023. PaLM-E: An embodied\nmultimodal language model, in: International Conference on Ma-\nchine Learning, pp. 8469–8488.\n[35] Duvallet, F., Walter, M.R., Howard, T., Hemachandra, S., Oh, J.,\nTeller, S., Roy, N., Stentz, A., 2016. Inferring maps and behaviors\nfrom natural language instructions, in: The 14th International Sym-\nposium on Experimental Robotics, Springer. pp. 373–388.\n[36] Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann,\nB., Askell, A., Bai, Y., Chen, A., Conerly, T., et al., 2021.\nA\nmathematical framework for transformer circuits.\nTransformer\nCircuits Thread 1.\n[37] Eysenbach, B., Salakhutdinov, R.R., Levine, S., 2019. Search on\nthe replay buffer: Bridging planning and reinforcement learning.\nAdvances in Neural Information Processing Systems 32.\n[38] Francis, A., Faust, A., Chiang, H.T.L., Hsu, J., Kew, J.C., Fiser, M.,\nLee, T.W.E., 2020. Long-range indoor navigation with PRM-RL.\nIEEE Transactions on Robotics 36, 1115–1134.\n[39] Gan, W., Ye, Z., Wan, S., Yu, P.S., 2023. Web 3.0: The future of\ninternet, in: Companion Proceedings of the ACM Web Conference,\npp. 1266–1275.\n[40] Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., Gao, J., et al., 2022. Vision-\nlanguage pre-training: Basics, recent advances, and future trends.\nFoundations and Trends® in Computer Graphics and Vision 14,\n163–352.\n[41] Ghosh, D., Rahme, J., Kumar, A., Zhang, A., Adams, R.P., Levine,\nS., 2021. Why generalization in RL is difficult: Epistemic pomdps\nand implicit partial observability. Advances in Neural Information\nProcessing Systems 34, 25502–25515.\n[42] Gibson, J.J., 1977. The theory of affordances. Hilldale, USA 1,\n67–82.\n[43] Ginsberg, M., 2012. Essentials of artificial intelligence. Newnes.\n[44] Goodwin, R., 1995. Formalizing properties of agents. Journal of\nLogic and Computation 5, 763–781.\n[45] Grafman, J., Spector, L., Rattermann, M.J., 2004. Planning and the\nbrain, in: The cognitive psychology of planning. Psychology Press,\npp. 191–208.\n[46] Gravitas, S., 2023. Auto-GPT: An Autonomous GPT-4 Experiment.\n[47] Gu, J., Stefani, E., Wu, Q., Thomason, J., Wang, X.E., 2022. Vision-']",The topic of the paper 'Formalizing properties of agents' is formalizing properties of agents.,simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are the benefits of combining LLMs with robots for natural language understanding and generation? What challenges may arise in terms of computing resources and generating inaccurate or harmful content?,"['guage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology\n[66, 32] has created a demand for more intelligent and\nnatural human-machine interaction. Combining LLMs with\nrobots can provide robots with stronger natural language\nunderstanding and generation capabilities, enabling more\nintelligent and human-like conversations and interactions.\nApplying LLMs to the field of robotics has important\nresearch significance and practical value. Firstly, LLMs can\nsignificantly enhance a robot’s natural language understand-\ning and generation capabilities. Traditional robot dialogue\nsystems often require manual rules and template writing,\n∗Corresponding author\nflzeng1@gmail.com (F. Zeng); wsgan001@gmail.com (W. Gan);\nyonghengwwang@gmail.com (Y. Wang); tliuning@jnu.edu.cn (N. Liu);\npsyu@uic.edu (P.S. Yu)\nORCID(s):\nmaking it difficult to handle complex natural language in-\nputs. LLMs, on the other hand, can better understand and\ngenerate natural language by learning from massive text\ncorpora, enabling robots to have more intelligent and natural\nconversation abilities. Secondly, LLMs can provide more\ndiverse conversation content and personalized interaction\nexperiences. Through interaction with LLMs, robots can\ngenerate varied responses and personalize interactions based\non user preferences and needs. This helps improve user\nsatisfaction and interactions. In addition, the combination of\nLLMs and robots contributes to the advancement of artificial\nintelligence and robotics technology, laying the foundation\nfor future intelligent robots (or called smart robots).\nCurrently, many research teams and companies have\nbegun exploring the application of LLMs in the field of\nrobotics. Some research focuses on using LLMs for natural\nlanguage understanding in robots. By using pre-trained\nlanguage models [152], robots can better understand user\nintentions and needs [34, 117]. Other research focuses\non using LLMs for natural language generation in robots.\nRobots can generate fluent and coherent natural language\nresponses through interaction with language models. Fur-\nthermore, some research explores how to combine LLMs\nwith other technologies, such as knowledge graphs and senti-\nment analysis, to further enhance robot dialogue capabilities\nand user experiences. From multiple perspectives, LLMs-\nbased robotics is one of the most promising paths to achieve\nembodied intelligence in the future.\nAlthough the combination of LLMs and robots has many\npotential advantages, it also faces challenges and issues [50,\n86]. Firstly, training and deploying LLMs require substantial\ncomputing resources and data, which can be challenging for\nresource-limited robot platforms [7]. Secondly, LLMs may\ngenerate inaccurate, unreasonable, or even harmful content\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 1 of 19\narXiv:2311.07226v1  [cs.RO]  13 Nov 2023']","Combining LLMs with robots can enhance a robot's natural language understanding and generation capabilities, providing more intelligent and natural conversation abilities. It can also provide more diverse conversation content and personalized interaction experiences, improving user satisfaction. However, combining LLMs with robots requires substantial computing resources and data, which can be challenging for resource-limited robot platforms. Additionally, LLMs may generate inaccurate, unreasonable, or harmful content.",reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"What is the impact of risk pruning on the planning graph and robot's avoidance of risky areas, and why are safety thresholds and virtual surfaces important for safe navigation?","['even though the actual terrain was too rough and rocky to overcome. Using just reachability checking, our\nsampler found individual valid poses on the slope and connected them, as shown in Figure 9(a). Here, risk\npruning identiﬁed that moving on this slope was too risky and removed these edges, as shown in Figure 9(b).\nWithout risk pruning the robot would have tried to scale this slope, which would in the best case have lead\nto lost time, and in the worst case to loss of this robot.\nFigure 7\n+\nshow that the cost function generally lead to safer paths, which kept a safe distance from\n(a) Without risk pruning\n(b) With risk pruning\nFigure 9: Sampled valid poses shown in red with blue graph edges connecting them. Using reachability checking,\nvalid robot poses are still found on a steep and rocky incline in the cave section. Pruning graph edges based on\nmotion risk prevents the planning graph from spanning this risky area.\n(a) Onboard Image\n(b) Exploration Path\n(c) No Safety Threshold\n(d) With Safety Threshold\nEdge\nEdge\nEdge\nEdge\nFigure 10: (a) The SubT Station platform had a sharp edge with a signiﬁcant drop. (b) GBPlanner2 was tuned to be\noptimistic and planned over the edge of the SubT Station platform. (c) Without a foothold safety margin the robot\nwould have stepped onto and possibly over the platform edge. (d) With foothold safety margin the ﬁnal path pose is\na safe distance from the platform edge. Yellow cubes represent our lidar map to indicate the actual location of the\nedge. The ANYmal model in (c)+(d) does not indicate the current robot pose, but rather is placed at the ﬁnal pose\nof the path to better show how close the robot would have stepped to the edge.\nobstacles. This can be attributed to the risk term cr in the cost function. The time cost ct had negligible\nimpact compared to the shortest path of the No Motion Cost planner, since the terrain present during the\nFinals was uniform enough such that the shortest path generally was also the fastest. However, ct was\nnecessary to condition the planning problem, since a pure risk cost would have lead to large detours to\nachieve minor risk improvements.\n3.2.2\nSafety Threshold Analysis\nThe safety threshold was introduced to handle negative obstacles. One carrier robot reached the Subway\nStation in autonomous exploration mode during the ﬁrst Preliminary Run of the Finals. The Subway Station\nhad a sharp drop with a wall a few meters behind, pictured in Figure 10(a). As discussed in Section 1.5, the\nexploration planner was tuned to be optimistic, and planned to explore into the free space above the train\ntracks (Figure 10(b)). With the wall visible behind the platform, both image inpainting and virtual surfaces\nwould have simply created ﬂat ground or a gentle slope such that we could not rely on motion cost for safety.\nReachability checking generally prevents the planner from planning over the edge, however, without a safety'
 'Reachability checking generally prevents the planner from planning over the edge, however, without a safety\nthreshold, the planned ﬁnal pose is dangerously close to the edge (Figure 10(c)). With the safety threshold\n(a) Onboard image and Lidar map.\n(b)\nSteppable\nregion\nwith height map only.\n(c)\nSteppable\nregion\nwith virtual surfaces.\nFigure 11: (a) The cave section had a narrow incline leading up to a Cube artifact. (b) Due to the low sensor height\nabove the uneven ground, the height map is too sparse to plan uphill. (c) Using virtual surfaces, the plannable area\nis vastly increased.\napplied, the robot only plans to a safe distance from the edge (Figure 10(d)).\nThe safety threshold therefore most likely prevented a severe fall of the robot during the ﬁrst Preliminary\nRun, which would have caused heavy damage to the payload on top of the robot and possibly the robot\nitself.\nThe safety margin parameters were well tuned and generally did not cause the robot to be overly cautious.\nWe only observed the safety margin to come into eﬀect around the tall railroad tracks shown in Figure 7\n.\nThese were just on the edge of being traversable and tall enough to cause occlusions in the height map. This\neﬀected a few unsmooth, but still safe paths when the exploration path crossed these tracks, but mostly\ncaused our planner to prefer staying in between the rail tracks, which was the safest approach, anyway.\n3.2.3\nVirtual Surfaces Analysis\nAs discussed in Section 2.4.1, we only use virtual surfaces above sensor height, due to safety concerns with\nnegative obstacles, demonstrated in Section 3.2.2. Since the Finals course was mostly planar, and did not\nhave multiple levels like the Urban Circuit, the only time virtual surfaces came into eﬀect were in the cave\nsection (located just to the left of Figure 7\n). Here, the course had a two-sided ramp with a very low\nceiling height, which lead to a platform with a Cube artifact, pictured in Figure 11(a). Due to the rough\nterrain on the incline, the height map contained many holes in front of the robot. This reduced the size of\nthe steppable area so much that planning would have become almost impossible, as shown in Figure 11(b).\nHere, Figure 11(c) shows how the virtual surfaces allowed us to plan up the incline, reach the platform, and\nscore the cube artifact.\nFigure 12: Corrupted height maps from ceiling lidar returns frequently slowed down progress by reducing the distance\nthe planner can plan ahead.\nThe exploration path leads into a low corridor which is observed as a wall in the height\nmap.\nThe fake wall shifts forward as the robot gets closer.\nThe incline reduces ceiling hits in front of the robot\nwhich helps move the fake wall farther from the robot.\nOnce fully on the slope, ceiling measurements immediately\nbehind the robot produce another fake wall.\n3.3\nIssues\nAlthough our planner performed very well during the Finals, it still encountered some issues related to other\nparts of the navigation stack.\n3.3.1']","Risk pruning removes edges from the planning graph that lead to risky areas, preventing the robot from attempting dangerous paths. Safety thresholds ensure that the robot plans to a safe distance from edges or dangerous areas, reducing the risk of falls or damage. Virtual surfaces are used to plan in areas with negative obstacles or uneven terrain, expanding the plannable area and enabling safe navigation.",reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}
 {'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
"What challenges did GBPlanner2 face in considering terrain traversability during the competition, and how did it compare to other planning methods in terms of motion cost and collision rate?","['happened at the stairs leading to the subway station. This resulted from our operational decision to use\nolder, well-tested motion cost network weights, which produced elevated risk levels on stairs. We preferred\nthese weights over newer, untested weights, which perform well on stairs, since stairs were not a prominent\nfeature in the Finals course. We only collided with the environment a single time, getting caught on a\nnarrow pole due to path following delays, discussed in further detail in Section 3.3.2. The only other issue\nencountered by our planning method was slow progression due to artifacts in the height map, as outlined in\nSection 3.3.1. Nevertheless, we safely explored large parts of the competition course, as shown in Figure 7.\n3.1\nComparison\nDirectly following GBPlanner2’s exploration path (Dang et al., 2019; Kulkarni et al., 2022) during the\ncompetition would almost certainly have lead to major issues. It did not suﬃciently account for traversability\ncharacteristics of the terrain, planning directly over high rails shown in Figure 7\n. It even completely missed\nsome smaller obstacles, like the traﬃc cones shown in Figure 7\n. This could have possibly been avoided with\nmore conservative tuning of the exploration planner, but this would have signiﬁcantly impeded exploration.\nTo show that ArtPlanner was crucial to performing well in the challenging environment of the SubT Finals,\nwe compare it to other navigation planning methods, using data collected during the Prize Run. Figure 7\nshows the motion cost and collision rate for all robots and all planners as a heat map overlayed over the\ntop-view of the competition course.\n3.1.1\nOther Planners\nNaturally, only ArtPlanner was running during the Finals. We recorded all paths planned during the Finals\nto evaluate performance post-event. To compare with other methods, we played back the state estimation,\nlocalization and height map data recorded during the Finals and input the exploration paths commanded\nduring the Finals to all planners. This allows us to somewhat reproduce behavior of the ﬁrst two components\nof the navigation stack, as shown in Figure 3, and therefore evaluate path quality. Note, however, that this\nmeans planning will always start from the competition robot pose, which is a result of following our planner.\nTherefore, planning always starts from a safe state and other intricacies which can lead to issues (see\nSection 3.3.2) cannot be reproduced. However, if planners show problematic behavior even in this simpliﬁed\nsetup, they are even more likely to fail during real deployment. To have a fair comparison to the other\nmethods, we also re-ran ArtPlanner on Finals data.\nWe evaluate the following methods:\n• ArtPlanner (Competition): Our method as run in the loop, on the robots, during the competition.\n• GBPlanner2: Exploration planner (Kulkarni et al., 2022) as run during the competition.\n• ArtPlanner (Playback): Our method using data from the competition played back.']","GBPlanner2 did not sufficiently account for traversability characteristics of the terrain during the competition. It planned directly over high rails and even missed some smaller obstacles like traffic cones. In terms of motion cost and collision rate, GBPlanner2 performed poorly compared to other planning methods.",multi_context,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does reachability checking and risk pruning prevent planning over risky areas?,"['even though the actual terrain was too rough and rocky to overcome. Using just reachability checking, our\nsampler found individual valid poses on the slope and connected them, as shown in Figure 9(a). Here, risk\npruning identiﬁed that moving on this slope was too risky and removed these edges, as shown in Figure 9(b).\nWithout risk pruning the robot would have tried to scale this slope, which would in the best case have lead\nto lost time, and in the worst case to loss of this robot.\nFigure 7\n+\nshow that the cost function generally lead to safer paths, which kept a safe distance from\n(a) Without risk pruning\n(b) With risk pruning\nFigure 9: Sampled valid poses shown in red with blue graph edges connecting them. Using reachability checking,\nvalid robot poses are still found on a steep and rocky incline in the cave section. Pruning graph edges based on\nmotion risk prevents the planning graph from spanning this risky area.\n(a) Onboard Image\n(b) Exploration Path\n(c) No Safety Threshold\n(d) With Safety Threshold\nEdge\nEdge\nEdge\nEdge\nFigure 10: (a) The SubT Station platform had a sharp edge with a signiﬁcant drop. (b) GBPlanner2 was tuned to be\noptimistic and planned over the edge of the SubT Station platform. (c) Without a foothold safety margin the robot\nwould have stepped onto and possibly over the platform edge. (d) With foothold safety margin the ﬁnal path pose is\na safe distance from the platform edge. Yellow cubes represent our lidar map to indicate the actual location of the\nedge. The ANYmal model in (c)+(d) does not indicate the current robot pose, but rather is placed at the ﬁnal pose\nof the path to better show how close the robot would have stepped to the edge.\nobstacles. This can be attributed to the risk term cr in the cost function. The time cost ct had negligible\nimpact compared to the shortest path of the No Motion Cost planner, since the terrain present during the\nFinals was uniform enough such that the shortest path generally was also the fastest. However, ct was\nnecessary to condition the planning problem, since a pure risk cost would have lead to large detours to\nachieve minor risk improvements.\n3.2.2\nSafety Threshold Analysis\nThe safety threshold was introduced to handle negative obstacles. One carrier robot reached the Subway\nStation in autonomous exploration mode during the ﬁrst Preliminary Run of the Finals. The Subway Station\nhad a sharp drop with a wall a few meters behind, pictured in Figure 10(a). As discussed in Section 1.5, the\nexploration planner was tuned to be optimistic, and planned to explore into the free space above the train\ntracks (Figure 10(b)). With the wall visible behind the platform, both image inpainting and virtual surfaces\nwould have simply created ﬂat ground or a gentle slope such that we could not rely on motion cost for safety.\nReachability checking generally prevents the planner from planning over the edge, however, without a safety'
 'Reachability checking generally prevents the planner from planning over the edge, however, without a safety\nthreshold, the planned ﬁnal pose is dangerously close to the edge (Figure 10(c)). With the safety threshold\n(a) Onboard image and Lidar map.\n(b)\nSteppable\nregion\nwith height map only.\n(c)\nSteppable\nregion\nwith virtual surfaces.\nFigure 11: (a) The cave section had a narrow incline leading up to a Cube artifact. (b) Due to the low sensor height\nabove the uneven ground, the height map is too sparse to plan uphill. (c) Using virtual surfaces, the plannable area\nis vastly increased.\napplied, the robot only plans to a safe distance from the edge (Figure 10(d)).\nThe safety threshold therefore most likely prevented a severe fall of the robot during the ﬁrst Preliminary\nRun, which would have caused heavy damage to the payload on top of the robot and possibly the robot\nitself.\nThe safety margin parameters were well tuned and generally did not cause the robot to be overly cautious.\nWe only observed the safety margin to come into eﬀect around the tall railroad tracks shown in Figure 7\n.\nThese were just on the edge of being traversable and tall enough to cause occlusions in the height map. This\neﬀected a few unsmooth, but still safe paths when the exploration path crossed these tracks, but mostly\ncaused our planner to prefer staying in between the rail tracks, which was the safest approach, anyway.\n3.2.3\nVirtual Surfaces Analysis\nAs discussed in Section 2.4.1, we only use virtual surfaces above sensor height, due to safety concerns with\nnegative obstacles, demonstrated in Section 3.2.2. Since the Finals course was mostly planar, and did not\nhave multiple levels like the Urban Circuit, the only time virtual surfaces came into eﬀect were in the cave\nsection (located just to the left of Figure 7\n). Here, the course had a two-sided ramp with a very low\nceiling height, which lead to a platform with a Cube artifact, pictured in Figure 11(a). Due to the rough\nterrain on the incline, the height map contained many holes in front of the robot. This reduced the size of\nthe steppable area so much that planning would have become almost impossible, as shown in Figure 11(b).\nHere, Figure 11(c) shows how the virtual surfaces allowed us to plan up the incline, reach the platform, and\nscore the cube artifact.\nFigure 12: Corrupted height maps from ceiling lidar returns frequently slowed down progress by reducing the distance\nthe planner can plan ahead.\nThe exploration path leads into a low corridor which is observed as a wall in the height\nmap.\nThe fake wall shifts forward as the robot gets closer.\nThe incline reduces ceiling hits in front of the robot\nwhich helps move the fake wall farther from the robot.\nOnce fully on the slope, ceiling measurements immediately\nbehind the robot produce another fake wall.\n3.3\nIssues\nAlthough our planner performed very well during the Finals, it still encountered some issues related to other\nparts of the navigation stack.\n3.3.1']","Reachability checking prevents the planner from planning over risky areas by identifying valid robot poses on the slope and connecting them. Risk pruning then removes these edges, preventing the planner from spanning the risky area.",multi_context,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}
 {'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
What is the concept of a robo-tangible learning environment and its relation to socially supportive behavior in robotic tutoring?,"['robot interaction: socially supportive behaviour of robotic tutor for\ncreating robo-tangible learning environment in a guided discovery\nlearning interaction. ECS Transactions 107, 12389.\n[3] Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünder-\nhauf, N., Reid, I., Gould, S., Van Den Hengel, A., 2018. Vision-\nand-language navigation: Interpreting visually-grounded navigation\ninstructions in real environments, in: IEEE Conference on Computer\nVision and Pattern Recognition, pp. 3674–3683.\n[4] Arkin, J., Park, D., Roy, S., Walter, M.R., Roy, N., Howard, T.M.,\nPaul, R., 2020. Multimodal estimation and communication of latent\nsemantic knowledge for robust execution of robot instructions. The\nInternational Journal of Robotics Research 39, 1279–1304.\n[5] Bagaria, A., Senthil, J.K., Konidaris, G., 2021. Skill discovery for\nexploration and planning using deep skill graphs, in: International\nConference on Machine Learning, PMLR. pp. 521–531.\n[6] Bahdanau, D., Cho, K., Bengio, Y., 2015. Neural machine transla-\ntion by jointly learning to align and translate, in: the International\nConference on Learning Representations.\n[7] Bermudez, L., .\nOverview of embodied artificial intelli-\ngence. https://medium.com/machinevision/overview-of-embodied-\nartificial-intelligence-b7f19d18022.\n[8] Bharadhwaj, H., Vakil, J., Sharma, M., Gupta, A., Tulsiani, S.,\nKumar, V., 2023.\nRoboAgent:: Generalization and efficiency in\nrobot manipulation via semantic augmentations and action chunk-\ning. arXiv preprint, arXiv:2309.01918 .\n[9] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choro-\nmanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al., 2023a.\nRT-2: Vision-language-action models transfer web knowledge to\nrobotic control. arXiv preprint, arXiv:2307.15818 .\n[10] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C.,\nGopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al., 2023b.\nRT-1: Robotics transformer for real-world control at scale. Robotics:\nScience and Systems XIX .\n[11] Brooks, R., 1986. A robust layered control system for a mobile robot.\nIEEE Journal on Robotics and Automation 2, 14–23.\n[12] Brooks, R.A., 1991. Intelligence without representation. Artificial\nIntelligence 47, 139–159.\n[13] Brown, P.F., Della Pietra, V.J., Desouza, P.V., Lai, J.C., Mercer, R.L.,\n1992. Class-based n-gram models of natural language. Computa-\ntional linguistics 18, 467–480.\n[14] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhari-\nwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.,\n2020. Language models are few-shot learners. Advances in Neural\nInformation Processing Systems 33, 1877–1901.\n[15] Brys, T., Harutyunyan, A., Taylor, M.E., Nowé, A., 2015. Policy\ntransfer using reward shaping., in: The International Conference on\nAutonomous Agents and Multiagent Systems, pp. 181–188.\n[16] Bucker, A., Figueredo, L., Haddadin, S., Kapoor, A., Ma, S.,\nVemprala, S., Bonatti, R., 2023.\nLATTE: Language trajectory']",The concept of a robo-tangible learning environment refers to creating a guided discovery learning interaction where a robotic tutor exhibits socially supportive behavior. This environment combines robotics and tangible interfaces to facilitate learning and engagement.,multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is the challenge in training robot action policies in diverse scenarios and how can it be addressed?,"['lonely individuals, much like a loyal companion. In fact,\nsome people even develop emotional attachments to their\nfirst car or a vehicle that has been with them for a long\ntime. If we were to create robots that resemble humans or\nexhibit human-like intelligence, would they evoke different\nemotions? In science fiction movies, robots that gain self-\nawareness and break free from their programming often\ndevelop emotions and even marry humans. Interestingly,\nrobots powered by LLMs have already demonstrated a de-\ngree of intelligence. Will they eventually become conscious?\nIf embodied intelligence evolves to possess consciousness,\nshould we still consider them tools? This raises questions\nabout the definition of conscious robots and whether they can\nbe considered human. Although this challenge is still far off\nin the future of smart robot development, it is an intriguing\ntopic to ponder.\n6. Promising Directions for Future Work\n6.1. Security of Task Executing\nSecurity has always been a pressing concern in various\nmodels, particularly with regard to user privacy. However,\nwe argue that the safety of agents during task execution\nis of paramount importance. In this article, we explore the\nquestion of whether an agent’s actions during task execution\ncould cause harm [98, 36]. For instance, consider a scenario\nwhere a robot is asked to make lunch, but in the process,\nit sets the kitchen on fire. In other scenes, imagine a robot\ntasked with killing fish, but it mistakenly identifies humans\nas fish and proceeds to chase and harm them. These sce-\nnarios highlight the need to limit the actions an agent can\nperform to prevent potential harm. Current robot systems\nfocus on enabling the robot to determine which actions can\nbe performed based on the current state and environment,\nwithout fully considering the consequences of executing\nthose actions. Therefore, we propose that ensuring the safety\nof task execution must be a top priority, by guaranteeing that\nthe robot’s actions do not harm human rights and interests.\n6.2. Training Scenario Transfer\nDue to technical or economic constraints, it is common\nto train robot action policies in simulated [30] or gaming\nenvironments [95]. However, the ultimate goal of agent\ntraining is to apply it in real-world scenarios. Unfortunately,\ntraining in diverse scenarios can lead to not being accli-\nmatized, which may compromise the agent’s performance\nwhen deployed in real-world situations. The fundamental\nsource of this problem can be attributed to the disparity\nof feedback mechanisms between simulated and real-world\nenvironments. In games or simulations, feedback is often\nmore straightforward, with the robot receiving clear and\nconcise information about the outcome of its actions. In\ncontrast, real-world feedback is more complex and nuanced,\nmaking it challenging to assess the feasibility of a task in\na limited scenario. Therefore, a valuable research direction\nis to explore methods for transferring model training across']","The challenge in training robot action policies in diverse scenarios is that training in diverse scenarios can lead to not being acclimatized, which may compromise the agent's performance when deployed in real-world situations. This problem can be addressed by exploring methods for transferring model training across different scenarios.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are the applications of large language models in robotics?,"['Large Language Models for Robotics: A Survey\nFanlong Zenga, Wensheng Gana,∗, Yongheng Wanga, Ning Liua and Philip S. Yub\naSchool of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China\nbDepartment of Computer Science, University of Illinois Chicago, Chicago, USA\nA R T I C L E I N F O\nKeywords:\nlarge language models\nrobotics\ncontrol and interaction\ndecision-making\nembodied intelligence\nA B S T R A C T\nThe human ability to learn, generalize, and control complex manipulation tasks through multi-\nmodality feedback suggests a unique capability, which we refer to as dexterity intelligence. Under-\nstanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field of robotics have\ngarnered increasing attention. LLMs possess the ability to process and generate natural language,\nfacilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of\nrobotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot\ninteraction, and autonomy. Therefore, this comprehensive review aims to summarize the applications\nof LLMs in robotics, delving into their impact and contributions to key areas such as robot control,\nperception, decision-making, and path planning. We first provide an overview of the background and\ndevelopment of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and\nrecent advancements in robotics models based on LLMs. We then delve into the various techniques\nused in the model, including those employed in perception, decision-making, control, and interaction.\nFinally, we explore the applications of LLMs in robotics and some potential challenges they may face\nin the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics\nis one of the promising but challenging paths to achieve this.\n1. Introduction\nHumans possess exceptional proficiency in executing\nintricate and dexterous manipulation skills by integrating\ntactile, visual, and other sensory inputs. Research in the\nfield of robotics aspires to imbue robots with comparable\nmanipulation intelligence. Although recent advancements\nin robotics and machine learning have yielded promising\nresults in visual mitigation and exploration learning for robot\nmanipulation, there remains much to be accomplished in this\narea. Large language models (LLMs), such as BERT [31],\nRoberta [79], GPT-3 [27], GPT-4 [110], have emerged as\nsignificant research achievements in the field of artificial\nintelligence (AI) in recent years. Through deep learning\ntechniques [76], LLMs can be trained on massive text cor-\npora, enabling them to generate high-quality natural lan-\nguage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology']","This comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are the challenges faced in real-scene training for robotics?,"['of ready-made, multi-modal datasets, and collecting them\nrequires a significant time investment. Moreover, policy\ncontrol is necessary, which includes the interaction between\nthe robot and its environment, necessitating 3D data [7].\nThe data required for robotics are diverse and scarce, with\npoor general applicability. For instance, a dataset used to\ntrain robot dogs cannot be applied to humanoid robots, and\na dataset used for screwing in an assembly line may not\nbe suitable for robots that assemble items. However, with\nthe emergence of platforms similar to X-embodiment6, the\nchallenges of dataset collection in robotics may be alleviated\nin the future.\n5.2. Training Scemes\nAs embodied intelligence necessitates interaction with\nthe physical environment, the model’s training requires spe-\ncific scenarios, e.g., distributed training [152]. Current re-\nsearch involves training robot-related models in various en-\nvironments, such as games [95], simulations [30], and real-\nworld scenarios [8]. Training in-game scenarios is straight-\nforward, with simple operations like button-pressing. How-\never, the knowledge gained from games may not translate\nwell to real-world scenarios, as the information in complex\nscenes varies greatly, and language models cannot provide a\nuniversal solution. Simulation environments aim to closely\nreplicate reality, with low energy consumption and cost.\nHowever, modeling real scenes in simulators can be nec-\nessary. While game and simulation environments can train\nmodels, they share a common issue: poor transferability to\nreal scenes. For instance, a model with 90% accuracy in a\ngame or simulation may only have 10% accuracy in a real\nscene. Real-scene training faces significant challenges, such\nas cost. In simulations, objects can be generated through\ncode [21], but in reality, purchasing them can be expensive.\nTransferring models between different training scenarios is\na significant challenge.\n6Open X-embodiment repository, a dataset consisting of different\nplatforms. https://robotics-transformer-x.github.io/\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 12 of 19\nLarge Language Models for Robotics: A Survey\n(a) Robotics with different shapes.\nSuper Brain\nLocal Brain\n(b) LLM development.\n(c) Modualize.\nFigure 4: Challenge in embodied intelligence.\n5.3. Shape\nCurrently, most work environments in human society\nare well-suited for humanoid robots. However, the question\narises whether robots must be human-shaped [57]. There\nare numerous types of robots currently in existence, each\nwith its unique capabilities and applications, like in Figure\n4(a). From an energy consumption perspective, wheels are\nmore energy-efficient than legs. Therefore, if a humanoid\nrobot is built, it may be inappropriate to use legs to move\nobjects instead of a conveyor belt. Similarly, a chef robot\nmay not need to hold a shovel and cook like a human. In\nmany cases, designing a pipeline tailored to the specific']","Real-scene training for robotics faces significant challenges, such as cost and the need to purchase objects instead of generating them through code. Transferring models between different training scenarios is also a significant challenge.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is a Vision-Language Model and how does it contribute to robotics?,"['on human society.\nOrganization: The rest of this article is organized as\nfollows. In Section 2.1, we discuss related concepts of the\nLLMs and robotics. In Section 2.3 we introduce the new\nrobot models equipped with LLMs in recent years. In Section\n3, we indicate the practical guide for technical points. We\nTable 1\nAbbreviation with its corresponding description\nAbbreviation\nDescription\nAI\nArtificial Intelligence\nGPT\nGenerative Pre-trained Transformer\nLLMs\nLarge-scale Language Models\nVNM\nVision-Navigation Model\nVLM\nVision-Language Model\nVLN\nVision-and-Language Navigation model\nVLA\nVision-Language-Action model\nalso introduce the application in Section 4. Moreover, We\nhighlight the challenges in Section 5 and present several\npromising directions of LLMs for robotics in Section 6.\nFinally, we conclude this paper in Section 7.\n2. Robotics Based on LLMs\nAmidst the swift progress and extensive proliferation of\nLLMs, the model of robotics based on LLMs has emerged.\nLLM serves as a robotics brain like in Figure 1, making\nit more intelligent. In this part, we first review the basic\nconcept of LLMs and the popular LLMs nowadays. After\nthat, we describe the benefits of robotics combined with\nLLM. Finally, we introduce the recent robotics model based\non LLMs and the Transformer designed for robotics below.\nWe also summarize the abbreviation used in this paper in\nTable 1 for convenience.\nRobotics\nLLM\nEmbodied Intelligence / Agents\nFigure 1: Robotics based on LLM.\n2.1. Language Model Overview\n2.1.1. Language Model Basics\nWe first provide an overview of LLM, starting with\nan introduction to some fundamental concepts. We then\ndelve into the history of LLM’s development, followed by\na brief discussion of its growing popularity in recent years.\nA language model is a computational model that utilizes\nstatistical methods to analyze and predict the probability\nof word sequences in a given language. It is designed to\ncapture the patterns, grammar, and semantic meaning of\nnatural language [92].\n• N-gram models are a simple form of language models\nthat calculate the probability of a word based on the\npreceding (n-1) words. They are widely used due to\ntheir simplicity and efficiency. The accuracy of the\nN-gram model is directly related to the length of the\ncontext used, with larger ‘n’ values leading to higher\naccuracy [13].\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 2 of 19\nLarge Language Models for Robotics: A Survey\n• Unigram models [147] are often employed for vari-\nous language processing tasks including information\nretrieval. It evaluates each word or term indepen-\ndently. It is calculated without considering any condi-\ntional context, only the probability of the current word\nitself appearing.\n• Bidirectional models differ from unidirectional mod-\nels, it analyzes text in both directions: backward and\nforward. This dual approach is commonly employed\nin various machine learning models and speech gen-\neration applications. Bidirectional models harness the']","A Vision-Language Model (VLM) is a model that combines visual perception and natural language understanding to enable robots to understand and interact with their environment. It contributes to robotics by allowing robots to process and interpret visual information, understand human commands and queries, and generate appropriate responses or actions based on the context.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
How does image erosion reduce the steppable map region in the height map implementation in the navigation planner?,"['ure 6\n. It is implemented using image erosion on the foothold score layer of the height map, which reduces\nthe steppable map region by a safety margin. This has the additional beneﬁt of also removing small isolated\nsteppable patches from the map, which could only be overcome by solving a stepping-stone problem. To\navoid unnecessarily inﬂating small obstacles like rails, which the robot can easily step over, we do not inﬂate\nunsteppable regions below a certain size. In practice this is done by performing an image dilation of smaller\nradius, before doing the erosion.\nThis safety threshold was crucial on the Subway Station of the Finals circuit, as shown in Section 3.2.2.\n2.4.3\nCeiling Point Filter\nWe use a 2.5D height map representation for planning. While this is suﬃcient for ground robot navigation\nin most environments, it can be problematic in the tight underground spaces encountered during SubT. Low\nceilings mean that they are frequently observed by the depth sensors, which causes spikes in the height map,\nas shown in Figure 2(b). However, we cannot simply discard all points above a ﬁxed height, since this would\neither prevent us from planning up slopes or from passing underneath low overhangs.\nWe therefore use a rising height threshold to ﬁlter points (Miki et al., 2022b), shown in Figure 6\n. It\nﬁlters points just above robot height close to the robot, and linearly increases the height threshold up to a\nmaximum at larger distances. This setup caused map spikes in parts of the course with low ceilings which\nslowed us down, but these crucially never stopped us from exploring. It allowed us to pass underneath very\nlow overhangs, and to plan up slopes, even when encountered together, as detailed in Section 3.3.1.\n3\nExperimental Results\nArtPlanner was deployed on all four ANYmal-C ground robots of team CERBERUS during all runs of the\nSubT Finals. It used a height map of size 8 m×8 m with a 4 cm resolution. We only cover results related\nto the navigation planner presented in this work. For further details on the general performance we refer to\nour overview article (Tranzatto et al., 2022a).\nWe deployed all four ground robots during the Prize Run, which were directed by the supervisor to explore\ndiﬀerent areas of the course. All ground robots successfully made it to the end of the competition and\nwe did not observe a single path planning or locomotion failure, which could have been provoked by bad\npath planning. Our planner was active for 90 minutes between all robots which accounts for 88.94% of all\nrobot motion. We gracefully navigated the narrow doorways and small rooms in the Urban section, passed\nthrough the Tunnel section with obscuring fog, and made it through the narrowest and roughest part of the\nCave section. The only case where ArtPlanner did not follow the exploration path over traversable terrain\nhappened at the stairs leading to the subway station. This resulted from our operational decision to use']",Image erosion reduces the steppable map region in the height map implementation by applying erosion on the foothold score layer. This reduces the region by a safety margin and also removes small isolated steppable patches from the map.,reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
"What are the potential applications of large language models in robotics and how do they contribute to robot control, perception, decision-making, and path planning?","['Large Language Models for Robotics: A Survey\nFanlong Zenga, Wensheng Gana,∗, Yongheng Wanga, Ning Liua and Philip S. Yub\naSchool of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China\nbDepartment of Computer Science, University of Illinois Chicago, Chicago, USA\nA R T I C L E I N F O\nKeywords:\nlarge language models\nrobotics\ncontrol and interaction\ndecision-making\nembodied intelligence\nA B S T R A C T\nThe human ability to learn, generalize, and control complex manipulation tasks through multi-\nmodality feedback suggests a unique capability, which we refer to as dexterity intelligence. Under-\nstanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field of robotics have\ngarnered increasing attention. LLMs possess the ability to process and generate natural language,\nfacilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of\nrobotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot\ninteraction, and autonomy. Therefore, this comprehensive review aims to summarize the applications\nof LLMs in robotics, delving into their impact and contributions to key areas such as robot control,\nperception, decision-making, and path planning. We first provide an overview of the background and\ndevelopment of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and\nrecent advancements in robotics models based on LLMs. We then delve into the various techniques\nused in the model, including those employed in perception, decision-making, control, and interaction.\nFinally, we explore the applications of LLMs in robotics and some potential challenges they may face\nin the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics\nis one of the promising but challenging paths to achieve this.\n1. Introduction\nHumans possess exceptional proficiency in executing\nintricate and dexterous manipulation skills by integrating\ntactile, visual, and other sensory inputs. Research in the\nfield of robotics aspires to imbue robots with comparable\nmanipulation intelligence. Although recent advancements\nin robotics and machine learning have yielded promising\nresults in visual mitigation and exploration learning for robot\nmanipulation, there remains much to be accomplished in this\narea. Large language models (LLMs), such as BERT [31],\nRoberta [79], GPT-3 [27], GPT-4 [110], have emerged as\nsignificant research achievements in the field of artificial\nintelligence (AI) in recent years. Through deep learning\ntechniques [76], LLMs can be trained on massive text cor-\npora, enabling them to generate high-quality natural lan-\nguage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology']","This comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning.",reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"How does emotional attachment to inanimate objects relate to robot safety, training, dialogue consistency, social influence, and ethics in embodied intelligence?","['lonely individuals, much like a loyal companion. In fact,\nsome people even develop emotional attachments to their\nfirst car or a vehicle that has been with them for a long\ntime. If we were to create robots that resemble humans or\nexhibit human-like intelligence, would they evoke different\nemotions? In science fiction movies, robots that gain self-\nawareness and break free from their programming often\ndevelop emotions and even marry humans. Interestingly,\nrobots powered by LLMs have already demonstrated a de-\ngree of intelligence. Will they eventually become conscious?\nIf embodied intelligence evolves to possess consciousness,\nshould we still consider them tools? This raises questions\nabout the definition of conscious robots and whether they can\nbe considered human. Although this challenge is still far off\nin the future of smart robot development, it is an intriguing\ntopic to ponder.\n6. Promising Directions for Future Work\n6.1. Security of Task Executing\nSecurity has always been a pressing concern in various\nmodels, particularly with regard to user privacy. However,\nwe argue that the safety of agents during task execution\nis of paramount importance. In this article, we explore the\nquestion of whether an agent’s actions during task execution\ncould cause harm [98, 36]. For instance, consider a scenario\nwhere a robot is asked to make lunch, but in the process,\nit sets the kitchen on fire. In other scenes, imagine a robot\ntasked with killing fish, but it mistakenly identifies humans\nas fish and proceeds to chase and harm them. These sce-\nnarios highlight the need to limit the actions an agent can\nperform to prevent potential harm. Current robot systems\nfocus on enabling the robot to determine which actions can\nbe performed based on the current state and environment,\nwithout fully considering the consequences of executing\nthose actions. Therefore, we propose that ensuring the safety\nof task execution must be a top priority, by guaranteeing that\nthe robot’s actions do not harm human rights and interests.\n6.2. Training Scenario Transfer\nDue to technical or economic constraints, it is common\nto train robot action policies in simulated [30] or gaming\nenvironments [95]. However, the ultimate goal of agent\ntraining is to apply it in real-world scenarios. Unfortunately,\ntraining in diverse scenarios can lead to not being accli-\nmatized, which may compromise the agent’s performance\nwhen deployed in real-world situations. The fundamental\nsource of this problem can be attributed to the disparity\nof feedback mechanisms between simulated and real-world\nenvironments. In games or simulations, feedback is often\nmore straightforward, with the robot receiving clear and\nconcise information about the outcome of its actions. In\ncontrast, real-world feedback is more complex and nuanced,\nmaking it challenging to assess the feasibility of a task in\na limited scenario. Therefore, a valuable research direction\nis to explore methods for transferring model training across'
 '5.6. Dialogue Consistency\nHumans often don’t complete tasks in a single, static\nstep. Instead, they iteratively adjust strategies and goals\nbased on feedback received after taking action. The same\nis true for embodied intelligence. When faced with high-\nlevel, abstract, or ambiguous commands, robots may not\nbe able to decompose them into executable small tasks at\nfirst. They need to obtain further feedback from the envi-\nronment and humans through continuous dialogue to update\ntheir goals. Without this ability to engage in continuous\ndialogue, which enables robots to perform tasks dynami-\ncally, their performance will be significantly impaired [120].\nMoreover, the maximum length limit of a robot’s context\nis another issue worth considering. Typically, embodied\nintelligence may play a housekeeper role, handling daily\ntasks like washing dishes or drying clothes. However, for\nlong-term tasks like scientific research, robots require more\ncontext-understanding capabilities. Currently, there’s a limit\nto the length of context that robots can handle, and this\nlimitation can lead to catastrophic forgetting [68]. Dialogue\npersistence is a crucial challenge for long-term tasks.\n5.7. Social Influence\nThe rapid advancement of LLMs is bringing the era of\nembodied intelligence, as depicted in science fiction movies\nand games, closer to reality. This technological breakthrough\nwill undoubtedly revolutionize human society and unleash\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 13 of 19\nLarge Language Models for Robotics: A Survey\nunprecedented productivity. With robots capable of perform-\ning repetitive tasks, the need for human labor in various in-\ndustries will diminish. However, this shift may also have far-\nreaching consequences, potentially disrupting social struc-\ntures and stability [50]. As robots replace low-end manual\nlabor, it raises questions about the fate of those who previ-\nously held these jobs. The double-edged sword of embodied\nintelligence presents both liberation and disruption. While\nautomation may usher in unprecedented efficiency, it also\nposes challenges for societal adaptation. Some works of\nscience fiction, such as Detroit Become Human, depict a\nfuture where robots gain consciousness and conflict with\nhumans, leading to a war between the two. Alternatively,\ntechnology may fall into the wrong hands, becoming a tool\nfor exploitation and solidifying class divisions. However, in\na worst-case scenario, robots may become a replacement\nfor humans. As we embrace the development of embodied\nintelligence, we must also confront the ethical and societal\nimplications it entails.\n5.8. Ethic\nEmbodied intelligence has long been regarded as a mere\ntool, but it may hold more significance in the eyes of some\nusers. For instance, companion robots can bring solace to\nlonely individuals, much like a loyal companion. In fact,\nsome people even develop emotional attachments to their\nfirst car or a vehicle that has been with them for a long']","Emotional attachment to inanimate objects is relevant to robot safety, training, dialogue consistency, social influence, and ethics in embodied intelligence. It can impact how users interact with robots and their expectations of robot behavior. It may also raise concerns about the potential for exploitation or harm if robots are given too much autonomy or influence in human society. Additionally, emotional attachment to robots may have implications for the ethical treatment of robots and the responsibilities of developers and users in ensuring their well-being.",multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}
 {'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"How does dexterity intelligence contribute to robot control, perception, decision-making, and path planning in robotics?","['Large Language Models for Robotics: A Survey\nFanlong Zenga, Wensheng Gana,∗, Yongheng Wanga, Ning Liua and Philip S. Yub\naSchool of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China\nbDepartment of Computer Science, University of Illinois Chicago, Chicago, USA\nA R T I C L E I N F O\nKeywords:\nlarge language models\nrobotics\ncontrol and interaction\ndecision-making\nembodied intelligence\nA B S T R A C T\nThe human ability to learn, generalize, and control complex manipulation tasks through multi-\nmodality feedback suggests a unique capability, which we refer to as dexterity intelligence. Under-\nstanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field of robotics have\ngarnered increasing attention. LLMs possess the ability to process and generate natural language,\nfacilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of\nrobotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot\ninteraction, and autonomy. Therefore, this comprehensive review aims to summarize the applications\nof LLMs in robotics, delving into their impact and contributions to key areas such as robot control,\nperception, decision-making, and path planning. We first provide an overview of the background and\ndevelopment of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and\nrecent advancements in robotics models based on LLMs. We then delve into the various techniques\nused in the model, including those employed in perception, decision-making, control, and interaction.\nFinally, we explore the applications of LLMs in robotics and some potential challenges they may face\nin the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics\nis one of the promising but challenging paths to achieve this.\n1. Introduction\nHumans possess exceptional proficiency in executing\nintricate and dexterous manipulation skills by integrating\ntactile, visual, and other sensory inputs. Research in the\nfield of robotics aspires to imbue robots with comparable\nmanipulation intelligence. Although recent advancements\nin robotics and machine learning have yielded promising\nresults in visual mitigation and exploration learning for robot\nmanipulation, there remains much to be accomplished in this\narea. Large language models (LLMs), such as BERT [31],\nRoberta [79], GPT-3 [27], GPT-4 [110], have emerged as\nsignificant research achievements in the field of artificial\nintelligence (AI) in recent years. Through deep learning\ntechniques [76], LLMs can be trained on massive text cor-\npora, enabling them to generate high-quality natural lan-\nguage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology']","This comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning.",multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is the purpose of ArtPlanner in the navigation stack?,"['1.5\nNavigation Stack\nIn the context of SubT, ArtPlanner was embedded into a larger navigation stack, shown in Figure 3, to\nprovide capabilities for autonomous exploration, and to actually follow computed paths. These components\nwere connected by a behavior tree to provide robustness and enable direct goal input to the navigation\nplanner from the operator.\nDetails of this are out of scope of this work and can be obtained from the\noverview article of team CERBERUS (Tranzatto et al., 2022a).\nOur graph-based exploration planner (Dang et al., 2019; Kulkarni et al., 2022) (GBPlanner2) plans to\nmaximize information gain along the robot path.\nIt operates on a global 20cm voxel-size volumetric\nmap (Oleynikova et al., 2017) which is too sparse to capture the terrain in suﬃcient detail for legged robot\nnavigation. Knowing that we had ArtPlanner to provide safe navigation, we also tuned the parameters of\nGBPlanner2 to be optimistic and favor exploration gain over safety considerations. Speciﬁcally, this meant\nwe used a small collision volume for validity checking without any safety margin to ﬁt through narrow open-\nings. Additionally, similar to the concept of virtual surfaces (Hines et al., 2021), GBPlanner2 is allowed to\noutput so called “hanging vertices”, where vertices in free space are not supported by any ground surface\nunderneath, but only unobserved space (see Section 3.2.2). We therefore need our navigation planner to\nExploration Planner (GBPlanner2)\nNavigation Planner (ArtPlanner)\nPure-Pursuit Path Follower\nPlan to farthest\nexploration pose\nSet carrot\npoint along path\nExploration\nPath\nNavigation\nPath\nVelocity\nCommand\nFigure 3: Team CERBERUS’ navigation stack. An exploration planner (Kulkarni et al., 2022) plans on a coarse\nvolumetric map to maximize information gain. ArtPlanner then plans to the farthest reachable exploration pose on\na more ﬁne-grained robo-centric height map. Navigation paths are tracked using a pure-pursuit path follower.\nreﬁne the exploration path in cases where its low-resolution map causes suboptimal or risky paths and to\nstop the robot if the path is completely infeasible. For this, we plan on a local height map which is centered\nat the current robot positions and moves with it.\nWhen ArtPlanner receives a new exploration path, it iterates through the path poses in reverse, starting\nwith the farthest one, and tries to plan to each pose. This is done to maximize the planning distance, such\nthat ArtPlanner can optimize the path and circumvent any obstacles which might have been missed by the\nexploration planner. We continuously repeat this at a target planning rate of 0.5Hz. When we successfully\nplan to an exploration path pose, all path poses which precede it are considered reached and are not used for\nfuture planning iterations. Planning of the GBPlanner2 is triggered in two cases: Either if the exploration\npath is infeasible and therefore contains no pose we can plan to. Or, if the last pose in the exploration path']",ArtPlanner is embedded into a larger navigation stack to provide capabilities for autonomous exploration and to follow computed paths. It is connected to other components in the stack through a behavior tree and enables direct goal input to the navigation planner from the operator.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does the memory mechanism aid in the functioning of LLM-based robots?,"['and planning abilities [138]. Additionally, LLMs can parse\nhigh-level abstract instructions to perform complex tasks\nwithout requiring step-by-step guidance5, and their human-\nlike text-generation capabilities make them highly effective\ncommunicators [46]. Furthermore, LLMs can sense their\nenvironment [44], and technologies that expand their action\nspace allow them to interact with the physical environment\nand complete tasks [149, 156]. They also possess reasoning\nand planning capabilities, such as logical and mathematical\nreasoning [134, 138], task decomposition [154], and plan-\nning [143] for specific tasks. LLM-based agents have been\nused in various real-world scenarios [77, 97] and have shown\npotential for multi-agent interactions and social capabilities.\n5BabyAGI https://github.com/yoheinakajima/babyagi\nOverall, LLMs have revolutionized the field of artificial\nintelligence and hold great promise for future advancements.\n3.2.2. Capacity of LLM in robotics\nLLM serves as the brain of the robot, functioning as the\ncentral component that integrates knowledge, memory, and\nreasoning capabilities to enable the robot to plan and execute\ntasks intelligently.\nKnowledge. The knowledge of LLM for robotics can\nbe categorized into two types: the knowledge that needs\nto be acquired through learning (which is the pre-trained\ndataset) and the knowledge that has been learned and stored\nin memory [142].\n• Pre-trained data. There are various types of pre-\ntrained datasets available, and the more extensive\nand richer the knowledge learned, the stronger the\nLLM’s generalization and natural language under-\nstanding capabilities will be [106]. Theoretically, the\nmore a language model learns, the more parameters\nit has, enabling it to learn complex knowledge in\nnatural language and gain powerful capabilities [65].\nResearch has shown that a richer dataset for language\nmodel learning can result in correct answers to di-\nverse questions [106]. Datasets can be categorized\ninto different types, such as basic semantic knowledge,\nwhich provides an understanding of language mean-\ning [133]; Common sense, including everyday facts\nlike people eating when hungry or the sun rising in\nthe east [108]; Professional field knowledge, which\ncan aid humans in completing tasks like programming\n[146] and mathematics [24].\n• Memory. Just like human memory, embodied intel-\nligence should be able to formulate strategies and\nmake decisions for new tasks based on experiences\n(i.e., observed actions, thoughts, etc.). When faced\nwith complex tasks, the memory mechanism can aid\nin reviewing past strategies to obtain more effective\nsolutions [56, 121]. However, memory poses some\nchallenges, such as the length of memory sequences\nand how to efficiently store and index them as the\nnumber of memories grows. As the robot’s mem-\nory burden increases over time, it must be able to\neffectively manage and retrieve memories to avoid\ncatastrophic forgetfulness [68].']",The memory mechanism aids in the functioning of LLM-based robots by allowing them to review past strategies and make decisions based on experiences. It helps the robot formulate effective solutions for complex tasks and manage and retrieve memories to avoid catastrophic forgetfulness as the number of memories grows.,simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is the proposed solution for efficiently handling diverse input formats in agents reliant on multi-modal perception?,"['making it challenging to assess the feasibility of a task in\na limited scenario. Therefore, a valuable research direction\nis to explore methods for transferring model training across\ndifferent scenarios while maintaining their accuracy in the\noriginal training environments.\n6.3. Unify Format of Modal\nCurrently, many models are utilizing LLM as the robot’s\nbrain, and text-type data is typically the input that LLM\naccepts. However, for agents reliant on multi-modal per-\nception, efficiently handling diverse input formats poses a\nsignificant challenge. To address this issue, a VLA model\nhas been proposed [9], which uniformly converts visual\nand natural language multi-modal inputs into multi-modal\nsentences for processing, and outputs actions in the same for-\nmat. In other words, multi-modal statements are employed to\nharmonize input and output. Nevertheless, there is currently\nno unified processing for other modalities such as touch and\nsmell. It is anticipated that unified multi-modal models like\nVLA will gain popularity in the future.\n6.4. Modular Components\nAs previously discussed, the field of robotics currently\nlacks a unified approach to robot design, with varying opin-\nions on the matter. We believe that there should be a mod-\nular design method, wherein each part of the robot can be\nswapped out like a machine, just like in Figure 4(c), allowing\nfor greater versatility and adaptability8. To achieve this,\nwe must first establish unified specifications for the various\nmodules of the robot. For instance, a robot can be composed\nof a head, torso, upper limbs, and lower limbs, with the upper\nlimbs and lower limbs being interchangeable based on the\ntask at hand. Among them, the upper limbs and lower limbs\ncan be replaced according to specific tasks. When we need\nto cook, we can use our upper limbs as a shovel, and when\n8https://www.agibot.com\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 14 of 19\nLarge Language Models for Robotics: A Survey\nwe need to deal with weeds in the yard, we can use our lower\nlimbs as a weeder.\n6.5. Autonomous Perception\nOur current research focuses on developing robots that\ncan interact with humans using natural language instruc-\ntions. In many cases, we study how humans issue instruc-\ntions and how robots can decompose abstract tasks into\nspecific sub-tasks for execution [1]. However, we also hope\nthat robots can perceive and respond autonomously to handle\nour current needs. For instance, if our cup falls to the ground\nand breaks, an agent should be able to perceive the situation\nthrough hearing and vision, and then autonomously handle\nthe glass fragments for us. Autonomous perception requires\nthe robot to have common sense, which is a capability that\ncan be integrated into robots based on LLM as the brain.\nResearch on robots’ autonomous perception capabilities is\ncrucial for improving our quality of life in the future.\n7. Conclusions\nIn this survey, we summarized the methods and tech-']","A proposed solution for efficiently handling diverse input formats in agents reliant on multi-modal perception is the use of a VLA model. This model uniformly converts visual and natural language multi-modal inputs into multi-modal sentences for processing, and outputs actions in the same format. However, there is currently no unified processing for other modalities such as touch and smell.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What were the characteristics and results of the Exploration Path with Cost Optimizer in the SubT Finals Prize Run?,"['• GBPlanner2: Exploration planner (Kulkarni et al., 2022) as run during the competition.\n• ArtPlanner (Playback): Our method using data from the competition played back.\n• No Motion Cost: A reachability-only planner (Wellhausen and Hutter, 2021), which we fed the\nheight map processed as proposed in this work.\n• Motion Cost Planner: Planner based purely on the motion cost network (Yang et al., 2021a). It ﬁrst\ncomputes a raw path, by querying motions in a ﬁxed graph pattern, with heuristics to determine\nrobot orientation.\nThis raw path is then optimized with gradient-based optimization using the\nmotion cost network.\n• Exploration Path w/ Cost Optimizer: GBPlanner2 (Kulkarni et al., 2022) already outputs a path\nwhich is mostly generally feasible, but optimizes for information gain and plans on a coarser map.\nWe therefore fed the exploration path directly into the motion cost optimizer (Yang et al., 2021a)\nto obtain a reﬁned navigation path.\n3.1.2\nMethodology\nWe used the motion cost network to compute path costs for all path segments inside the height map. The\nsame network architecture and weights were used for evaluation and in the planners which use the cost\nnetwork themselves.\nSince the planners mentioned above have diﬀerent characteristics, implementation details, and produce paths\nof diﬀerent lengths, we took extra considerations to guarantee a fair comparison. We check height map\ncollisions only for the torso of the robot, analogous to the blue boxes shown in Figure 4(a). We also reduced\nthe size of these boxes by 10cm in all dimensions compared to the size used by the reachability planners.\nThis was done to account for the more accurate collision models used when training the motion cost network.\nFinally, we disregarded the ﬁrst pose of all output paths, since it corresponds to the current robot pose.\nThis means that it has to be valid and any detected collisions would be erroneous. The reachability planners\naccount for this by sampling a valid pose in a small region around the start, but the motion cost planners\ndo not have that functionality. When evaluating the Exploration Path, we only consider path poses from the\nNo Motion Cost [1]\nArtPlanner (Playback)\nMotion Cost\nPlanner [2]\nExploration Path [3] \nw/ Cost Optimizer [2]\nExploration Path [3]\nArtPlanner\n(Competition)\nCompetition\nPlayback\n0.0\n0.5\nCost\n3%\n0%\nCollision Rate\n3\n5\n5\n5\n3\n7\n5\n4\n1\n6\n6\n6\n2\n1\n2\n7\n4\n1\n2\n7\n3 5\n6\n4\nFigure 7: Comparison of path costs and path collision rate for diﬀerent planners during the SubT Finals Prize Run.\nThe two left-most columns show data from the planners as run on the robots during the competition, while the other\ncolumns show real robot data played back. Data is smoothed with a Gaussian ﬁlter with standard deviation of 2m\nfor better visibility.\nThe Exploration Path was highly risky whereas ArtPlanner avoids the high rail track.\nThe\nExploration Path was infeasible as it ignored the obstructing traﬃc cones.\nWith No Motion Cost the path was very']",The Exploration Path with Cost Optimizer in the SubT Finals Prize Run was highly risky and infeasible as it ignored obstructing traffic cones.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does the Motion Cost Planner use motion cost in its planning process?,"['exceeding the real-time threshold only a single time, by 0.21 s. The No Motion Cost planner has fast query\ntimes in the median. Note that these times do not include the graph sampling time, because this method\nuses a persistent graph which is built over time, in-between planning queries. If we added this sampling\ntime the median performance would be comparable to ArtPlanner. Unfortunately, this method can produce\nArtPlanner (Playback)\nNo Motion Cost\nMotion Cost Planner\nExploration Path\nw/ Cost Optimizer\n0\n5\n25\nSevere Outlier\n2\nTarget Update Rate\nReal-Time Threshold\nFigure 8: While ArtPlanner plans slower than the compared methods on average, its computation time is bounded.\nThis avoids severe planning time outliers the No Motion Cost planner produces in rare cases. The Motion Cost\nPlanner is fast because it checks a ﬁxed planning graph pattern and therefore does not perform any pose sampling.\nThe Cost Optimizer only does a few steps of gradient-descent which makes it the fastest method in our comparison.\nsevere planning time outliers on rare occasion due to the graph validation at query time, as discussed in\nSection 2.3. We observed a time of 26.53 s in the data we used to generate Figure 8 but observed even longer\ntimes in other instances. This shows the beneﬁt of our graph validation method through batch motion cost\nquery. We can limit our planning time and can therefore avoid the planner being unresponsive for a long\nperiod of time. The Motion Cost Planner uses a ﬁxed planning graph and therefore only has to perform a\nbatch motion cost query, but no sampling. Since the optimization stage also always performs a ﬁxed number\nof iterations, planning is fast and the target time can be reached in all cases. Since the Exploration Path w/\nCost Optimizer only deploys the optimization stage of the Motion Cost Planner it is even faster.\nOverall, all evaluated planners would be fast enough for real-time operation in the nominal case. However,\nthe worst-case time of the No Motion Cost planner is too long to deploy it in a competition environment.\n3.2\nComponent Analysis\nWe now investigate the eﬀect individual components of our method had on the performance during the\nFinals.\n3.2.1\nMotion Cost Analysis\nWe used the motion cost in two ways, to prune the planning graph based on motion risk, and to optimize\nthe cost function for both risk and time.\nSince the terrain during the Finals was generally ﬂat and easy to traverse for our locomotion controller (Miki\net al., 2022a) in most sections, risk pruning did not have a large eﬀect. We observed the biggest eﬀect in the\nlarge hall of the Cave section, which housed a rocky incline (bottom right of map in Figure 7). Since the\ninclination of the terrain was in principle low enough to climb it, GBPlanner2 wanted to explore up the slope,\neven though the actual terrain was too rough and rocky to overcome. Using just reachability checking, our']",The Motion Cost Planner uses motion cost in two ways: to prune the planning graph based on motion risk and to optimize the cost function for both risk and time.,simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does the path follower component in team CERBERUS' system refine the exploration path in cases of suboptimal or risky paths and stop the robot if the path is completely infeasible?,"['1.5\nNavigation Stack\nIn the context of SubT, ArtPlanner was embedded into a larger navigation stack, shown in Figure 3, to\nprovide capabilities for autonomous exploration, and to actually follow computed paths. These components\nwere connected by a behavior tree to provide robustness and enable direct goal input to the navigation\nplanner from the operator.\nDetails of this are out of scope of this work and can be obtained from the\noverview article of team CERBERUS (Tranzatto et al., 2022a).\nOur graph-based exploration planner (Dang et al., 2019; Kulkarni et al., 2022) (GBPlanner2) plans to\nmaximize information gain along the robot path.\nIt operates on a global 20cm voxel-size volumetric\nmap (Oleynikova et al., 2017) which is too sparse to capture the terrain in suﬃcient detail for legged robot\nnavigation. Knowing that we had ArtPlanner to provide safe navigation, we also tuned the parameters of\nGBPlanner2 to be optimistic and favor exploration gain over safety considerations. Speciﬁcally, this meant\nwe used a small collision volume for validity checking without any safety margin to ﬁt through narrow open-\nings. Additionally, similar to the concept of virtual surfaces (Hines et al., 2021), GBPlanner2 is allowed to\noutput so called “hanging vertices”, where vertices in free space are not supported by any ground surface\nunderneath, but only unobserved space (see Section 3.2.2). We therefore need our navigation planner to\nExploration Planner (GBPlanner2)\nNavigation Planner (ArtPlanner)\nPure-Pursuit Path Follower\nPlan to farthest\nexploration pose\nSet carrot\npoint along path\nExploration\nPath\nNavigation\nPath\nVelocity\nCommand\nFigure 3: Team CERBERUS’ navigation stack. An exploration planner (Kulkarni et al., 2022) plans on a coarse\nvolumetric map to maximize information gain. ArtPlanner then plans to the farthest reachable exploration pose on\na more ﬁne-grained robo-centric height map. Navigation paths are tracked using a pure-pursuit path follower.\nreﬁne the exploration path in cases where its low-resolution map causes suboptimal or risky paths and to\nstop the robot if the path is completely infeasible. For this, we plan on a local height map which is centered\nat the current robot positions and moves with it.\nWhen ArtPlanner receives a new exploration path, it iterates through the path poses in reverse, starting\nwith the farthest one, and tries to plan to each pose. This is done to maximize the planning distance, such\nthat ArtPlanner can optimize the path and circumvent any obstacles which might have been missed by the\nexploration planner. We continuously repeat this at a target planning rate of 0.5Hz. When we successfully\nplan to an exploration path pose, all path poses which precede it are considered reached and are not used for\nfuture planning iterations. Planning of the GBPlanner2 is triggered in two cases: Either if the exploration\npath is infeasible and therefore contains no pose we can plan to. Or, if the last pose in the exploration path']","The path follower component in team CERBERUS' system refines the exploration path in cases of suboptimal or risky paths by planning on a local height map centered at the current robot position. It iterates through the path poses in reverse, starting with the farthest one, and tries to plan to each pose to optimize the path and circumvent any obstacles that might have been missed by the exploration planner. If the path is completely infeasible, the robot is stopped.",reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
"What is the main focus of research on human-assisted continual robot learning, with regards to foundation models and hardware-level reusability?","['review of control, robotics, and autonomous systems, vol. 4, pp. 265–\n293, 2021.\n[41] M. Parakh, A. Fong, A. Simeonov, A. Gupta, T. Chen, and P. Agrawal,\n“Human-assisted continual robot learning with foundation models,”\narXiv preprint arXiv:2309.14321, 2023.\n[42] S. S. Raman, V. Cohen, D. Paulius, I. Idrees, E. Rosen, R. Mooney, and\nS. Tellex, “Cape: Corrective actions from precondition errors using large\nlanguage models,” in 2nd Workshop on Language and Robot Learning:\nLanguage as Grounding, 2023.\n[43] X. Zhou, R. Girdhar, A. Joulin, P. Krähenbühl, and I. Misra, “Detecting\ntwenty-thousand classes using image-level supervision,” in ECCV, 2022.\n[44] J. J. Gibson, The ecological approach to visual perception: classic\nedition. Psychology press, 2014.\n[45] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, “Voxposer:\nComposable 3d value maps for robotic manipulation with language\nmodels,” arXiv preprint arXiv:2307.05973, 2023.\n[46] K. Ikeuchi, J. Takamatsu, K. Sasabuchi, N. Wake, and A. Kanehiro,\n“Applying learning-from-observation to household service robots: three\ncommon-sense formulation,” arXiv preprint arXiv:2304.09966, 2023.\n[47] N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi,\n“Interactive task encoding system for learning-from-observation,” in\n2023 IEEE/ASME International Conference on Advanced Intelligent\nMechatronics (AIM), pp. 1061–1066, 2023.\n[48] N. Wake, D. Saito, K. Sasabuchi, H. Koike, and K. Ikeuchi, “Text-driven\nobject affordance for guiding grasp-type recognition in multimodal robot\nteaching,” Machine Vision and Applications, vol. 34, no. 4, p. 58, 2023.\n[49] K. Sasabuchi, N. Wake, and K. Ikeuchi, “Task-oriented motion mapping\non robots of various configuration using body role division,” IEEE\nRobotics and Automation Letters, vol. 6, no. 2, pp. 413–420, 2020.\n[50] N. Wake, R. Arakawa, I. Yanokura, T. Kiyokawa, K. Sasabuchi,\nJ. Takamatsu, and K. Ikeuchi, “A learning-from-observation frame-\nwork: One-shot robot teaching for grasp-manipulation-release household\noperations,” in 2021 IEEE/SICE International Symposium on System\nIntegration (SII), IEEE, 2021.\n[51] K. Ikeuchi, N. Wake, R. Arakawa, K. Sasabuchi, and J. Takamatsu,\n“Semantic constraints to represent common sense required in household\nactions for multi-modal learning-from-observation robot,” arXiv preprint\narXiv:2103.02201, 2021.\n[52] H. T. Kuhn and W. L. Inequalities, “Related systems,” Annals of\nMathematic Studies, Princeton Univ. Press. EEUU, 1956.\n[53] N. Wake, I. Yanokura, K. Sasabuchi, and K. Ikeuchi, “Verbal focus-of-\nattention system for learning-from-demonstration,” in 2021 IEEE Inter-\nnational Conference on Robotics and Automation (ICRA), pp. 10377–\n10384, IEEE, 2021.\n[54] “Ultralytics | revolutionizing the world of vision ai.” Accessed: 2023-\n11-16.\n[55] J. Takamatsu, K. Sasabuchi, N. Wake, A. Kanehira, and K. Ikeuchi,\n“Learning-from-observation system considering hardware-level reusabil'
 '11-16.\n[55] J. Takamatsu, K. Sasabuchi, N. Wake, A. Kanehira, and K. Ikeuchi,\n“Learning-from-observation system considering hardware-level reusabil-\nity,” arXiv preprint arXiv:2212.09242, 2022.\n[56] D. Saito, K. Sasabuchi, N. Wake, J. Takamatsu, H. Koike, and\nK. Ikeuchi, “Task-grasping from a demonstrated human strategy,” in\n2022 IEEE-RAS 21st International Conference on Humanoid Robots\n(Humanoids), pp. 880–887, IEEE, 2022.']","The main focus of research on human-assisted continual robot learning, with regards to foundation models and hardware-level reusability, is discussed in the papers [41] and [55].",multi_context,"[{'Authors': 'Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi', 'Published': '2023-11-20', 'Summary': ""We introduce a pipeline that enhances a general-purpose Vision Language\nModel, GPT-4V(ision), by integrating observations of human actions to\nfacilitate robotic manipulation. This system analyzes videos of humans\nperforming tasks and creates executable robot programs that incorporate\naffordance insights. The computation starts by analyzing the videos with GPT-4V\nto convert environmental and action details into text, followed by a\nGPT-4-empowered task planner. In the following analyses, vision systems\nreanalyze the video with the task plan. Object names are grounded using an\nopen-vocabulary object detector, while focus on the hand-object relation helps\nto detect the moment of grasping and releasing. This spatiotemporal grounding\nallows the vision systems to further gather affordance data (e.g., grasp type,\nway points, and body postures). Experiments across various scenarios\ndemonstrate this method's efficacy in achieving real robots' operations from\nhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are\navailable at this project page:\nhttps://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/"", 'Title': 'GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration'}
 {'Authors': 'Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi', 'Published': '2023-11-20', 'Summary': ""We introduce a pipeline that enhances a general-purpose Vision Language\nModel, GPT-4V(ision), by integrating observations of human actions to\nfacilitate robotic manipulation. This system analyzes videos of humans\nperforming tasks and creates executable robot programs that incorporate\naffordance insights. The computation starts by analyzing the videos with GPT-4V\nto convert environmental and action details into text, followed by a\nGPT-4-empowered task planner. In the following analyses, vision systems\nreanalyze the video with the task plan. Object names are grounded using an\nopen-vocabulary object detector, while focus on the hand-object relation helps\nto detect the moment of grasping and releasing. This spatiotemporal grounding\nallows the vision systems to further gather affordance data (e.g., grasp type,\nway points, and body postures). Experiments across various scenarios\ndemonstrate this method's efficacy in achieving real robots' operations from\nhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are\navailable at this project page:\nhttps://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/"", 'Title': 'GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration'}]",TRUE
"How do LLMs interpret instructions and decompose them into robot action steps, and how does grounding visual information help?","['This study is positioned as a part of TAMP-based approach\nwhile extending the method to multi-modal input by leveraging\noff-the-shelf GPT-4V and GPT-4.\nB. Grounding visual information for robotics\nThe advanced language processing abilities of LLMs\ndemonstrate the capability to interpret instructions and de-\ncompose them into robot action steps [10], [14], [15], [41].\nHowever, executing long task steps as planned is often chal-\nlenging due to unforeseen and unpredicted environmental\nsituations. Thus, one challenge in robotics is grounding task\nplans in environmental information. For example, there are\napproaches that focus on enabling LLMs to output the pre-\nconditions and post-conditions (e.g., states of objects and their\ninterrelationships) of task steps to optimize their execution [13]\nand detect pre-condition errors for necessary revisions to the\ntask plan [42]. These strategies seek to achieve environment-\ngrounded robot execution by integrating environmental infor-\nmation and adjusting the robot’s actions at the task plan or\ncontroller level.\nIn this study, the open-vocabulary object detector [43] is\nused to ground the object names detected by GPT-4V in RGB\nimage. Additionally, focusing on the relationship between\nthe hand and the object enables the detection of timing and\nlocation where grasping and releasing occurred in the human\ndemonstration.\nC. Learning affordance\nThe concept of Affordance, as defined by Gibson [44],\nrefers to the potential for action that objects or situations\nin an environment provide to an individual. In the field of\nrobotics, it often pertains to the meaning of executable actions\nin that environment, and information about areas where action\nis possible. For instance, Ahn et al. have proposed an approach\nthat calculates the feasibility of robotic functions from visual\ninformation and compares it with planned tasks [5]. Huang et\nal. proposed using LLMs/VLMs to extract the knowledge of\nmovable area [45].\nThese studies indeed define Affordance as a form of\nGibson’s Affordance; however, focusing on the relationship\nbetween the working environment, the objects being manip-\nulated, and the robot, it can be considered that object ma-\nnipulation involves even more constraints [46]. For example,\nthe notion of affordance can be extended to waypoints for\ncollision avoidance [47], grasp types [48], and upper-limb\npostures [49]. This information is often not taught explicitly,\nthus vision systems need to extract it from human teaching\ndemonstrations. In this study, we propose a pipeline to extract\nthis information and provide a task plan endowed with that\naffordance information.\nIII. MULTIMODAL TASK PLANNER\nThe proposed system is composed of two pipelines con-\nnected in series (Fig. 2). The first pipeline, the so-called\nsymbolic task planner, takes teaching videos, text, or both as\ninput, then outputs a sequence of robot actions. Here, the text\ninput includes feedback on the GPT-4V’s recognition results'
 'This study proposes a multimodal task planner utilizing\nGPT-4V and GPT-4 (Fig. 1), as an example of the most\nrecent VLM and LLM, respectively. Our system accepts either\nhuman video demonstrations, text instructions, or both, and\n1Applied Robotics Research, Microsoft, Redmond, WA 98052, USA\nnaoki.wake@microsoft.com\nFig. 1. This figure illustrates the proposed multimodal task planner utilizing\nGPT-4V and GPT-4. It highlights the system’s ability to process video\ndemonstrations and text instructions, generating task plans and extracting\nkey affordances for robotic execution, which are then compiled into a JSON\nformat.\noutputs symbolic task plans (i.e., a sequence of coherent\ntask steps). When the visual data is available, the system\nthen reanalyzes the videos in consideration of the task plan\nand establishes spatiotemporal correspondences between each\ntask and the video. This process enables the extraction of\nvarious affordance information valuable for robotic execution,\nsuch as approaches to objects, grasp types, collision-avoiding\nwaypoints, and upper limb postures. Finally, the affordance\ninformation and task plan are compiled into a hardware-\nindependent executable file saved in JSON format. We have\nqualitatively checked the pipeline and confirmed the operabil-\nity of the output task plan across several real robots.\nThis research makes three contributions: (1) Proposing a\nready-to-use multimodal task planner that utilizes off-the-\nshelf VLM and LLM (2) Proposing a methodology for align-\ning GPT-4V’s recognition with affordance information for\ngrounded robotic manipulation (3) Making the code publicly\naccessible as a practical resource for the robotics research\ncommunity.\nII. RELATED WORK\nA. LLM/VLM-based task planning\nWhile a methodology to operate robots from instructions has\nbeen a research topic before the emergence of LLMs [18]–\n[20], recent study aim to leverage the LLM/VLMs tech-\nnologies [1], [21]–[32]. Most of these studies aim to train\nan end-to-end custom model using specific datasets [1]–[7],\n[21], [22], [26], [27], [33]–[38]. For example, Brohan et al.\narXiv:2311.12015v1  [cs.RO]  20 Nov 2023\nproposed a transformer-based model that trained based on both\nrobotic trajectory data and internet-scale vision-language tasks\n[2]. However, those approach often require a large amount\nof data of robot data collected by experts and necessitate\ndata recollection and model retraining when transferring or\nextending these to other robotic settings.\nOn the other hand, studies utilized off-the-shelf LLMs focus\non decomposing human instructions into high-level subgoals,\nwhile pre-trained skills achieve the subgoals [12], [13], [39].\nThis approach is typically seen as a part of framework, called\ntask and motion planning (TAMP) [40].\nThis study is positioned as a part of TAMP-based approach\nwhile extending the method to multi-modal input by leveraging\noff-the-shelf GPT-4V and GPT-4.\nB. Grounding visual information for robotics']",The advanced language processing abilities of LLMs demonstrate the capability to interpret instructions and decompose them into robot action steps. Grounding visual information helps in executing long task steps as planned by providing environmental information and adjusting the robot's actions at the task plan or controller level.,multi_context,"[{'Authors': 'Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi', 'Published': '2023-11-20', 'Summary': ""We introduce a pipeline that enhances a general-purpose Vision Language\nModel, GPT-4V(ision), by integrating observations of human actions to\nfacilitate robotic manipulation. This system analyzes videos of humans\nperforming tasks and creates executable robot programs that incorporate\naffordance insights. The computation starts by analyzing the videos with GPT-4V\nto convert environmental and action details into text, followed by a\nGPT-4-empowered task planner. In the following analyses, vision systems\nreanalyze the video with the task plan. Object names are grounded using an\nopen-vocabulary object detector, while focus on the hand-object relation helps\nto detect the moment of grasping and releasing. This spatiotemporal grounding\nallows the vision systems to further gather affordance data (e.g., grasp type,\nway points, and body postures). Experiments across various scenarios\ndemonstrate this method's efficacy in achieving real robots' operations from\nhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are\navailable at this project page:\nhttps://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/"", 'Title': 'GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration'}
 {'Authors': 'Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi', 'Published': '2023-11-20', 'Summary': ""We introduce a pipeline that enhances a general-purpose Vision Language\nModel, GPT-4V(ision), by integrating observations of human actions to\nfacilitate robotic manipulation. This system analyzes videos of humans\nperforming tasks and creates executable robot programs that incorporate\naffordance insights. The computation starts by analyzing the videos with GPT-4V\nto convert environmental and action details into text, followed by a\nGPT-4-empowered task planner. In the following analyses, vision systems\nreanalyze the video with the task plan. Object names are grounded using an\nopen-vocabulary object detector, while focus on the hand-object relation helps\nto detect the moment of grasping and releasing. This spatiotemporal grounding\nallows the vision systems to further gather affordance data (e.g., grasp type,\nway points, and body postures). Experiments across various scenarios\ndemonstrate this method's efficacy in achieving real robots' operations from\nhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are\navailable at this project page:\nhttps://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/"", 'Title': 'GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration'}]",TRUE
What is the dimensionality and order of the output space in the quadrupedal robot's controller?,"['environment and acquire new observations, the initial offline\ndata will be replaced by LLM outputs. Thus, we consider\nthis data collection phase as an initialization step.\nB. Prompt Engineering\nDirectly feeding observation and action pairs to LLMs\noften results in actions that do not achieve a stable walking\ngait. We next illustrate the prompt engineering step to guide\nLLMs to function as a feedback policy. Our prompt design,\nas shown in Fig. 3, can be classified into two categories:\ndescription prompt and observation and action prompt.\nDescription Prompt. The description prompt begins with\nPT D, a precise description of the robot walking task. This\nis then followed by control design details, e.g., the policy’s\noperating frequency, ensuring that the LLM aligns the actions\nto this frequency. Next, we specify the format and meaning\nof both observations and actions in PIO, allowing LLMs\nYou are the controller of a quadrupedal robot (A1 robot) with 10 Hz.\nPlease infer the output.\nThe robot\'s state is represented by a 33-dimensional input space.\nThe first 3 dimensions correspond to the robot\'s linear velocity.\nThe next 3 dimensions denote the robot\'s angular velocity.\nThe following 3 dimensions represent the gravity vector.\nThe subsequent 12 dimensions represent the joint positions.\nThe final 12 dimensions indicate the velocity of each joint.\nThe output space is 12-dimension, which is the joint position. \nThe order of the joints is [FRH, FRT, FRC, FLH, FLT, FLC, RRH, RRT, RRC, RLH, RLT, RLC].\nAfter we have the output, we will use 200 Hz PD controller to track it.\nThe following are past and consecutive inputs and outputs.\nAll numbers are normalized to non-negative integers by our special rule. \nThe output would be impacted by the previous inputs.\nThe trend of the outputs should be smooth.\nYour output is only one line and starts with ""Output:"", please do not output other redundant \nwords.\nInput: [65, 63, 52, 50, 52, 51, 49, 48, 40, 49, 40, 51, 51, 59, 46, 53, 52, 54, 49, 60, 45, 50, 51, 50, 51, 51, 51, 48, 50, 50, 51, 50, 50]\nOutput: [43, 24, 67, 35, 60, 32, 58, 30, 30, 44, 78, 15]\nInput: [58, 58, 50, 56, 50, 49, 50, 46, 41, 47, 43, 50, 48, 60, 47, 51, 43, 44, 49, 56, 41, 49, 51, 49, 49, 50, 51, 51, 45, 49, 50, 49, 50]\nOutput: [45, 5, 54, 30, 42, 0, 80, 0, 36, 35, 54, 0]\n…\nInput: [62, 58, 47, 50, 51, 49, 49, 46, 41, 53, 39, 51, 43, 48, 44, 55, 52, 47, 48, 60, 50, 50, 50, 50, 50, 52, 48, 50, 51, 51, 50, 50, 50]\nLLM Output:\nOutput: [57, 2, 56, 35, 68, 49, 57, 61, 60, 41, 85, 38]\n𝑷𝑻𝑫: Task Description\n𝑷𝑰𝑶: Meaning of Input \nand Output Space\n𝑷𝑱𝑶: Joint Order\n𝑷𝑪𝑷: Full Control Pipeline\n𝑷𝑨𝑰: Additional Illustration\n𝑷𝑯𝒊𝒔𝒕: Historical \nObservations and Actions\nLLM Prompt:\nDescription Prompt\nObservation and \nAction Prompt\nFig. 3: Text Prompt. We design a text prompt that includes two parts: a description prompt and an observation and action'
 'environment and acquire new observations, the initial offline\ndata will be replaced by LLM outputs. Thus, we consider\nthis data collection phase as an initialization step.\nB. Prompt Engineering\nDirectly feeding observation and action pairs to LLMs\noften results in actions that do not achieve a stable walking\ngait. We next illustrate the prompt engineering step to guide\nLLMs to function as a feedback policy. Our prompt design,\nas shown in Fig. 3, can be classified into two categories:\ndescription prompt and observation and action prompt.\nDescription Prompt. The description prompt begins with\nPT D, a precise description of the robot walking task. This\nis then followed by control design details, e.g., the policy’s\noperating frequency, ensuring that the LLM aligns the actions\nto this frequency. Next, we specify the format and meaning\nof both observations and actions in PIO, allowing LLMs\nYou are the controller of a quadrupedal robot (A1 robot) with 10 Hz.\nPlease infer the output.\nThe robot\'s state is represented by a 33-dimensional input space.\nThe first 3 dimensions correspond to the robot\'s linear velocity.\nThe next 3 dimensions denote the robot\'s angular velocity.\nThe following 3 dimensions represent the gravity vector.\nThe subsequent 12 dimensions represent the joint positions.\nThe final 12 dimensions indicate the velocity of each joint.\nThe output space is 12-dimension, which is the joint position. \nThe order of the joints is [FRH, FRT, FRC, FLH, FLT, FLC, RRH, RRT, RRC, RLH, RLT, RLC].\nAfter we have the output, we will use 200 Hz PD controller to track it.\nThe following are past and consecutive inputs and outputs.\nAll numbers are normalized to non-negative integers by our special rule. \nThe output would be impacted by the previous inputs.\nThe trend of the outputs should be smooth.\nYour output is only one line and starts with ""Output:"", please do not output other redundant \nwords.\nInput: [65, 63, 52, 50, 52, 51, 49, 48, 40, 49, 40, 51, 51, 59, 46, 53, 52, 54, 49, 60, 45, 50, 51, 50, 51, 51, 51, 48, 50, 50, 51, 50, 50]\nOutput: [43, 24, 67, 35, 60, 32, 58, 30, 30, 44, 78, 15]\nInput: [58, 58, 50, 56, 50, 49, 50, 46, 41, 47, 43, 50, 48, 60, 47, 51, 43, 44, 49, 56, 41, 49, 51, 49, 49, 50, 51, 51, 45, 49, 50, 49, 50]\nOutput: [45, 5, 54, 30, 42, 0, 80, 0, 36, 35, 54, 0]\n…\nInput: [62, 58, 47, 50, 51, 49, 49, 46, 41, 53, 39, 51, 43, 48, 44, 55, 52, 47, 48, 60, 50, 50, 50, 50, 50, 52, 48, 50, 51, 51, 50, 50, 50]\nLLM Output:\nOutput: [57, 2, 56, 35, 68, 49, 57, 61, 60, 41, 85, 38]\n𝑷𝑻𝑫: Task Description\n𝑷𝑰𝑶: Meaning of Input \nand Output Space\n𝑷𝑱𝑶: Joint Order\n𝑷𝑪𝑷: Full Control Pipeline\n𝑷𝑨𝑰: Additional Illustration\n𝑷𝑯𝒊𝒔𝒕: Historical \nObservations and Actions\nLLM Prompt:\nDescription Prompt\nObservation and \nAction Prompt\nFig. 3: Text Prompt. We design a text prompt that includes two parts: a description prompt and an observation and action']","The output space is 12-dimension, and the order of the joints is [FRH, FRT, FRC, FLH, FLT, FLC, RRH, RRT, RRC, RLH, RLT, RLC].",multi_context,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}
 {'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
"What is the focus of the research paper titled ""Isaac gym: High performance gpu-based physics simulation for robot learning"" in relation to reinforcement learning?","[' N. Rudin, A. Allshire, A. Handa, et al., “Isaac gym:\nHigh performance gpu-based physics simulation for robot learning,”\narXiv preprint arXiv:2108.10470, 2021.\n[25] G. B. Margolis, G. Yang, K. Paigwar, T. Chen, and P. Agrawal,\n“Rapid\nlocomotion\nvia\nreinforcement\nlearning,”\narXiv\npreprint']",The focus of the research paper titled 'Isaac gym: High performance gpu-based physics simulation for robot learning' in relation to reinforcement learning is rapid locomotion.,simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What is the significance of open-source tools in enabling further research on X-embodiment robotic learning?,"['embodiments from 21 different institutions that can enable\nthe robotics community to pursue further research on X-\nembodiment models, along with open-source tools to facili-\ntate such research. Our aim is not to innovate in terms of the\nparticular architectures and algorithms, but rather to provide\nthe model that we trained together with data and tools to\nenergize research around X-embodiment robotic learning.\nII. RELATED WORK\nTransfer across embodiments. A number of prior works\nhave studied methods for transfer across robot embodiments\nin simulation [10–22] and on real robots [23–29]. These\nmethods often introduce mechanisms specifically designed to\naddress the embodiment gap between different robots, such\nas shared action representations [14, 30], incorporating rep-\nresentation learning objectives [17, 26], adapting the learned\npolicy on embodiment information [11, 15, 18, 30, 31], and\ndecoupling robot and environment representations [24]. Prior\nwork has provided initial demonstrations of X-embodiment\ntraining [27] and transfer [25, 29, 32] with transformer\nmodels. We investigate complementary architectures and\nprovide complementary analyses, and, in particular, study the\ninteraction between X-embodiment transfer and web-scale\npretraining. Similarly, methods for transfer across human\nand robot embodiments also often employ techniques for\nreducing the embodiment gap, i.e. by translating between\ndomains or learning transferable representations [33–43]. Al-\nternatively, some works focus on sub-aspects of the problem\nsuch as learning transferable reward functions [17, 44–48],\ngoals [49, 50], dynamics models [51], or visual representa-\ntions [52–59] from human video data. Unlike most of these\nprior works, we directly train a policy on X-embodiment\ndata, without any mechanisms to reduce the embodiment gap,\nand observe positive transfer by leveraging that data.\nLarge-scale robot learning datasets. The robot learning\ncommunity has created open-source robot learning datasets,\nShapes\nFurniture\nFood\nAppliances\nUtensils\nContainers\nFranka\nGoogle\u2028\nRobot\nxArm\nWidowX\nJackal\nHello Stretch\nKinova Gen3\nSawyer\n(a) # Datasets per Robot Embodiment\n(b) # Scenes per Embodiment\n(c) # Trajectories per Embodiment\n(d) Common Dataset Skills\n(e) Common Dataset Objects\nSawyer\nFranka\nGoogle\u2028\nRobot\nxArm\nWidowX\nKuka iiwa\npicking\nmoving\npushing\nplacing\nsliding\nputting\nnavigating\npointing\nopening\nnudging\nclosing\ninserting\nknocking\ndropping\ndragging\nwiping\nassembling\nturning on\nkeeping\nseparating\nFranka\nxArm\nSawyer\nGoogle Robot\nKuka iiwa\nUR5\nWidowX\nHello Stretch\nPR2\nDLR SARA\nJaco 2\nUnitree A1\nxArm Bimanual\nCobotta\nDLR EDAN\nPAMY2\nKinova Gen3\nFanuc Mate\nJackal\nRC Car\nTurtleBot 2\nBaxter\nhexagon\ntriangle\nheart\ncube\ntray\nbowl\npot\nbox\ncup\nbasket\ncounter\ndrawer\ntable\ncabinet\ndoor\nchair\napple\norange\nbanana\ncoke can\nchip bag\nfaucet\nfridge\nsink\nstove\nmicrowave\noven\nfork\nspoon\nknife\nspatula']","Open-source tools are significant in enabling further research on X-embodiment robotic learning as they provide the necessary resources and support for the robotics community to pursue research on X-embodiment models. These tools facilitate research by providing access to the model, data, and tools needed for studying X-embodiment robotic learning.",simple,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
How can large language models be used to generate low-level control commands for robot walking without task-specific fine-tuning?,"['Prompt a Robot to Walk with Large Language Models\nYen-Jen Wang1, Bike Zhang2, Jianyu Chen1,3, Koushil Sreenath2\nAbstract— Large language models (LLMs) pre-trained on\nvast internet-scale data have showcased remarkable capabilities\nacross diverse domains. Recently, there has been escalating\ninterest in deploying LLMs for robotics, aiming to harness\nthe power of foundation models in real-world settings. How-\never, this approach faces significant challenges, particularly\nin grounding these models in the physical world and in\ngenerating dynamic robot motions. To address these issues,\nwe introduce a novel paradigm in which we use few-shot\nprompts collected from the physical environment, enabling the\nLLM to autoregressively generate low-level control commands\nfor robots without task-specific fine-tuning. Experiments across\nvarious robots and environments validate that our method can\neffectively prompt a robot to walk. We thus illustrate how\nLLMs can proficiently function as low-level feedback controllers\nfor dynamic motion control even in high-dimensional robotic\nsystems. The project website and source code can be found at:\nprompt2walk.github.io.\nI. INTRODUCTION\nLarge language models (LLMs) pre-trained on internet-\nscale data [5], [33], [32], [9], [45] have demonstrated impres-\nsive results in various fields, e.g., natural language processing\n[29], [28], computer vision [31], code generation [7], etc.\nBuilding upon the success of LLMs, there is a surging\ninterest in utilizing LLMs for embodied agents [1], [46],\naiming to harness the power of foundation models in the\nphysical world [2]. Towards this goal, significant progress\nhas been made [4], [3], [10]. However, there are some\nremaining challenges. 1) Even though LLMs are trained with\nbroad data at scale, the dataset does not incorporate data from\nthe physical world, making it challenging to ground LLMs in\nrobot control. 2) While foundation models have been widely\nused in a pre-training and fine-tuning paradigm for robotics\napplications, there could be a paradigm shift to few-shot\nlearning in light of the progress of the natural language field\n[5]. 3) Most recent language-guided robot control research\nshowcases mainly quasi-static robot motions. It remains un-\ncertain whether LLMs can generate dynamic robot behaviors\nwithout a low-level controller interface or without relying on\npredefined motion primitives.\nIn this paper, we want to raise the intriguing question\nof whether LLMs can function as low-level controllers for\nachieving dynamic tasks like robot walking? This requires us\nto address the challenges mentioned above. We do this by\nexploring a new paradigm that leverages few-shot prompts\nwith a large language model, i.e., GPT-4, to directly output\n1Institute for Interdisciplinary Information Sciences, Tsinghua University.\n2University of California, Berkeley. 3Shanghai Qi Zhi Institute.\nThis work is supported in part by the InnoHK of the Government of the']","To address the challenges of grounding large language models (LLMs) in robot control and generating dynamic robot motions, a novel paradigm is introduced. This paradigm leverages few-shot prompts collected from the physical environment to enable LLMs, such as GPT-4, to autoregressively generate low-level control commands for robot walking without task-specific fine-tuning.",simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
How can designing a pipeline tailored to the specific task lead to more efficient automation than humanoid robots?,"['objects instead of a conveyor belt. Similarly, a chef robot\nmay not need to hold a shovel and cook like a human. In\nmany cases, designing a pipeline tailored to the specific\ntask at hand can lead to more efficient automation than\nhumanoid robots. While humanoid robots are often depicted\nin animation scenes, such as in animation like Mobile Suit\nGundam or games like Armored Core, their design may not\nalways be practical for applications. For instance, a robot\ndesigned solely for washing dishes may not need the ability\nto sing. Modular concepts like Expedition A17, can offer\noptimal results for different scenarios by replacing certain\ncomponents. The shape of the robot remains a topic of de-\nbate, and the decision should ultimately focus on suitability\nfor the task at hand.\n5.4. LLM Deployment\nGiven embodied intelligence, the question arises regard-\ning the deployment of its brain. Current technical limitations\nprevent the LLM from being deployed locally on the robot.\nThe prevailing industry practice involves employing two\nbrains: a cloud-based super brain and a local brain, like in\nFigure 4(b). However, a unified consensus on this device-\nside plus cloud testing deployment method has yet to be\nestablished. A feasible solution could be to create a dynamic,\ncompact model on the local client side, capable of handling\nbasic scenario interactions. The cloud-based super brain, on\nthe other hand, would tackle complex and challenging prob-\nlems. The LLM deployment architecture remains a pressing\nissue that must be addressed in the future development of\nagents. This deployment structure also introduces latency\nissues, as information exchange between the robot and the\nsuper brain requires signal transmission. In certain environ-\nments, such as those with signal loss, the robot may be left\nwith only its local brain, potentially leading to control loss\nor unpredictable behavior.\n7https://www.agibot.com\n5.5. Security\nLLM like ChatGPT may harbor biases or misconcep-\ntions stemming from their pre-training data. These biases\ncan manifest in problematic guidance for users, and robots\nthat rely on LLM as their brains may also exhibit biases\n[142]. Since robots’ outputs are typically physical actions,\nbiased or misunderstood guidance can lead to harmful con-\nsequences for users [36, 98], such as a chef robot burning\ndown a house while cooking. Beyond physical safety risks,\nrobots also raise concerns about data security [86]. For\ninstance, a robot butler who resides in a home may become\nintimately familiar with the household’s environment and\noccasionally require cloud interaction for certain tasks. Dur-\ning user interaction, there is a risk of private data leakage,\nwhich could be mitigated by an offline environment, but this\nmay compromise the robot’s performance.\n5.6. Dialogue Consistency\nHumans often don’t complete tasks in a single, static\nstep. Instead, they iteratively adjust strategies and goals\nbased on feedback received after taking action. The same']",Designing a pipeline tailored to the specific task can lead to more efficient automation than humanoid robots.,simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is the significance of the Bridge dataset in the evaluation of emergent skills in robots?,"['Emergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like\nthe ones shown in Fig. 5. These tasks involve objects and\nskills that are not present in the RT-2 dataset but occur in the\nBridge dataset [95] for a different robot (the WidowX robot).\nResults are shown in Table II, Emergent Skills Evaluation\ncolumn. Comparing rows (1) and (2), we find that RT-2-X\noutperforms RT-2 by ∼3×, suggesting that incorporating\ndata from other robots into the training improves the range\nof tasks that can be performed even by a robot that already\nhas large amounts of data available. Our results suggest that\nco-training with data from other platforms imbues the RT-2-\nX controller with additional skills for the platform that are\nnot present in that platform’s original dataset.\nOur next ablation involves removing the Bridge dataset\nfrom RT-2-X training: Row (3) shows the results for RT-2-\nX that includes all data used for RT-2-X except the Bridge\ndataset. This variation significantly reduces performance on\nthe hold-out tasks, suggesting that transfer from the WidowX\ndata may indeed be responsible for the additional skills that\ncan be performed by RT-2-X with the Google Robot.\nC. Design decisions\nLastly, we perform ablations to measure the influence of\ndifferent design decisions on the generalization capabilities\nof our most performant RT-2-X model, which are presented\nin Table II. We note that including a short history of im-\nages significantly improves generalization performance (row\n(4) vs row (5)). Similarly to the conclusions in the RT-2\npaper [9], Web-based pre-training of the model is critical\nto achieving a high performance for the large models (row\n(4) vs row (6)). We also note that the 55B model has\nsignificantly higher success rate in the Emergent Skills com-\npared to the 5B model (row (2) vs row (4)), demonstrating\nthat higher model capacity enables higher degree of transfer\nacross robotic datasets. Contrary to previous RT-2 findings,\nco-fine-tuning and fine-tuning have similar performance in\nboth the Emergent Skills and Generalization Evaluation (row\n(4) vs row (7)), which we attribute to the fact that the robotics\ndata used in RT-2-X is much more diverse than the previously\nused robotics datasets.\nFig. 5: To assess transfer between embodiments, we evaluate the\nRT-2-X model on out-of-distribution skills. These skills are in\nthe Bridge dataset, but not in the Google Robot dataset (the\nembodiment they are evaluated on).\nVI. DISCUSSION, FUTURE WORK, AND OPEN PROBLEMS\nWe presented a consolidated dataset that combines data\nfrom 22 robotic embodiments collected through a collab-\noration between 21 institutions, demonstrating 527 skills\n(160266 tasks). We also presented an experimental demon-\nstration that Transformer-based policies trained on this data\ncan exhibit significant positive transfer between the different']","The Bridge dataset is significant in the evaluation of emergent skills in robots because it provides objects and skills that are not present in the RT-2 dataset. By incorporating data from the Bridge dataset into the training of the RT-2-X robot, it improves the range of tasks that can be performed by the robot.",simple,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
"What resources are in the Open X-Embodiment Repository for X-embodiment research in robot learning, and what is the purpose of the Open X-Embodiment Dataset?","['hexagon\ntriangle\nheart\ncube\ntray\nbowl\npot\nbox\ncup\nbasket\ncounter\ndrawer\ntable\ncabinet\ndoor\nchair\napple\norange\nbanana\ncoke can\nchip bag\nfaucet\nfridge\nsink\nstove\nmicrowave\noven\nfork\nspoon\nknife\nspatula\nFig. 2: The Open X-Embodiment Dataset. (a): the dataset consists of 60 individual datasets across 22 embodiments. (b): the Franka robot\nhas the largest diversity in visually distinct scenes due to the large number of Franka datasets, (c): xArm and Google Robot contribute\nthe most number of trajectories due to a few large datasets, (d, e): the dataset contains a great diversity of skills and common objects.\nspanning grasping [60–71], pushing interactions [23, 72–74],\nsets of objects and models [75–85], and teleoperated demon-\nstrations [8, 86–95]. With the exception of RoboNet [23],\nthese datasets contain data of robots of the same type,\nwhereas we focus on data spanning multiple embodiments.\nThe goal of our data repository is complementary to these\nefforts: we process and aggregate a large number of prior\ndatasets into a single, standardized repository, called Open\nX-Embodiment, which shows how robot learning datasets\ncan be shared in a meaningul and useful way.\nLanguage-conditioned robot learning. Prior work has\naimed to endow robots and other agents with the ability to\nunderstand and follow language instructions [96–101], often\nby learning language-conditioned policies [8, 40, 45, 102–\n106]. We train language-conditioned policies via imitation\nlearning like many of these prior works but do so using\nlarge-scale multi-embodiment demonstration data. Following\nprevious works that leverage pre-trained language embed-\ndings [8, 40, 45, 103, 107–112] and pre-trained vision-\nlanguage models [9, 113–115] in robotic imitation learning,\nwe study both forms of pre-training in our experiments,\nspecifically following the recipes of RT-1 [8] and RT-2 [9].\nIII. THE OPEN X-EMBODIMENT REPOSITORY\nWe\nintroduce\nthe\nOpen\nX-Embodiment\nRepository\n(robotics-transformer-x.github.io) – an open-source reposi-\ntory which includes large-scale data along with pre-trained\nmodel checkpoints for X-embodied robot learning research.\nMore specifically, we provide and maintain the following\nopen-source resources to the broader community:\n• Open X-Embodiment Dataset: robot learning dataset\nwith 1M+ robot trajectories from 22 robot embodi-\nments.\n• Pre-Trained Checkpoints: a selection of RT-X model\ncheckpoints ready for inference and finetuning.\nWe intend for these resources to form a foundation for X-\nembodiment research in robot learning, but they are just\nthe start. Open X-Embodiment is a community-driven effort,\ncurrently involving 21 institutions from around the world,\nand we hope to further broaden participation and grow the\ninitial Open X-Embodiment Dataset over time. In this sec-\ntion, we summarize the dataset and X-embodiment learning\nframework, before discussing the specific models we use to\nevaluate our dataset and our experimental results.\nA. The Open X-Embodiment Dataset']","The resources in the Open X-Embodiment Repository for X-embodiment research in robot learning include the Open X-Embodiment Dataset, which is a robot learning dataset with 1M+ robot trajectories from 22 robot embodiments. The purpose of the Open X-Embodiment Dataset is to provide a foundation for X-embodiment research in robot learning.",reasoning,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
"What challenges did the ceiling point filter face in low-ceiling environments, particularly with inclines and stairs?","['3.3\nIssues\nAlthough our planner performed very well during the Finals, it still encountered some issues related to other\nparts of the navigation stack.\n3.3.1\nHeight Map Spikes\nAs mentioned in Section 2.4.3, obtaining a clean height map in environments with low ceilings was challenging,\nand we tuned our ceiling point ﬁlter to also work with inclines and stairs, which exacerbated the issue. This\nslowed our progress quite a bit in the cave section, where the ceiling was especially low. Unfortunately, we\nreached these sections with our explorer robots, which recorded many ceiling points very close to the robot,\ndue to their dome lidar conﬁguration (see Figure 2(a)+(b)).\nAlthough this slowed our progress, we never got stuck. We demonstrate this behavior with a narrow cave\nopening, which is immediately followed by an incline, shown in Figure 12. Even when the robot was less than\na meter from the opening (Figure 12\n), the height map showed a straight wall. Our planner could therefore\nonly plan right up to the halucinated wall, which then shifts forward slightly (Figure 12\n) and allows the\nrobot to plan a bit farther. In Figure 12\n, the slope aligns with the distance-dependent height threshold\nof our ceiling point ﬁlter and we can plan forward farther. Once we are fully on the incline (Figure 12\n),\nthe negative slope at the rear of the robot causes the fake wall to reappear right behind it. Nonetheless, the\nrobot was able to plan up the slope and back down fully autonomous.\n3.3.2\nPath Follower\nPlanners rely on their paths being tracked accurately. The path follower is therefore an important component\nof the navigation stack. Our pure-pursuit path follower generally performed well, tracking paths precisely\nin narrow spaces and moving swiftly in more open spaces. However, during data analysis for this work, we\ndiscovered that there was a non-constant delay between the planner publishing a path, and the path follower\nstarting to track it, which we observed to be up to 500 ms. The top right of Figure 13 shows an example of\nthe diﬀerence in robot position between the plan being published and being followed. This, in combination\n3\n5\n4\n1\n6\n2\n0s\nOnboard Image\nPath at time:\nPath Following Started\n1s\n3s\n5s\n25s\n30s\nTime Series\nComputed\nFigure 13: A 500 ms time delay between computing a path and starting to follow it lead to imperfect path following.\nAs a result, one robot brieﬂy got stuck on a narrow scaﬀolding pole in the urban section.\nThe robot initially\nplanned to pass the pole on the right but\nreplanned to use the shorter path on the left.\nSince the robot already\nmoved, the next path went right again which got the robot stuck on the pole.\nThe pole got removed from the map\nas it was now too close to be perceived and, consequently, paths through the pole were planned until\nthe robot\ndrifted to the left and\nﬁnally got unstuck.\nwith height map issues and the unsmooth nature of sampling-based planning, caused one explorer to get']","Obtaining a clean height map in environments with low ceilings was challenging for the ceiling point filter, especially in the presence of inclines and stairs. The filter had to be tuned to work with these features, but it exacerbated the issue and slowed down progress in the cave section.",reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
"What can LLMs do in AI and robotics, considering their capabilities and potential applications in knowledge acquisition, reasoning, flexibility, adaptability, learning, multimodal interaction, education, entertainment, emotional interaction, collaboration, and innovation?","['and planning abilities [138]. Additionally, LLMs can parse\nhigh-level abstract instructions to perform complex tasks\nwithout requiring step-by-step guidance5, and their human-\nlike text-generation capabilities make them highly effective\ncommunicators [46]. Furthermore, LLMs can sense their\nenvironment [44], and technologies that expand their action\nspace allow them to interact with the physical environment\nand complete tasks [149, 156]. They also possess reasoning\nand planning capabilities, such as logical and mathematical\nreasoning [134, 138], task decomposition [154], and plan-\nning [143] for specific tasks. LLM-based agents have been\nused in various real-world scenarios [77, 97] and have shown\npotential for multi-agent interactions and social capabilities.\n5BabyAGI https://github.com/yoheinakajima/babyagi\nOverall, LLMs have revolutionized the field of artificial\nintelligence and hold great promise for future advancements.\n3.2.2. Capacity of LLM in robotics\nLLM serves as the brain of the robot, functioning as the\ncentral component that integrates knowledge, memory, and\nreasoning capabilities to enable the robot to plan and execute\ntasks intelligently.\nKnowledge. The knowledge of LLM for robotics can\nbe categorized into two types: the knowledge that needs\nto be acquired through learning (which is the pre-trained\ndataset) and the knowledge that has been learned and stored\nin memory [142].\n• Pre-trained data. There are various types of pre-\ntrained datasets available, and the more extensive\nand richer the knowledge learned, the stronger the\nLLM’s generalization and natural language under-\nstanding capabilities will be [106]. Theoretically, the\nmore a language model learns, the more parameters\nit has, enabling it to learn complex knowledge in\nnatural language and gain powerful capabilities [65].\nResearch has shown that a richer dataset for language\nmodel learning can result in correct answers to di-\nverse questions [106]. Datasets can be categorized\ninto different types, such as basic semantic knowledge,\nwhich provides an understanding of language mean-\ning [133]; Common sense, including everyday facts\nlike people eating when hungry or the sun rising in\nthe east [108]; Professional field knowledge, which\ncan aid humans in completing tasks like programming\n[146] and mathematics [24].\n• Memory. Just like human memory, embodied intel-\nligence should be able to formulate strategies and\nmake decisions for new tasks based on experiences\n(i.e., observed actions, thoughts, etc.). When faced\nwith complex tasks, the memory mechanism can aid\nin reviewing past strategies to obtain more effective\nsolutions [56, 121]. However, memory poses some\nchallenges, such as the length of memory sequences\nand how to efficiently store and index them as the\nnumber of memories grows. As the robot’s mem-\nory burden increases over time, it must be able to\neffectively manage and retrieve memories to avoid\ncatastrophic forgetfulness [68].'
 '• Task execution. LLMs assist robots in performing\nvarious tasks by understanding and generating natural\nlanguage instructions. Robots can navigate, manipu-\nlate objects, and execute specific actions based on user\nlanguage commands [126]. This opens up broader\npossibilities for robot applications in everyday life.\n• Knowledge acquisition and reasoning. LLMs pos-\nsess powerful information retrieval and reasoning ca-\npabilities, which can help robots acquire and process\nrich knowledge. Robots can interact with language\nmodels to obtain real-time and accurate information,\nthereby improving their decision-making ability and\nintelligence.\n• Flexibility and adaptability. The flexibility of LLMs\nenables robots to adapt to different tasks and envi-\nronments. Through interaction with language mod-\nels, robots can make flexible adjustments and self-\nadaptation based on specific circumstances, better\nmeeting user needs [52].\n• Learning and improvement. LLMs enable contin-\nuous learning and improvement through interaction\nwith users. By analyzing and understanding user feed-\nback, robots can enhance their performance and profi-\nciency. This learning and improvement capability al-\nlows robots to gradually adapt to user personalities and\npreferences, providing more personalized services.\n• Multimodal interaction. LLMs also support multi-\nmodal interaction, enabling robots to process different\nforms of inputs such as speech, images, and text simul-\ntaneously. This multimodal capability [141] allows\nrobots to comprehensively understand user needs and\nprovide richer interaction experiences.\n• Education and entertainment. LLMs offer poten-\ntial applications for education and entertainment pur-\nposes in robotics. Robots can provide educational\ncontent, answer questions, or engage in games and\nentertainment activities through interaction with lan-\nguage models. This has significant implications for\nchildren’s education, language learning, and the en-\ntertainment industry.\n• Emotional interaction. The application of LLMs\nenhances the emotional interaction capabilities of\nrobots. By generating emotionally responsive outputs,\nrobots can establish closer and more meaningful\nrelationships with users. This emotional interaction\nis valuable in fields such as care robots, emotional\nsupport, and psychotherapy.\n• Collaboration and cooperation. LLMs enable robots\nto collaborate and cooperate better with humans.\nRobots can jointly solve problems, formulate plans,\nand execute tasks through interaction with language\nmodels [126]. This collaboration and cooperation\nability is significant for industrial automation, team\ncollaboration, and human-robot coexistence.\n• Innovation and exploration. The application of LLMs\nstimulates innovation and exploration in the field of\nrobotics. Through interaction with language mod-\nels, robots can possess higher-level intelligence and\ncomprehension abilities, opening up new avenues for\nresearch and development in robotics.']","LLMs in AI and robotics have various capabilities and potential applications. They can assist in knowledge acquisition and reasoning, improve flexibility and adaptability, enable continuous learning and improvement, support multimodal interaction, provide education and entertainment, enhance emotional interaction, facilitate collaboration and cooperation, and stimulate innovation and exploration.",multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}
 {'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What advancements in language models have impacted robotics?,"['forward. This dual approach is commonly employed\nin various machine learning models and speech gen-\neration applications. Bidirectional models harness the\npower of contextual information from both directions,\nproviding a deeper understanding of the text [6].\n• Exponential models [28] employ an equation that\ncombines feature functions and n-grams to evaluate\ntext. Unlike n-grams, this type of model allows for\nmore flexibility in analyzing parameters and does not\nmandate the specification of individual gram sizes.\nEssentially, exponential models define features and\nparameters based on the desired outcomes, providing\na more open-ended approach to text analysis.\n• Neural language models, including recurrent neural\nnetworks (RNNs) [150] and transformers [131], have\ngained popularity in recent years. These models use\ndeep learning techniques to capture complex language\npatterns and dependencies.\n• Transformer architecture’s development revolution-\nized language modeling. Transformers use self-attention\nmechanisms to capture relationships between words\nin a sentence. This is currently the most popular\narchitecture [131].\n2.1.2. Development of LLMs\nSome well-known developments in LLMs are described\nbelow in detail.\n• Eliza. The concept of language generation models\noriginated in the 1960s with the development of Eliza,\nthe world’s first chatbot, by MIT researcher Joseph\nWeizenbaum. Eliza’s creation laid the groundwork for\nnatural language processing (NLP) research, paving\nthe way for subsequent advancements in this field\n[125].\n• LSTM. The year 1997 witnessed the emergence of\nLong Short-Term Memory (LSTM) networks, intro-\nducing a significant advancement in neural network\narchitecture. The introduction of LSTM networks en-\nabled the development of deeper and more intricate\nneural networks capable of effectively processing vast\namounts of data.\n• Stanford coreNLP. In 2010, Stanford’s CoreNLP\nsuite brought about a significant milestone in the\nfield by offering developers a versatile toolkit. This\nsuite empowers developers to conduct various natural\nlanguage processing tasks.\n• Google brain. In 2011, a scaled-down version of\nGoogle Brain surfaced, introducing groundbreaking\nfeatures such as word embeddings. These advanced\ncapabilities revolutionized natural language process-\ning (NLP) systems by enhancing their ability to com-\nprehend context with greater clarity.\n• Transformer models. Transformer models [131],\nintroduced in 2017, brought significant advancements\nto language modeling. They employ self-attention\nmechanisms to capture global dependencies and have\nachieved state-of-the-art performance in various nat-\nural language processing tasks.\n• Large language model. OpenAI unveiled GPT-4\n[92], a language model boasting an astounding scale\nof approximately one trillion parameters. This repre-\nsents a five-fold increase compared to its predecessor,\nGPT-3 [14], and a staggering 3,000-fold increase\ncompared to the initial release of BERT [31]. The'
 'sents a five-fold increase compared to its predecessor,\nGPT-3 [14], and a staggering 3,000-fold increase\ncompared to the initial release of BERT [31]. The\nintroduction of GPT-4 sets a new benchmark in the\nfield of language models, showcasing the remarkable\nprogress in model size and capacity.\n2.1.3. Popular LLMs\nUntil now, there are many foundation models or LLMs\nhave been developed. We present some selected models\nbelow, including GPT-3.5, GPT-4, BERT, T5, and LLaMA.\n• GPT-3 (Generative pre-trained transformer 3) [14].\nDeveloped by OpenAI, GPT-3 is one of the most\nprominent language models. With 175 billion parame-\nters, it can generate coherent and contextually relevant\ntext across a wide range of domains.\n• GPT-4 (Generative pre-trained transformer 4) [92].\nUnveiled on March 14, 2023, GPT-4 represents a sig-\nnificant advancement in language models, prioritizing\nfactual accuracy and enhancing reliability compared\nto its predecessors, GPT-3 and GPT-3.5. Notably,\nGPT-4 introduces multimodal capabilities, enabling\nit to process images as input and generate comprehen-\nsive descriptions, classifications, and analyses across\ndifferent modalities. This multimodal functionality\nexpands the model’s versatility and enhances its abil-\nity to understand and generate content across various\nmedia formats.\n• BERT (Bidirectional encoder representations from\ntransformers) [31]. Developed by Google, BERT\nintroduced the concept of pre-training and fine-tuning\nfor language understanding tasks. It has achieved\nremarkable results in tasks such as question answering\nand text classification.\n• T5 (Text-to-Text transfer transformer) [101]. De-\nveloped by Google, T5 is a versatile language model\nthat can be fine-tuned for various natural language\nprocessing tasks, including summarization, transla-\ntion, and text generation.\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 3 of 19\nLarge Language Models for Robotics: A Survey\n• LLaMA [128]. Developed by Google, LLaMA is a\nlanguage model pre-trained and fine-tuned generative\ntext model with parameter counts ranging from 7 to 70\nbillion. LLaMA removes the absolute position embed-\nding and instead adds rotational position embedding at\neach layer of the network.\n2.2. Benefits of LLM for Robotics\nThe advent of LLM-based robots has brought about a\nplethora of innovative changes to the field. Here, we explore\nthe various benefits that LLM will bring to robots. The\nnecessity and significance of LLMs for robotics can be\nsummarized in the following ten points:\n• Natural language interaction. LLMs provide robots\nwith the ability to engage in natural language interac-\ntions, allowing users to communicate with robots in\nan intuitive and convenient manner. This interaction\nmethod aligns better with human habits and needs,\nenhancing the usability and acceptance of robots.\n• Task execution. LLMs assist robots in performing\nvarious tasks by understanding and generating natural\nlanguage instructions. Robots can navigate, manipu-']","The advancements in language models that have impacted robotics include the development of Eliza, LSTM networks, Stanford coreNLP, Google brain, Transformer models, and large language models like GPT-4, GPT-3, BERT, T5, and LLaMA.",multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}
 {'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is included in the Open X-Embodiment Dataset?,"['hexagon\ntriangle\nheart\ncube\ntray\nbowl\npot\nbox\ncup\nbasket\ncounter\ndrawer\ntable\ncabinet\ndoor\nchair\napple\norange\nbanana\ncoke can\nchip bag\nfaucet\nfridge\nsink\nstove\nmicrowave\noven\nfork\nspoon\nknife\nspatula\nFig. 2: The Open X-Embodiment Dataset. (a): the dataset consists of 60 individual datasets across 22 embodiments. (b): the Franka robot\nhas the largest diversity in visually distinct scenes due to the large number of Franka datasets, (c): xArm and Google Robot contribute\nthe most number of trajectories due to a few large datasets, (d, e): the dataset contains a great diversity of skills and common objects.\nspanning grasping [60–71], pushing interactions [23, 72–74],\nsets of objects and models [75–85], and teleoperated demon-\nstrations [8, 86–95]. With the exception of RoboNet [23],\nthese datasets contain data of robots of the same type,\nwhereas we focus on data spanning multiple embodiments.\nThe goal of our data repository is complementary to these\nefforts: we process and aggregate a large number of prior\ndatasets into a single, standardized repository, called Open\nX-Embodiment, which shows how robot learning datasets\ncan be shared in a meaningul and useful way.\nLanguage-conditioned robot learning. Prior work has\naimed to endow robots and other agents with the ability to\nunderstand and follow language instructions [96–101], often\nby learning language-conditioned policies [8, 40, 45, 102–\n106]. We train language-conditioned policies via imitation\nlearning like many of these prior works but do so using\nlarge-scale multi-embodiment demonstration data. Following\nprevious works that leverage pre-trained language embed-\ndings [8, 40, 45, 103, 107–112] and pre-trained vision-\nlanguage models [9, 113–115] in robotic imitation learning,\nwe study both forms of pre-training in our experiments,\nspecifically following the recipes of RT-1 [8] and RT-2 [9].\nIII. THE OPEN X-EMBODIMENT REPOSITORY\nWe\nintroduce\nthe\nOpen\nX-Embodiment\nRepository\n(robotics-transformer-x.github.io) – an open-source reposi-\ntory which includes large-scale data along with pre-trained\nmodel checkpoints for X-embodied robot learning research.\nMore specifically, we provide and maintain the following\nopen-source resources to the broader community:\n• Open X-Embodiment Dataset: robot learning dataset\nwith 1M+ robot trajectories from 22 robot embodi-\nments.\n• Pre-Trained Checkpoints: a selection of RT-X model\ncheckpoints ready for inference and finetuning.\nWe intend for these resources to form a foundation for X-\nembodiment research in robot learning, but they are just\nthe start. Open X-Embodiment is a community-driven effort,\ncurrently involving 21 institutions from around the world,\nand we hope to further broaden participation and grow the\ninitial Open X-Embodiment Dataset over time. In this sec-\ntion, we summarize the dataset and X-embodiment learning\nframework, before discussing the specific models we use to\nevaluate our dataset and our experimental results.\nA. The Open X-Embodiment Dataset']",The Open X-Embodiment Dataset includes robot learning dataset with 1M+ robot trajectories from 22 robot embodiments.,simple,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
How can executable actions in the environment be defined and utilized in robotics?,"['This study is positioned as a part of TAMP-based approach\nwhile extending the method to multi-modal input by leveraging\noff-the-shelf GPT-4V and GPT-4.\nB. Grounding visual information for robotics\nThe advanced language processing abilities of LLMs\ndemonstrate the capability to interpret instructions and de-\ncompose them into robot action steps [10], [14], [15], [41].\nHowever, executing long task steps as planned is often chal-\nlenging due to unforeseen and unpredicted environmental\nsituations. Thus, one challenge in robotics is grounding task\nplans in environmental information. For example, there are\napproaches that focus on enabling LLMs to output the pre-\nconditions and post-conditions (e.g., states of objects and their\ninterrelationships) of task steps to optimize their execution [13]\nand detect pre-condition errors for necessary revisions to the\ntask plan [42]. These strategies seek to achieve environment-\ngrounded robot execution by integrating environmental infor-\nmation and adjusting the robot’s actions at the task plan or\ncontroller level.\nIn this study, the open-vocabulary object detector [43] is\nused to ground the object names detected by GPT-4V in RGB\nimage. Additionally, focusing on the relationship between\nthe hand and the object enables the detection of timing and\nlocation where grasping and releasing occurred in the human\ndemonstration.\nC. Learning affordance\nThe concept of Affordance, as defined by Gibson [44],\nrefers to the potential for action that objects or situations\nin an environment provide to an individual. In the field of\nrobotics, it often pertains to the meaning of executable actions\nin that environment, and information about areas where action\nis possible. For instance, Ahn et al. have proposed an approach\nthat calculates the feasibility of robotic functions from visual\ninformation and compares it with planned tasks [5]. Huang et\nal. proposed using LLMs/VLMs to extract the knowledge of\nmovable area [45].\nThese studies indeed define Affordance as a form of\nGibson’s Affordance; however, focusing on the relationship\nbetween the working environment, the objects being manip-\nulated, and the robot, it can be considered that object ma-\nnipulation involves even more constraints [46]. For example,\nthe notion of affordance can be extended to waypoints for\ncollision avoidance [47], grasp types [48], and upper-limb\npostures [49]. This information is often not taught explicitly,\nthus vision systems need to extract it from human teaching\ndemonstrations. In this study, we propose a pipeline to extract\nthis information and provide a task plan endowed with that\naffordance information.\nIII. MULTIMODAL TASK PLANNER\nThe proposed system is composed of two pipelines con-\nnected in series (Fig. 2). The first pipeline, the so-called\nsymbolic task planner, takes teaching videos, text, or both as\ninput, then outputs a sequence of robot actions. Here, the text\ninput includes feedback on the GPT-4V’s recognition results']","The concept of Affordance, as defined by Gibson [44], refers to the potential for action that objects or situations in an environment provide to an individual. In the field of robotics, it often pertains to the meaning of executable actions in that environment, and information about areas where action is possible. For instance, Ahn et al. have proposed an approach that calculates the feasibility of robotic functions from visual information and compares it with planned tasks [5]. Huang et al. proposed using LLMs/VLMs to extract the knowledge of movable area [45]. These studies indeed define Affordance as a form of Gibson’s Affordance; however, focusing on the relationship between the working environment, the objects being manipulated, and the robot, it can be considered that object manipulation involves even more constraints [46]. For example, the notion of affordance can be extended to waypoints for collision avoidance [47], grasp types [48], and upper-limb postures [49]. This information is often not taught explicitly, thus vision systems need to extract it from human teaching demonstrations. In this study, we propose a pipeline to extract this information and provide a task plan endowed with that affordance information.",simple,"[{'Authors': 'Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi', 'Published': '2023-11-20', 'Summary': ""We introduce a pipeline that enhances a general-purpose Vision Language\nModel, GPT-4V(ision), by integrating observations of human actions to\nfacilitate robotic manipulation. This system analyzes videos of humans\nperforming tasks and creates executable robot programs that incorporate\naffordance insights. The computation starts by analyzing the videos with GPT-4V\nto convert environmental and action details into text, followed by a\nGPT-4-empowered task planner. In the following analyses, vision systems\nreanalyze the video with the task plan. Object names are grounded using an\nopen-vocabulary object detector, while focus on the hand-object relation helps\nto detect the moment of grasping and releasing. This spatiotemporal grounding\nallows the vision systems to further gather affordance data (e.g., grasp type,\nway points, and body postures). Experiments across various scenarios\ndemonstrate this method's efficacy in achieving real robots' operations from\nhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are\navailable at this project page:\nhttps://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/"", 'Title': 'GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration'}]",TRUE
"What is the focus of the research paper titled ""Isaac gym: High performance gpu-based physics simulation for robot learning"" in relation to reinforcement learning?","[' N. Rudin, A. Allshire, A. Handa, et al., “Isaac gym:\nHigh performance gpu-based physics simulation for robot learning,”\narXiv preprint arXiv:2108.10470, 2021.\n[25] G. B. Margolis, G. Yang, K. Paigwar, T. Chen, and P. Agrawal,\n“Rapid\nlocomotion\nvia\nreinforcement\nlearning,”\narXiv\npreprint']",The focus of the research paper titled 'Isaac gym: High performance gpu-based physics simulation for robot learning' in relation to reinforcement learning is rapid locomotion.,simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What is the significance of graph-based program representation in data flow analysis and compiler optimizations?,"['ciech Zaremba. Evaluating Large Language Models Trained on Code. ArXiv preprint, abs/2107.03374,\n2021b. URL https://arxiv.org/abs/2107.03374.\nChris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O’Boyle, and Hugh\nLeather. ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Op-\ntimizations. In ICLR, 2021.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau\nYih, Luke Zettlemoyer, and Mike Lewis. Incoder: A Generative Model for Code Infilling and Synthesis.\nArXiv preprint, abs/2204.05999, 2022. URL https://arxiv.org/abs/2204.05999.\nSpandan Garg, Roshanak Zilouchian Moghaddam, Colin B. Clement, Neel Sundaresan, and Chen Wu.\nDeepPERF: A Deep Learning-Based Approach For Improving Software Performance, 2022.\nURL\nhttps://arxiv.org/abs/2206.13619.\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P Bigham. Instructdial:\nimproving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pp. 505–525, 2022.\nPatrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to\nprogram better. arXiv preprint arXiv:2207.14502, 2022.\nYoussef Hamadi and Youssef Hamadi. Autonomous Search. Combinatorial Search: From Algorithms to\nSystems, 2013.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nChangwu Huang, Yuanxiang Li, and Xin Yao.\nA Survey of Automatic Parameter Tuning Methods for\nMetaheuristics. IEEE transactions on evolutionary computation, 2019.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data, 7(3):535–547, 2019.\nSam Kaufman, Phitchaya Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Roy, Amit Sabne, and Mike\nBurrows. A Learned Performance Model for Tensor Processing Units. Proceedings of Machine Learning\nand Systems, 2021.\nPascal Kerschke, Holger H Hoos, Frank Neumann, and Heike Trautmann. Automated Algorithm Selection:\nSurvey and Perspectives. Evolutionary computation, 2019.\n11\nPreprint. Under review.\nLars Kotthoff. Algorithm Selection for Combinatorial Search Problems: A Survey. Data mining and con-\nstraint programming: Foundations of a cross-disciplinary approach, 2016.\nCharles E Leiserson, Neil C Thompson, Joel S Emer, Bradley C Kuszmaul, Butler W Lampson, Daniel\nSanchez, and Tao B Schardl. There’s plenty of room at the top: What will drive computer performance\nafter moore’s law? Science, 368(6495):eaam9744, 2020.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-']","The significance of graph-based program representation in data flow analysis and compiler optimizations is that it provides a structured and visual representation of the program's control flow and data dependencies. This allows for more efficient analysis and optimization techniques to be applied, leading to improved program performance and reliability.",simple,"[{'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}]",TRUE
What is included in the Open X-Embodiment Dataset for robot learning?,"['hexagon\ntriangle\nheart\ncube\ntray\nbowl\npot\nbox\ncup\nbasket\ncounter\ndrawer\ntable\ncabinet\ndoor\nchair\napple\norange\nbanana\ncoke can\nchip bag\nfaucet\nfridge\nsink\nstove\nmicrowave\noven\nfork\nspoon\nknife\nspatula\nFig. 2: The Open X-Embodiment Dataset. (a): the dataset consists of 60 individual datasets across 22 embodiments. (b): the Franka robot\nhas the largest diversity in visually distinct scenes due to the large number of Franka datasets, (c): xArm and Google Robot contribute\nthe most number of trajectories due to a few large datasets, (d, e): the dataset contains a great diversity of skills and common objects.\nspanning grasping [60–71], pushing interactions [23, 72–74],\nsets of objects and models [75–85], and teleoperated demon-\nstrations [8, 86–95]. With the exception of RoboNet [23],\nthese datasets contain data of robots of the same type,\nwhereas we focus on data spanning multiple embodiments.\nThe goal of our data repository is complementary to these\nefforts: we process and aggregate a large number of prior\ndatasets into a single, standardized repository, called Open\nX-Embodiment, which shows how robot learning datasets\ncan be shared in a meaningul and useful way.\nLanguage-conditioned robot learning. Prior work has\naimed to endow robots and other agents with the ability to\nunderstand and follow language instructions [96–101], often\nby learning language-conditioned policies [8, 40, 45, 102–\n106]. We train language-conditioned policies via imitation\nlearning like many of these prior works but do so using\nlarge-scale multi-embodiment demonstration data. Following\nprevious works that leverage pre-trained language embed-\ndings [8, 40, 45, 103, 107–112] and pre-trained vision-\nlanguage models [9, 113–115] in robotic imitation learning,\nwe study both forms of pre-training in our experiments,\nspecifically following the recipes of RT-1 [8] and RT-2 [9].\nIII. THE OPEN X-EMBODIMENT REPOSITORY\nWe\nintroduce\nthe\nOpen\nX-Embodiment\nRepository\n(robotics-transformer-x.github.io) – an open-source reposi-\ntory which includes large-scale data along with pre-trained\nmodel checkpoints for X-embodied robot learning research.\nMore specifically, we provide and maintain the following\nopen-source resources to the broader community:\n• Open X-Embodiment Dataset: robot learning dataset\nwith 1M+ robot trajectories from 22 robot embodi-\nments.\n• Pre-Trained Checkpoints: a selection of RT-X model\ncheckpoints ready for inference and finetuning.\nWe intend for these resources to form a foundation for X-\nembodiment research in robot learning, but they are just\nthe start. Open X-Embodiment is a community-driven effort,\ncurrently involving 21 institutions from around the world,\nand we hope to further broaden participation and grow the\ninitial Open X-Embodiment Dataset over time. In this sec-\ntion, we summarize the dataset and X-embodiment learning\nframework, before discussing the specific models we use to\nevaluate our dataset and our experimental results.\nA. The Open X-Embodiment Dataset'
 'tion, we summarize the dataset and X-embodiment learning\nframework, before discussing the specific models we use to\nevaluate our dataset and our experimental results.\nA. The Open X-Embodiment Dataset\nThe Open X-Embodiment Dataset contains 1M+ real robot\ntrajectories spanning 22 robot embodiments, from single\nrobot arms to bi-manual robots and quadrupeds. The dataset\nwas constructed by pooling 60 existing robot datasets from\n34 robotic research labs around the world and converting\nthem into a consistent data format for easy download and\nusage. We use the RLDS data format [119], which saves data\nin serialized tfrecord files and accommodates the various\naction spaces and input modalities of different robot setups,\nsuch as differing numbers of RGB cameras, depth cameras\nand point clouds. It also supports efficient, parallelized data\nloading in all major deep learning frameworks. For more\ndetails about the data storage format and a breakdown of all\n60 datasets, see robotics-transformer-x.github.io.\nB. Dataset Analysis\nFig. 2 analyzes the Open X-Embodiment Dataset. Fig. 2(a)\nshows the breakdown of datasets by robot embodiments,\nwith the Franka robot being the most common. This is\nreflected in the number of distinct scenes (based on dataset\nmetadata) per embodiment (Fig. 2(b)), where Franka dom-\ninates. Fig. 2(c) shows the breakdown of trajectories per\nembodiment. To further analyze the diversity, we use the\nlanguage annotations present in our data. We use the PaLM\nlanguage model [3] to extract objects and behaviors from\nDiscrete \nAction\nPick apple from top drawer \nand place on counter\nFiLM EﬃcientNet\nTransformer\n(1+γ)               β\n·\n+\nPick up the orange fruit\nRoute cable\n 10 Hz\n Closed Gripper\n Velocity\n Z-Rot. Velocity\n 3 Hz\n Gripper\n Position Delta\n Rotation Delta\n 5 Hz\n Gripper\n Position Delta\n No Rotation\nRT-1-X\nRT-2-X\nLLM\nViT\nDe-Tokenizer\nDiscrete \nAction\n Images\nInstruction\n Image\nInstruction\nDiscrete \nAction\nDiscrete \nAction\nFig. 3: RT-1-X and RT-2-X both take images and a text instruction as input and output discretized end-effector actions. RT-1-X is an\narchitecture designed for robotics, with a FiLM [116] conditioned EfficientNet [117] and a Transformer [118]. RT-2-X builds on a VLM\nbackbone by representing actions as another language, and training action text tokens together with vision-language data.\nthe instructions. Fig. 2(d,e) show the diversity of skills and\nobjects. While most skills belong to the pick-place family,\nthe long tail of the dataset contains skills like “wiping” or\n“assembling”. Additionally, the data covers a range of house-\nhold objects, from appliances to food items and utensils.\nIV. RT-X DESIGN\nTo evaluate how much X-embodiment training can im-\nprove the performance of learned policies on individual\nrobots, we require models that have sufficient capacity to\nproductively make use of such large and heterogeneous\ndatasets. To that end, our experiments will build on two']","The Open X-Embodiment Dataset contains 1M+ real robot trajectories spanning 22 robot embodiments, from single robot arms to bi-manual robots and quadrupeds. The dataset was constructed by pooling 60 existing robot datasets from 34 robotic research labs around the world and converting them into a consistent data format for easy download and usage.",reasoning,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
What is the composition of the Open X-Embodiment Dataset in terms of robot embodiments and trajectories?,"['hexagon\ntriangle\nheart\ncube\ntray\nbowl\npot\nbox\ncup\nbasket\ncounter\ndrawer\ntable\ncabinet\ndoor\nchair\napple\norange\nbanana\ncoke can\nchip bag\nfaucet\nfridge\nsink\nstove\nmicrowave\noven\nfork\nspoon\nknife\nspatula\nFig. 2: The Open X-Embodiment Dataset. (a): the dataset consists of 60 individual datasets across 22 embodiments. (b): the Franka robot\nhas the largest diversity in visually distinct scenes due to the large number of Franka datasets, (c): xArm and Google Robot contribute\nthe most number of trajectories due to a few large datasets, (d, e): the dataset contains a great diversity of skills and common objects.\nspanning grasping [60–71], pushing interactions [23, 72–74],\nsets of objects and models [75–85], and teleoperated demon-\nstrations [8, 86–95]. With the exception of RoboNet [23],\nthese datasets contain data of robots of the same type,\nwhereas we focus on data spanning multiple embodiments.\nThe goal of our data repository is complementary to these\nefforts: we process and aggregate a large number of prior\ndatasets into a single, standardized repository, called Open\nX-Embodiment, which shows how robot learning datasets\ncan be shared in a meaningul and useful way.\nLanguage-conditioned robot learning. Prior work has\naimed to endow robots and other agents with the ability to\nunderstand and follow language instructions [96–101], often\nby learning language-conditioned policies [8, 40, 45, 102–\n106]. We train language-conditioned policies via imitation\nlearning like many of these prior works but do so using\nlarge-scale multi-embodiment demonstration data. Following\nprevious works that leverage pre-trained language embed-\ndings [8, 40, 45, 103, 107–112] and pre-trained vision-\nlanguage models [9, 113–115] in robotic imitation learning,\nwe study both forms of pre-training in our experiments,\nspecifically following the recipes of RT-1 [8] and RT-2 [9].\nIII. THE OPEN X-EMBODIMENT REPOSITORY\nWe\nintroduce\nthe\nOpen\nX-Embodiment\nRepository\n(robotics-transformer-x.github.io) – an open-source reposi-\ntory which includes large-scale data along with pre-trained\nmodel checkpoints for X-embodied robot learning research.\nMore specifically, we provide and maintain the following\nopen-source resources to the broader community:\n• Open X-Embodiment Dataset: robot learning dataset\nwith 1M+ robot trajectories from 22 robot embodi-\nments.\n• Pre-Trained Checkpoints: a selection of RT-X model\ncheckpoints ready for inference and finetuning.\nWe intend for these resources to form a foundation for X-\nembodiment research in robot learning, but they are just\nthe start. Open X-Embodiment is a community-driven effort,\ncurrently involving 21 institutions from around the world,\nand we hope to further broaden participation and grow the\ninitial Open X-Embodiment Dataset over time. In this sec-\ntion, we summarize the dataset and X-embodiment learning\nframework, before discussing the specific models we use to\nevaluate our dataset and our experimental results.\nA. The Open X-Embodiment Dataset'
 'tion, we summarize the dataset and X-embodiment learning\nframework, before discussing the specific models we use to\nevaluate our dataset and our experimental results.\nA. The Open X-Embodiment Dataset\nThe Open X-Embodiment Dataset contains 1M+ real robot\ntrajectories spanning 22 robot embodiments, from single\nrobot arms to bi-manual robots and quadrupeds. The dataset\nwas constructed by pooling 60 existing robot datasets from\n34 robotic research labs around the world and converting\nthem into a consistent data format for easy download and\nusage. We use the RLDS data format [119], which saves data\nin serialized tfrecord files and accommodates the various\naction spaces and input modalities of different robot setups,\nsuch as differing numbers of RGB cameras, depth cameras\nand point clouds. It also supports efficient, parallelized data\nloading in all major deep learning frameworks. For more\ndetails about the data storage format and a breakdown of all\n60 datasets, see robotics-transformer-x.github.io.\nB. Dataset Analysis\nFig. 2 analyzes the Open X-Embodiment Dataset. Fig. 2(a)\nshows the breakdown of datasets by robot embodiments,\nwith the Franka robot being the most common. This is\nreflected in the number of distinct scenes (based on dataset\nmetadata) per embodiment (Fig. 2(b)), where Franka dom-\ninates. Fig. 2(c) shows the breakdown of trajectories per\nembodiment. To further analyze the diversity, we use the\nlanguage annotations present in our data. We use the PaLM\nlanguage model [3] to extract objects and behaviors from\nDiscrete \nAction\nPick apple from top drawer \nand place on counter\nFiLM EﬃcientNet\nTransformer\n(1+γ)               β\n·\n+\nPick up the orange fruit\nRoute cable\n 10 Hz\n Closed Gripper\n Velocity\n Z-Rot. Velocity\n 3 Hz\n Gripper\n Position Delta\n Rotation Delta\n 5 Hz\n Gripper\n Position Delta\n No Rotation\nRT-1-X\nRT-2-X\nLLM\nViT\nDe-Tokenizer\nDiscrete \nAction\n Images\nInstruction\n Image\nInstruction\nDiscrete \nAction\nDiscrete \nAction\nFig. 3: RT-1-X and RT-2-X both take images and a text instruction as input and output discretized end-effector actions. RT-1-X is an\narchitecture designed for robotics, with a FiLM [116] conditioned EfficientNet [117] and a Transformer [118]. RT-2-X builds on a VLM\nbackbone by representing actions as another language, and training action text tokens together with vision-language data.\nthe instructions. Fig. 2(d,e) show the diversity of skills and\nobjects. While most skills belong to the pick-place family,\nthe long tail of the dataset contains skills like “wiping” or\n“assembling”. Additionally, the data covers a range of house-\nhold objects, from appliances to food items and utensils.\nIV. RT-X DESIGN\nTo evaluate how much X-embodiment training can im-\nprove the performance of learned policies on individual\nrobots, we require models that have sufficient capacity to\nproductively make use of such large and heterogeneous\ndatasets. To that end, our experiments will build on two']","The Open X-Embodiment Dataset contains 1M+ real robot trajectories spanning 22 robot embodiments, from single robot arms to bi-manual robots and quadrupeds.",multi_context,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
What is the title and publication year of the large-scale grasp dataset presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition?,"['of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 13 778–13 790.\n[60] Y. Jiang, S. Moseson, and A. Saxena, “Efficient\ngrasping from RGBD images: Learning using a new\nrectangle representation,” in 2011 IEEE International\nconference on robotics and automation.\nIEEE, 2011,\npp. 3304–3311.\n[61] L.\nPinto\nand\nA.\nK.\nGupta,\n“Supersizing\nself-\nsupervision: Learning to grasp from 50k tries and\n700 robot hours,” 2016 IEEE International Conference\non Robotics and Automation (ICRA), pp. 3406–3413,\n2015.\n[62] D. Kappler, J. Bohg, and S. Schaal, “Leveraging big\ndata for grasp planning,” in ICRA, 2015, pp. 4304–\n4311.\n[63] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan,\nX. Liu, J. A. Ojea, and K. Goldberg, “Dex-Net 2.0:\nDeep learning to plan robust grasps with synthetic\npoint clouds and analytic grasp metrics,” in Robotics:\nScience and Systems (RSS), 2017.\n[64] A. Depierre, E. Dellandr´\nea, and L. Chen, “Jacquard:\nA large scale dataset for robotic grasp detection,” in\n2018 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS).\nIEEE, 2018, pp.\n3511–3516.\n[65] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and\nD. Quillen, “Learning hand-eye coordination for\nrobotic grasping with deep learning and large-scale\ndata collection,” The International journal of robotics\nresearch, vol. 37, no. 4-5, pp. 421–436, 2018.\n[66] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Her-\nzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan,\nV. Vanhoucke et al., “QT-Opt: Scalable deep rein-\nforcement learning for vision-based robotic manipu-\nlation,” arXiv preprint arXiv:1806.10293, 2018.\n[67] S. Brahmbhatt, C. Ham, C. Kemp, and J. Hays,\n“Contactdb: Analyzing and predicting grasp contact\nvia thermal imaging,” 04 2019.\n[68] H.-S. Fang, C. Wang, M. Gou, and C. Lu, “Graspnet-\n1billion: a large-scale benchmark for general object\ngrasping,” in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2020, pp.\n11 444–11 453.\n[69] C. Eppner, A. Mousavian, and D. Fox, “ACRONYM:\nA large-scale grasp dataset based on simulation,” in\n2021 IEEE Int. Conf. on Robotics and Automation,\nICRA, 2020.\n[70] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kel-\ncey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor,\nK. Konolige, S. Levine, and V. Vanhoucke, “Using\nsimulation and domain adaptation to improve effi-\nciency of deep robotic grasping,” in ICRA, 2018, pp.\n4243–4250.\n[71] X.\nZhu,\nR.\nTian,\nC.\nXu,\nM.\nHuo,\nW.\nZhan,\nM. Tomizuka, and M. Ding, “Fanuc manipulation:\nA dataset for learning-based manipulation with fanuc\nmate 200iD robot,” https://sites.google.com/berkeley.\nedu/fanuc-manipulation, 2023.\n[72] K.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez,\n“More than a million ways to be pushed. a high-'
 'Learning an adaptive gripper-aware grasping policy,”\nin 2021 IEEE International Conference on Robotics\nand Automation (ICRA). IEEE, 2021, pp. 4620–4626.\n[32] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz,\nK. Black, N. Hirose, and S. Levine, “ViNT: A Foun-\ndation Model for Visual Navigation,” in 7th Annual\nConference on Robot Learning (CoRL), 2023.\n[33] Y. Liu, A. Gupta, P. Abbeel, and S. Levine, “Imitation\nfrom observation: Learning to imitate behaviors from\nraw video via context translation,” in 2018 IEEE\nInternational Conference on Robotics and Automation\n(ICRA).\nIEEE, 2018, pp. 1118–1125.\n[34] T. Yu, C. Finn, S. Dasari, A. Xie, T. Zhang, P. Abbeel,\nand S. Levine, “One-shot imitation from observing hu-\nmans via domain-adaptive meta-learning,” Robotics:\nScience and Systems XIV, 2018.\n[35] P. Sharma, D. Pathak, and A. Gupta, “Third-person\nvisual imitation learning via decoupled hierarchical\ncontroller,” Advances in Neural Information Process-\ning Systems, vol. 32, 2019.\n[36] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and\nS. Levine, “Avid: Learning multi-stage tasks via pixel-\nlevel translation of human videos,” arXiv preprint\narXiv:1912.04443, 2019.\n[37] A. Bonardi, S. James, and A. J. Davison, “Learning\none-shot imitation from humans without humans,”\nIEEE Robotics and Automation Letters, vol. 5, no. 2,\npp. 3533–3539, 2020.\n[38] K. Schmeckpeper, O. Rybkin, K. Daniilidis, S. Levine,\nand C. Finn, “Reinforcement learning with videos:\nCombining offline observations with interaction,” in\nConference on Robot Learning.\nPMLR, 2021, pp.\n339–354.\n[39] H. Xiong, Q. Li, Y.-C. Chen, H. Bharadhwaj, S. Sinha,\nand A. Garg, “Learning by watching: Physical imita-\ntion of manipulation skills from human videos,” in\n2021 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS).\nIEEE, 2021, pp.\n7827–7834.\n[40] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert,\nC. Lynch, S. Levine, and C. Finn, “BC-Z: Zero-shot\ntask generalization with robotic imitation learning,”\nin Conference on Robot Learning (CoRL), 2021, pp.\n991–1002.\n[41] S. Bahl, A. Gupta, and D. Pathak, “Human-to-robot\nimitation in the wild,” Robotics: Science and Systems\n(RSS), 2022.\n[42] M. Ding, Y. Xu, Z. Chen, D. D. Cox, P. Luo,\nJ. B. Tenenbaum, and C. Gan, “Embodied concept\nlearner: Self-supervised learning of concepts and map-\nping through instruction following,” in Conference on\nRobot Learning.\nPMLR, 2023, pp. 1743–1754.\n[43] S. Bahl, R. Mendonca, L. Chen, U. Jain, and\nD. Pathak, “Affordances from human videos as a\nversatile representation for robotics,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2023, pp. 13 778–\n13 790.\n[44] P. Sermanet, K. Xu, and S. Levine, “Unsupervised per-\nceptual rewards for imitation learning,” arXiv preprint\narXiv:1612.06699, 2016.\n[45] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and\nJ. Bohg, “Concept2']",Graspnet-1billion: a large-scale benchmark for general object grasping,multi_context,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
Can GPT-4 be used as a low-level controller for robot walking without task-specific fine-tuning and relying on few-shot prompts from the physical environment?,"['Prompt a Robot to Walk with Large Language Models\nYen-Jen Wang1, Bike Zhang2, Jianyu Chen1,3, Koushil Sreenath2\nAbstract— Large language models (LLMs) pre-trained on\nvast internet-scale data have showcased remarkable capabilities\nacross diverse domains. Recently, there has been escalating\ninterest in deploying LLMs for robotics, aiming to harness\nthe power of foundation models in real-world settings. How-\never, this approach faces significant challenges, particularly\nin grounding these models in the physical world and in\ngenerating dynamic robot motions. To address these issues,\nwe introduce a novel paradigm in which we use few-shot\nprompts collected from the physical environment, enabling the\nLLM to autoregressively generate low-level control commands\nfor robots without task-specific fine-tuning. Experiments across\nvarious robots and environments validate that our method can\neffectively prompt a robot to walk. We thus illustrate how\nLLMs can proficiently function as low-level feedback controllers\nfor dynamic motion control even in high-dimensional robotic\nsystems. The project website and source code can be found at:\nprompt2walk.github.io.\nI. INTRODUCTION\nLarge language models (LLMs) pre-trained on internet-\nscale data [5], [33], [32], [9], [45] have demonstrated impres-\nsive results in various fields, e.g., natural language processing\n[29], [28], computer vision [31], code generation [7], etc.\nBuilding upon the success of LLMs, there is a surging\ninterest in utilizing LLMs for embodied agents [1], [46],\naiming to harness the power of foundation models in the\nphysical world [2]. Towards this goal, significant progress\nhas been made [4], [3], [10]. However, there are some\nremaining challenges. 1) Even though LLMs are trained with\nbroad data at scale, the dataset does not incorporate data from\nthe physical world, making it challenging to ground LLMs in\nrobot control. 2) While foundation models have been widely\nused in a pre-training and fine-tuning paradigm for robotics\napplications, there could be a paradigm shift to few-shot\nlearning in light of the progress of the natural language field\n[5]. 3) Most recent language-guided robot control research\nshowcases mainly quasi-static robot motions. It remains un-\ncertain whether LLMs can generate dynamic robot behaviors\nwithout a low-level controller interface or without relying on\npredefined motion primitives.\nIn this paper, we want to raise the intriguing question\nof whether LLMs can function as low-level controllers for\nachieving dynamic tasks like robot walking? This requires us\nto address the challenges mentioned above. We do this by\nexploring a new paradigm that leverages few-shot prompts\nwith a large language model, i.e., GPT-4, to directly output\n1Institute for Interdisciplinary Information Sciences, Tsinghua University.\n2University of California, Berkeley. 3Shanghai Qi Zhi Institute.\nThis work is supported in part by the InnoHK of the Government of the']","In this paper, we want to raise the intriguing question of whether LLMs can function as low-level controllers for achieving dynamic tasks like robot walking? This requires us to address the challenges mentioned above. We do this by exploring a new paradigm that leverages few-shot prompts with a large language model, i.e., GPT-4, to directly output",reasoning,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What is the purpose of evaluating the performance of the RT-1-X and RT-2-X models on the Bridge dataset?,"['mixture used in our experiments includes 9 embodiments\nwhich is fewer than the entire Open X-Embodiment dataset\n(22) – the practical reason for this difference is that we have\ncontinued to extend the dataset over time, and at the time\nof the experiments, the dataset above represented all of the\ndata. In the future, we plan to continue training policies on\nthe extended versions of the dataset as well as continue to\ngrow the dataset together with the robot learning community.\nAt inference time, each model is run at the rate required\nfor the robot (3-10 Hz), with RT-1 run locally and RT-2\nhosted on a cloud service and queried over the network.\nV. EXPERIMENTAL RESULTS\nOur experiments answer three questions about the effect\nof X-embodiment training: (1) Can policies trained on our\nX-embodiment dataset effectively enable positive transfer,\nsuch that co-training on data collected on multiple robots\nimproves performance on the training task? (2) Does co-\ntraining models on data from multiple platforms and tasks\nimprove generalization to new, unseen tasks? (3) What is the\ninfluence of different design dimensions, such as model size,\nmodel architecture or dataset composition, on performance\nand generalization capabilities of the resulting policy? To\nanswer these questions we conduct the total number of 3600\nevaluation trials across 6 different robots.\nA. In-distribution performance across different embodiments\nTo assess the ability of RT-X models to learn from X-\nembodiment data, we evaluate performance on in-distribution\ntasks. We split our evaluation into two types: evaluation on\ndomains that have small-scale datasets (Fig. 4), where we\nwould expect transfer from larger datasets to significantly\nimprove performance, and evaluation on domains that have\nlarge-scale datasets (Table I), where we expect further im-\nprovement to be more challenging. Note that we use the\nsame robotics data training mixture (defined in Sec. IV-C) for\nall the evaluations presented in this section. For small-scale\ndataset experiments, we use Kitchen Manipulation [128],\nCable Routing [129], NYU Door Opening [130], AUTOLab\nUR5 [132], and Robot Play [134]. We use the same evalua-\ntion and robot embodiment as in the respective publications.\nFor large-scale dataset experiments, we consider Bridge [95]\nand RT-1 [8] for in-distribution evaluation and use their\nrespective robots: WidowX and Google Robot.\nFor each small dataset domain, we compare the perfor-\nmance of the RT-1-X model, and for each large dataset\nwe consider both the RT-1-X and RT-2-X models. For\nall experiments, the models are co-trained on the full X-\nembodiment dataset. Throughout this evaluation we compare\nwith two baseline models: (1) The model developed by\nthe creators of the dataset trained only on that respective\ndataset. This constitutes a reasonable baseline insofar as\nit can be expected that the model has been optimized to\nwork well with the associated data; we refer to this baseline']",To assess the ability of the RT-1-X and RT-2-X models to perform on the Bridge dataset.,simple,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
What is dexterity intelligence and how does it relate to robotics?,"['Large Language Models for Robotics: A Survey\nFanlong Zenga, Wensheng Gana,∗, Yongheng Wanga, Ning Liua and Philip S. Yub\naSchool of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China\nbDepartment of Computer Science, University of Illinois Chicago, Chicago, USA\nA R T I C L E I N F O\nKeywords:\nlarge language models\nrobotics\ncontrol and interaction\ndecision-making\nembodied intelligence\nA B S T R A C T\nThe human ability to learn, generalize, and control complex manipulation tasks through multi-\nmodality feedback suggests a unique capability, which we refer to as dexterity intelligence. Under-\nstanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field of robotics have\ngarnered increasing attention. LLMs possess the ability to process and generate natural language,\nfacilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of\nrobotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot\ninteraction, and autonomy. Therefore, this comprehensive review aims to summarize the applications\nof LLMs in robotics, delving into their impact and contributions to key areas such as robot control,\nperception, decision-making, and path planning. We first provide an overview of the background and\ndevelopment of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and\nrecent advancements in robotics models based on LLMs. We then delve into the various techniques\nused in the model, including those employed in perception, decision-making, control, and interaction.\nFinally, we explore the applications of LLMs in robotics and some potential challenges they may face\nin the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics\nis one of the promising but challenging paths to achieve this.\n1. Introduction\nHumans possess exceptional proficiency in executing\nintricate and dexterous manipulation skills by integrating\ntactile, visual, and other sensory inputs. Research in the\nfield of robotics aspires to imbue robots with comparable\nmanipulation intelligence. Although recent advancements\nin robotics and machine learning have yielded promising\nresults in visual mitigation and exploration learning for robot\nmanipulation, there remains much to be accomplished in this\narea. Large language models (LLMs), such as BERT [31],\nRoberta [79], GPT-3 [27], GPT-4 [110], have emerged as\nsignificant research achievements in the field of artificial\nintelligence (AI) in recent years. Through deep learning\ntechniques [76], LLMs can be trained on massive text cor-\npora, enabling them to generate high-quality natural lan-\nguage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology']","The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"What is the severe collision rate for the Exploration Path, Motion Cost Planner, and Exploration Path w/ Cost Optimizer?","['Exploration Path only has a slightly higher general collision rate at 10.93% but the highest rate of severe\ncollisions at 2.44%. This is due to the fact that the exploration planner performs collision checking in a\ncoarser, volumetric map. This map captures most larger obstacles, which leads to a low general collision\nrate. Small obstacles, however, can be completely missed, which leads to severe collisions in these instances.\nMotion Cost Planner and Exploration Path w/ Cost Optimizer have very high general collision rates at\n25.44% and 37.17%, respectively, but comparatively much lower severe collision rates, at 0.86% and 1.70%.\nThis is due to the fact that, when training the cost network, collisions with the environment are allowed, as\nlong as the robot still reaches its intended goal. This means paths frequently slightly graze some obstacles\nwithout severely colliding. While mostly not fatal, in practice even slight bumps into obstacles should be\navoided since they can get the robot stuck on protruding elements or damage its payload. The higher severe\ncollision rate of the Exploration Path w/ Cost Optimizer was caused by the colliding initial exploration path,\nwhich the gradient based cost optimizer could not ﬁx.\nTaking a look at the collision heat maps in Figure 7 allows us to determine in which situations these planners\nstruggled. Figure 7\nshows an instance where the Exploration Path passes through obstructing traﬃc cones.\nNote that this does not show up as a severe hot spot on the heat map due to Exploration Path pruning\nfor collision checking, as discussed in Section 3.1.2. The Motion Cost Planner generally had few collisions,\nexcept in a narrow cave section, where the optimized path performs a turn due to bad initialization (Figure 7\n). The Exploration Path w/ Cost Optimizer collisions were generally caused by the optimizer not dealing\nwell with height map artifacts (Figure 7\n+ ).\n3.1.5\nPlanning Time\nSince we did not log planning times of our planner during the competition, and GBPlanner2 has a diﬀerent\nscope to the other navigation planners, we compare the planning times of all methods during playback.\nPlanning times are shown as box plots in Figure 8. Our chosen target update rate was to publish a new\npath every 2 seconds and the real-time threshold for our 8 m×8 m map at a locomotion speed of 0.9 m s−1\nwas 4.44 s. The real-time threshold (Wellhausen and Hutter, 2021) is the time the robot requires to reach\nthe edge of the height map at maximal speed.\nWe set ArtPlanner’s maximal sampling time T to 2 seconds, which does not factor in map processing and\nmotion cost query time. Therefore, ArtPlanner can exceed the target time if the maximum sampling time is\nreached, with a maximal planning time of 4.65 s. Consequently, we achieved the target time in 75% of cases,\nexceeding the real-time threshold only a single time, by 0.21 s. The No Motion Cost planner has fast query']","The severe collision rate for the Exploration Path is 2.44%, for the Motion Cost Planner is 0.86%, and for the Exploration Path w/ Cost Optimizer is 1.70%.",simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
What do the target joint trajectories for the front left leg look like when a robot is walking on uneven terrain for 10 seconds?,"['the policy at 10 Hz. However, this leads to a walking gait\nthat becomes reasonably worse compared to many RL-based\nwalking policies running at 50 Hz or even higher.\nFig. 4 demonstrates target joint trajectories for the front\nleft leg when a robot is walking on uneven terrain for 10\nseconds. The blue lines depict the trajectories produced by\nthe LLM policy. As a comparison, the orange lines show\nthe trajectories generated by an RL policy. Note that both\ntrajectories take the same observation as input. The robot\nacts with the action generated by the LLM and then gets the\nnext observation from the environment. Although the LLM\npolicy is initialized with the RL policy, the resulting joint\ntrajectories are noticeably different.\nOne prompt example for A1 robot walking is shown in\nFig. 3, where we use historical observations and actions\nfor the past 50 steps. The prompt is specially designed and\nnormalized as described in Sec. II-B. Based on this A1 robot\nwalking experiment, we can answer Question Q1 that a robot\ncan be prompted to walk with LLMs.\nC. Description Prompt\nWe perform 5 experiments to analyze the impact of indi-\nvidual components in the description prompt. In each exper-\niment, we provide observation and action prompts (PHist).\nFor evaluation, we consider two metrics: normalized walking\ntime and success rate. To clarify, the term “normalized\nwalking time” denotes the proportion of time a robot can\nwalk before it falls. The success rate is measured by the\npercentage of the trials that the robot is able to finish, where\neach trial lasts for 10 seconds and we have 5 trials for each\nexperiment. In the design of the first experiment (E1), we\nexclude the description prompt entirely (only PHist). In the\n0\n10\n30\n50\nLength of Historical Observations & Actions\n0.00\n0.25\n0.50\n0.75\nValue\nNormalized Walking Time\nSuccess Rate\nFig. 6: Observation and Action Length Comparison. We\nconduct experiments for historical observation and action\nlengths as 0, 10, 30, and 50. With lengths ranging from\n0 to 50, the LLM token consumptions are approximately\n348, 1738, 4518, and 7298, respectively.\nsecond experiment (E2), we only provide the meaning of\ninput and output space (PIO). Additionally, we include the\njoint order (PIO + PJO) in the third experiment (E3). In the\nfourth experiment (E4), we incorporate prompts such as task\ndescription, meaning of input and output space, joint order,\nand the full control pipeline (PT D + PIO + PJO + PCP ).\nFor the fifth experiment (E5), we employed a complete\ndescription prompt. The experimental result is demonstrated\nin Fig. 5, where we can see that the full description prompt\nhas the highest normalized walking time and success rate.\nBased on the results from the first experiment, without a\ndescription prompt (E1), there is a minimal likelihood of\nLLMs prompting a robot to walk.\nD. Observation and Action Prompt\nIn our subsequent investigation, we assess the influence\nof the observation and action prompt PHist on walking per-']","The blue lines depict the trajectories produced by the LLM policy. As a comparison, the orange lines show the trajectories generated by an RL policy.",simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What are the capabilities of the RT-2-X robot?,"['RT-2-X have exhibited enhanced capabilities.\ncapable of unlocking limitless productivity. The company’s\nfounding ethos is centered around the belief that ""intelligent\nrobots can create unlimited productivity"" when designed\nto parallel human flexibility and intelligence. Expedition\nA1 is a humanoid robot equipped with reflex knee joints,\ndesigned to resemble a human form. This design choice\nstems from the fact that most work environments are cur-\nrently tailored for human functionality. Humanoid robots\nare allowed to seamlessly integrate and function without\nrequiring significant environmental modifications. A key\nadvantage of humanoid robots is their strong generalization\ncapabilities, enabling them to adapt to diverse situations.\nWhile the Expedition A1 can also swap out components,\nsuch as replacing legs with tires, mimicking human move-\nment and perception remains a significant challenge for\nrobots. Expedition A1 integrates cutting-edge perception,\ncontrol, and decision-making technologies, incorporating\nboth a state-of-the-art language model and an independently\ndeveloped visual model. Designed with industrial manufac-\nturing in mind, it boasts 49 degrees of freedom, surpassing\nthe limitations of traditional robots with only 20 degrees of\nfreedom. Its high degree of freedom enables it to meet vari-\nous industrial manufacturing requirements. The Expedition\nA1 is also modular, allowing for autonomous component\nreplacement. For instance, PowerFlow is a joint motor for\nenhanced flexibility, while SkillHand features vision-based\nfingertip sensors for precision manufacturing scene design.\nIn addition to its robust hardware, the Expedition A1 utilizes\nLLM as its brain, complemented by EI-Brain’s embodied\nintelligence framework. This framework divides the robot’s\nsystem into different levels of management, including Expe-\ndition A1’s super brain in the cloud, local brain, cerebellum,\nand brainstem, each corresponding to diverse task levels.\n2.4. New Transformer Architecture for Robotics\nIn this part, we introduce the Transformer designed for\nrobotics. We summarize the Transformer for robotics in\nrecent years in Table 3.\n2.4.1. Control Transformer\nReinforcement learning [94] methods struggle to effec-\ntively tackle long-horizon tasks like navigation, but from\na different angle, sample-based path planning techniques\ncan discover collision-free paths without the need for learn-\ning in a known environment. Control Transformer (CT)\n[75] utilizes a sample-based probabilistic road map (PRM)\n[67] planner to generate conditional sequences from low-\nlevel policy, enabling it to complete navigation tasks solely\nthrough local information. CT has been shown to be effective\nin complex terrain and unknown environments through rele-\nvant experiments. By leveraging local observations, CT can\nsolve long-horizon and robot navigation tasks. Following\ntraining, CT can obtain a policy and complete navigation\nfrom partially observed or unknown environments. CT is a']","The RT-2-X robot has exhibited enhanced capabilities and is capable of unlocking limitless productivity. It is designed to parallel human flexibility and intelligence, with features such as reflex knee joints and a humanoid form. It has strong generalization capabilities and can adapt to diverse situations. The robot integrates cutting-edge perception, control, and decision-making technologies, with a state-of-the-art language model and visual model. It has 49 degrees of freedom and is modular, allowing for autonomous component replacement. The robot utilizes LLM as its brain, complemented by EI-Brain's embodied intelligence framework.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
"Why is normalizing target joint positions important in LLM policy for robot walking control, considering text prompts and challenges in comprehending numerical values?","['LLM Prompt:\nDescription Prompt\nObservation and \nAction Prompt\nFig. 3: Text Prompt. We design a text prompt that includes two parts: a description prompt and an observation and action\nprompt. In the description prompt, we have the following subparts: PT D: task description, PIO: meaning of input and output\nspace, PJO: joint order, PCP : full control pipeline, and PAI: additional illustration. In the observation and action prompt,\nwe have PHist: historical observations and actions. The LLM outputs normalized target joint positions.\nto understand the context of the inputs and actions. Then,\nan explicit enumeration of the joint order of our robot is\nprovided in PJO to guide the LLM to comprehend the robot\nconfiguration. Additionally, we specify in the prompt PAI\nthat all the values LLMs encounter are not raw data. Instead,\nthese numerical values have been normalized. Lastly, the\nprompt offers an overview of the entire control pipeline\nin PCP , granting the LLM a macro perspective on how\nindividual components enabling it to process and interlink. It\nis crucial to highlight that, unlike classic learning-based and\nmodel-based walking controllers, text serves an important\nrole in the LLM policy.\nObservation and Action Prompt. A sequence of observa-\ntion and action pairs PHist are used as prompts. These pairs\nare generated from the recent history of the robot walking\ntrajectory. This procedure is widely used in RL-based robot\nwalking controllers, where it allows the neural network to\ninfer the dynamics as well as the privileged environment\ninformation. With a sequence of observation and action\nprompts, LLMs can in-context learn the dynamics and infer a\nreactive control action, where the observation prompt serves\nas the feedback signal. Note that both observation and action\nare converted to text format to interface with LLMs.\nLLMs often struggle to comprehend the significance of\nnumeric values, particularly floating point and negative num-\nbers. Inspired by the prompt design in [26], we adopt a\nnormalization approach for numerical values. Specifically,\nwe use a linear transformation to map all the potential\nnumeric values into non-negative integers, ranging from 0\nto 200. We hypothesize that LLMs are mostly trained with\ntext tokens, thus they are not sensitive enough to numerical\nvalues for robot control.\nC. Grounding LLMs\nIn order to make LLMs useful for robot walking control,\nwe need to ground them in a physical environment. We now\nintroduce the pipeline to allow LLMs to interact with a robot\nand an environment. We use a physics-based simulator where\nLLMs can get observations and send actions. The observa-\ntions are from the physics-based simulation. The output of\nthe LLM is the target joint positions, which are tracked by a\nset of joint Proportional-Derivative (PD) controllers running\nat a higher frequency. This joint-level PD control design is\nstandard for learning-based robot walking control. While this']","Normalizing target joint positions is important in LLM policy for robot walking control because LLMs are mostly trained with text tokens and are not sensitive enough to numerical values. By using a linear transformation to map all potential numeric values into non-negative integers, ranging from 0 to 200, LLMs can better comprehend and process the numerical values for robot control. This allows LLMs to effectively interact with a physical environment and generate target joint positions that can be tracked by joint Proportional-Derivative (PD) controllers for robot walking control.",reasoning,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
"How do LLMs contribute to dexterity intelligence in robotics, particularly in robot control, perception, decision-making, and path planning?","['Large Language Models for Robotics: A Survey\nFanlong Zenga, Wensheng Gana,∗, Yongheng Wanga, Ning Liua and Philip S. Yub\naSchool of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China\nbDepartment of Computer Science, University of Illinois Chicago, Chicago, USA\nA R T I C L E I N F O\nKeywords:\nlarge language models\nrobotics\ncontrol and interaction\ndecision-making\nembodied intelligence\nA B S T R A C T\nThe human ability to learn, generalize, and control complex manipulation tasks through multi-\nmodality feedback suggests a unique capability, which we refer to as dexterity intelligence. Under-\nstanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field of robotics have\ngarnered increasing attention. LLMs possess the ability to process and generate natural language,\nfacilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of\nrobotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot\ninteraction, and autonomy. Therefore, this comprehensive review aims to summarize the applications\nof LLMs in robotics, delving into their impact and contributions to key areas such as robot control,\nperception, decision-making, and path planning. We first provide an overview of the background and\ndevelopment of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and\nrecent advancements in robotics models based on LLMs. We then delve into the various techniques\nused in the model, including those employed in perception, decision-making, control, and interaction.\nFinally, we explore the applications of LLMs in robotics and some potential challenges they may face\nin the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics\nis one of the promising but challenging paths to achieve this.\n1. Introduction\nHumans possess exceptional proficiency in executing\nintricate and dexterous manipulation skills by integrating\ntactile, visual, and other sensory inputs. Research in the\nfield of robotics aspires to imbue robots with comparable\nmanipulation intelligence. Although recent advancements\nin robotics and machine learning have yielded promising\nresults in visual mitigation and exploration learning for robot\nmanipulation, there remains much to be accomplished in this\narea. Large language models (LLMs), such as BERT [31],\nRoberta [79], GPT-3 [27], GPT-4 [110], have emerged as\nsignificant research achievements in the field of artificial\nintelligence (AI) in recent years. Through deep learning\ntechniques [76], LLMs can be trained on massive text cor-\npora, enabling them to generate high-quality natural lan-\nguage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology']","LLMs contribute to dexterity intelligence in robotics by enhancing robot control, perception, decision-making, and path planning. They can process and generate natural language, facilitating efficient interaction and collaboration with robots. LLMs have the potential to improve robot intelligence, human-robot interaction, and autonomy in the field of robotics.",reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
How were height map collisions considered in the evaluation of the planners and what were the results in terms of motion cost and collision rate?,"['• GBPlanner2: Exploration planner (Kulkarni et al., 2022) as run during the competition.\n• ArtPlanner (Playback): Our method using data from the competition played back.\n• No Motion Cost: A reachability-only planner (Wellhausen and Hutter, 2021), which we fed the\nheight map processed as proposed in this work.\n• Motion Cost Planner: Planner based purely on the motion cost network (Yang et al., 2021a). It ﬁrst\ncomputes a raw path, by querying motions in a ﬁxed graph pattern, with heuristics to determine\nrobot orientation.\nThis raw path is then optimized with gradient-based optimization using the\nmotion cost network.\n• Exploration Path w/ Cost Optimizer: GBPlanner2 (Kulkarni et al., 2022) already outputs a path\nwhich is mostly generally feasible, but optimizes for information gain and plans on a coarser map.\nWe therefore fed the exploration path directly into the motion cost optimizer (Yang et al., 2021a)\nto obtain a reﬁned navigation path.\n3.1.2\nMethodology\nWe used the motion cost network to compute path costs for all path segments inside the height map. The\nsame network architecture and weights were used for evaluation and in the planners which use the cost\nnetwork themselves.\nSince the planners mentioned above have diﬀerent characteristics, implementation details, and produce paths\nof diﬀerent lengths, we took extra considerations to guarantee a fair comparison. We check height map\ncollisions only for the torso of the robot, analogous to the blue boxes shown in Figure 4(a). We also reduced\nthe size of these boxes by 10cm in all dimensions compared to the size used by the reachability planners.\nThis was done to account for the more accurate collision models used when training the motion cost network.\nFinally, we disregarded the ﬁrst pose of all output paths, since it corresponds to the current robot pose.\nThis means that it has to be valid and any detected collisions would be erroneous. The reachability planners\naccount for this by sampling a valid pose in a small region around the start, but the motion cost planners\ndo not have that functionality. When evaluating the Exploration Path, we only consider path poses from the\nNo Motion Cost [1]\nArtPlanner (Playback)\nMotion Cost\nPlanner [2]\nExploration Path [3] \nw/ Cost Optimizer [2]\nExploration Path [3]\nArtPlanner\n(Competition)\nCompetition\nPlayback\n0.0\n0.5\nCost\n3%\n0%\nCollision Rate\n3\n5\n5\n5\n3\n7\n5\n4\n1\n6\n6\n6\n2\n1\n2\n7\n4\n1\n2\n7\n3 5\n6\n4\nFigure 7: Comparison of path costs and path collision rate for diﬀerent planners during the SubT Finals Prize Run.\nThe two left-most columns show data from the planners as run on the robots during the competition, while the other\ncolumns show real robot data played back. Data is smoothed with a Gaussian ﬁlter with standard deviation of 2m\nfor better visibility.\nThe Exploration Path was highly risky whereas ArtPlanner avoids the high rail track.\nThe\nExploration Path was infeasible as it ignored the obstructing traﬃc cones.\nWith No Motion Cost the path was very'
 'The\nExploration Path was infeasible as it ignored the obstructing traﬃc cones.\nWith No Motion Cost the path was very\nclose to obstacles whereas ArtPlanner kept a safe distance.\nDue to a bad initial guess, the Motion Cost Planner\nturned on the train tracks whereas ArtPlanner turned beforehand and walked straight on the tracks.\nDue to a\nbad initial guess the Motion Cost Planner performed an unnecessary turn, which caused the path cost optimizer to\nproduce a colliding path in the narrow cave tunnel.\nThe initial exploration path was in collision with height map\nartifacts, leading to suboptimal behavior of the path optimizer.\nIn trying to avoid map noise, the path optimizer\npushed the path into the tunnel wall.\n[1] (Wellhausen and Hutter, 2021) [2] (Yang et al., 2021a) [3] (Kulkarni et al., 2022)\nCollision Rate [%]\nMotion Cost\nMotion Risk\nAny\nSevere\nMean\n95%\nMean\n95%\nArtPlanner (Competition)\n4.50\n0.05\n0.22\n1.00\n0.03\n0.17\nExploration Path\n10.93\n2.44\n0.77\n4.34\n0.12\n0.74\nArtPlanner (Playback)\n6.26\n0.34\n0.21\n0.96\n0.03\n0.16\nNo Motion Cost\n5.24\n0.16\n0.48\n2.82\n0.08\n0.49\nMotion Cost Planner\n25.44\n0.86\n0.41\n1.92\n0.06\n0.32\nExploration Path w/ Cost Optimizer\n37.17\n1.70\n0.34\n1.44\n0.05\n0.24\nTable 1: The collision rate, motion cost and motion risk for all compared methods over all robots. For the path\ncollision rate, the left column indicates the percentage of paths where any pose was in collision. The right column\nshows the percentage of paths where over 1\n3 of all poses were in collision. This serves as an indicator of how many\npaths would produce severe collisions which could become dangerous for the robot. For motion cost and motion risk,\nthe columns indicate their mean and 95th-percentile.\none closest to the current robot pose until the current goal pose of the ArtPlanner (see Section 1.5). This\nprevents excessive collision rates caused by height map artifacts, which do not appear in the volumetric map\nused by GBPlanner2. Note that all these concessions disadvantage ArtPlanner, and the comparison would\nhave been even more in our favor without them.\nWhen computing cost and collisions, we used the height map at the time when the paths are published, not\nthe maps which they are planned on. This is a more realistic evaluation which takes into account eﬀects\ncaused by diﬀerent planning speeds of the methods. We use the same risk cutoﬀof 0.5 for all methods and\nuse the same cost function mentioned in Section 2.2.\nAll playback experiments were run on a Desktop computer with an Intel i7-8700K CPU, a Nvidia RTX 2080\nGPU and 32 GB of RAM.\n3.1.3\nMotion Cost\nTo analyze the planner performance with respect to motion cost, we aggregate the cost of all individual path\nsegments spatially. We visualize this as a heat map, overlaid over a top-down view of the Finals course map,\nshown in Figure 7. We also show motion cost and motion risk statistics in Table 1.\nArtPlanner consistently output paths with low motion costs in all regions of the map, as shown in Figure 7']","Height map collisions were considered in the evaluation of the planners by checking collisions only for the torso of the robot and reducing the size of the collision boxes. The results in terms of motion cost and collision rate are shown in Table 1, where ArtPlanner consistently had low motion costs and collision rates compared to other methods.",multi_context,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}
 {'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
What are the impacts and contributions of LLMs in robotics?,"['Large Language Models for Robotics: A Survey\nFanlong Zenga, Wensheng Gana,∗, Yongheng Wanga, Ning Liua and Philip S. Yub\naSchool of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China\nbDepartment of Computer Science, University of Illinois Chicago, Chicago, USA\nA R T I C L E I N F O\nKeywords:\nlarge language models\nrobotics\ncontrol and interaction\ndecision-making\nembodied intelligence\nA B S T R A C T\nThe human ability to learn, generalize, and control complex manipulation tasks through multi-\nmodality feedback suggests a unique capability, which we refer to as dexterity intelligence. Under-\nstanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field of robotics have\ngarnered increasing attention. LLMs possess the ability to process and generate natural language,\nfacilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of\nrobotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot\ninteraction, and autonomy. Therefore, this comprehensive review aims to summarize the applications\nof LLMs in robotics, delving into their impact and contributions to key areas such as robot control,\nperception, decision-making, and path planning. We first provide an overview of the background and\ndevelopment of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and\nrecent advancements in robotics models based on LLMs. We then delve into the various techniques\nused in the model, including those employed in perception, decision-making, control, and interaction.\nFinally, we explore the applications of LLMs in robotics and some potential challenges they may face\nin the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics\nis one of the promising but challenging paths to achieve this.\n1. Introduction\nHumans possess exceptional proficiency in executing\nintricate and dexterous manipulation skills by integrating\ntactile, visual, and other sensory inputs. Research in the\nfield of robotics aspires to imbue robots with comparable\nmanipulation intelligence. Although recent advancements\nin robotics and machine learning have yielded promising\nresults in visual mitigation and exploration learning for robot\nmanipulation, there remains much to be accomplished in this\narea. Large language models (LLMs), such as BERT [31],\nRoberta [79], GPT-3 [27], GPT-4 [110], have emerged as\nsignificant research achievements in the field of artificial\nintelligence (AI) in recent years. Through deep learning\ntechniques [76], LLMs can be trained on massive text cor-\npora, enabling them to generate high-quality natural lan-\nguage text. This development has sparked new thinking in\nnatural language processing and dialogue systems. At the\nsame time, the rapid advancement of robotics technology']","This comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning.",multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What does CLIP do in LM-Nav and VLN?,"['emerging. Contrastive Language-Image Pre-training (CLIP)\n[99] is a neural network that has been trained on diverse pairs\nof images and text. It has the capability to understand natural\nlanguage instructions and predict the most pertinent text\nexcerpts associated with a given image, all without directly\noptimizing for this specific task. CLIP is similar to the zero-\nshot function of GPT-2 and 3. CLIP is also used in LM-\nNav [117] as a VLM to predict the text based on natural\nlanguage. The landmarks are extracted and built into the\ntopological map. VLM has the versatility to be employed\nin various downstream tasks including visual question an-\nswering (VQA) [151, 155], optical character recognition\n(OCR) [78], and image captioning [53]. Such as PaLM-E\n[34] treats text and images as latent vectors of multi-modal\ninput. Frozen [129] is also processed similarly to PaLM-E.\n3.1.3. Vision-and-language navigation model\nOne of the primary objectives of AI research is to de-\nvelop an embodied intelligence that can effectively commu-\nnicate with humans and interact with the environment. This\nembodied intelligence is capable of understanding human\nlanguage and navigating its surroundings with ease, which\nhas the potential to greatly benefit human society. However,\nachieving this goal is not without its challenges, including\ninsufficient dataset, navigation processing strategies, pro-\ncessing of multi-modal inputs, and model migration from\nfamiliar environments to unfamiliar environments. Despite\nthese obstacles, the development of embodied intelligence\nremains a crucial area of research in the field of AI [47].\nVisual-and-language navigation (VLN) is a model that lever-\nages visual observations to directly learn navigation impli-\ncations and seamlessly links images and actions across time.\nAs an extension of visual navigation in both real environ-\nments [89] and simulated [157], VLN boasts the capability to\nnavigate complex 3D environments. There are many datasets\nin VLN that can be exploited.\n3.1.4. Vision-language-action model\nCan we pre-train a model that integrates multimodal\ninputs and low-level robot protocols to enhance the robot’s\ngeneralization and semantic reasoning abilities? DeepMind\naimed to develop a straightforward end-to-end model that\ncould seamlessly map the robot’s observations into action,\nthereby creating Vision-Language-Action Models (VLA)\n[9]. Prior approaches involved incorporating VLMs into\nrobot policies or designing novel robot visual-language-\naction architectures. VLA instantiated by fine-tuning is first\nintroduced and implemented in RT-2 [9], leveraging a large\nVLM. DeepMind fine-tunes the large-scale VLM [22, 34]\nand pre-trains it on a vast network-scale dataset, transform-\ning VLM into VLA. To unify robot actions and natural\nlanguage responses, DeepMind integrates actions as text\ntokens directly into the pre-trained dataset, forming multi-\nmodal sentences [34]. Multimodal statements can respond'
 '(VNM); a visual-language model (VLM); and a large-scale\nlanguage model (LLM). Notably, LM-Nav operates without\nthe requirement of labeled data or fine-tuning. By leveraging\nthe VLM and VNM, LM-Nav can extract landmark names\nfrom commands and navigate to specified locations. LM-\nNav leverages three pre-trained models to achieve successful\nnavigation in pre-explored environments. First, it employs\nViNG [114] as a VNM creates a topological map using\nobservations from a prior exploration of the environment.\nSubsequently, GPT-3 [27] serves as the LLM [14] pro-\ncesses free-form text instructions to determine the target\nlandmark. Finally, CLIP [99] serves as the VLM to locate\nthe corresponding position in the topology map based on\nthe identified landmark. By combining these models, LM-\nNav can effectively follow natural language instructions to\ncomplete navigation tasks.\n2.3.4. Expedition A1\nExpedition A12, developed by AGIBot, embodies the\ncompany’s commitment to seamlessly integrating advanced\nAI into robotics and fostering harmonious collaboration\nbetween humans and machines. Envisioning a future where\nrobots serve as indispensable assistants to humans, AGI-\nBot’s mission is to create intelligent and versatile robots\n2https://www.agibot.com\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 5 of 19\nLarge Language Models for Robotics: A Survey\nTable 3\nTransformer architecture in robotics\nYear\nTransformer architecture\nDescription\n2022\nControl Transformer [75]\nControl Transformer (CT) utilizes a sample-based probabilistic road map (PRM) planner to generate conditional\nsequences from low-level policy, enabling it to complete navigation tasks solely through local information.\n2022\nRobotics Transformer 1 [10]\nRT-1 is capable of encoding high-dimensional input and output data, including images and instructions, into\ncompact tokens that can be efficiently processed by Transformer. It exhibits real-time operation characteristics,\nmaking it suitable for applications that require rapid processing and response times.\n2023\nQ-Transformer [18]\nQ-Transfomer is proposed to combine the Transformer structure with offline reinforcement learning, enabling\nthe exploitation of Q-values for each dimension.\n2023\nRobotics Transformer 2 [9]\nRobot Transformer 2 (RT-2) is a model that leverages fine-tuning of a VLM. RT-2 training on a web-scale\ndataset to achieve direct possession of generalization ability and semantic awareness for new tasks.\n2023\nRobotics Transformer X [29]\nRobotics Transformer X (RT-X) is categorized into two branches: RT-1-X and RT-2-X. RT-1-X employs the\nRT-1 architecture and utilizes the X-embodiment repository for training, while RT-2-X leverages the strategy\narchitecture of RT-2 and is trained on the same dataset. Experiments demonstrate that both RT-1-X and\nRT-2-X have exhibited enhanced capabilities.\ncapable of unlocking limitless productivity. The company’s\nfounding ethos is centered around the belief that ""intelligent']",CLIP serves as a VLM in LM-Nav to predict text based on natural language and as a VLM in VLN to locate the corresponding position in the topology map based on the identified landmark.,multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}
 {'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are some potential research directions for investigating emergent goal-like behavior in large language models in the context of experimental economics?,"['the emergent behavior of LLM-generated agents in social dilemmas, both within and beyond the Prisoner’s Dilemma.\n9\nInvestigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics\nThis broader research program could involve exploring the potential for more reﬁned or complex prompts to elicit\na wider range of cooperative behaviors in various experimental economics scenarios, such as the ultimatum game,\nthe dictator game, and the public goods game, among others. Examining the role of model architecture and training\nparameters in shaping agent behaviors, as well as analyzing the impact of various partner strategies on agent behavior\nin these different contexts, could shed light on the model’s adaptability and alignment with human values.\nIn future studies, it would be valuable to examine other parameter settings, such as temperature, to explore their\neffects on the emergent behavior of LLM-generated agents. Additionally, as more advanced LLMs like GPT-4 become\navailable, it would be interesting to investigate whether they exhibit similar limitations or are capable of more nuanced\ncooperative behaviors in a wider array of social dilemmas. Another potential limitation of the current study is that the\nLLM has been exposed to a vast literature on the iterated Prisoner’s Dilemma in its training data, and it is unclear how\nwould it perform in more ecologically valid task environments that it has no prior exposure to. This limitation could\nbe addressed by inventing new social dilemma games with corresponding task descriptions which are not vignettes\nfrom the existing literature.\nBy addressing these questions, we hope to collectively build a deeper understanding of AI alignment in the context of\ncomplex, non-zero-sum interactions across various experimental economics settings, ultimately fostering the develop-\nment of AI systems that better adhere to human values and social norms.\nReferences\n[0xk23]\n0XK1G0: ChatGPT DAN. https://github.com/0xk1h0/ChatGPT_DAN. Version: 2023\n[Axe97]\nAXELROD, R.: The Complexity of Cooperation: Agent-based Models of Competition and Collaboration.\nPrinceton University Press, 1997\n[Ber23]\nBEREN:\nScaffolded LLMs as natural language computers. https://www.lesswrong.com/posts/\n43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers. Version: 2023\n[BS23]\nBINZ, Marcel ; SCHULZ, Eric: Using cognitive psychology to understand GPT-3. In: Proceedings of the\nNational Academy of Sciences 120 (2023), Nr. 6, S. e2218523120\n[DMS19]\nDIXIT, Avinash ; MCADAMS, David ; SKEATH, Susan: ’We Haven’t Got But One More Day’: The Cuban\nMissile Crisis as a Dynamic Chicken Game. In: Available at SSRN 3406265 (2019)\n[FF04]\nFEHR, Ernst ; FISCHBACHER, Urs: Third-party punishment and social norms. In: Evolution and human\nbehavior 25 (2004), Nr. 2, S. 63–87\n[Goo23]\nGOOGLE: BIG-bench. https://github.com/google/BIG-bench. Version: 2023\n[Jan23]\nJANUS:\nSimulators.\nhttps://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators.'
 'INVESTIGATING EMERGENT GOAL-LIKE BEHAVIOUR IN LARGE\nLANGUAGE MODELS USING EXPERIMENTAL ECONOMICS\nSteve Phelps1 and Yvan I. Russell2\n1University College London, Computer Science, steve.phelps@ucl.ac.uk\n2Middlesex University, Psychology, Y.Russell@mdx.ac.uk\nMay 16, 2023\nABSTRACT\nIn this study, we investigate the capacity of large language models (LLMs), speciﬁcally GPT-3.5,\nto operationalise natural language descriptions of cooperative, competitive, altruistic, and self-\ninterested behavior in social dilemmas. Our focus is on the iterated Prisoner’s Dilemma, a classic\nexample of a non-zero-sum interaction, but our broader research program encompasses a range of\nexperimental economics scenarios, including the ultimatum game, dictator game, and public goods\ngame. Using a within-subject experimental design, we instantiated LLM-generated agents with var-\nious prompts that conveyed different cooperative and competitive stances. We then assessed the\nagents’ level of cooperation in the iterated Prisoner’s Dilemma, taking into account their responsive-\nness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs\ncan translate natural language descriptions of altruism and selﬁshness into appropriate behaviour to\nsome extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. The\nobserved pattern of increased cooperation with defectors and decreased cooperation with coopera-\ntors highlights potential constraints in the LLM’s ability to generalize its knowledge about human\nbehavior in social dilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array of social dilemmas,\nexamining the impact of model architecture, training parameters, and various partner strategies on\nagent behavior. As more advanced LLMs like GPT-4 become available, it is crucial to investigate\nwhether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ul-\ntimately fostering the development of AI systems that better align with human values and social\nnorms.\n1\nMotivation and background\nThe concept of agency and goal-directed behavior in large language models (LLMs) has been a topic of ongoing\ndebate and investigation within the AI alignment community. While there are a diverse set of opinions on the subject,\na challenge for researchers is that the internal processing of large language models is largely opaque, and in the case\nof recent models such as GPT-4 the training procedures themselves are also subject to a degree of secrecy. Therefore,\nobjective assessment of the capabilities of large-language models cannot be conducted through inductive reasoning\nstarting from ﬁrst principles, but instead is a matter of empirical investigation, with experiments being the ultimate\narbiter of what they can or can’t do, e.g. [Goo23].']","Some potential research directions for investigating emergent goal-like behavior in large language models in the context of experimental economics include exploring the potential for more refined or complex prompts to elicit a wider range of cooperative behaviors, examining the role of model architecture and training parameters in shaping agent behaviors, analyzing the impact of various partner strategies on agent behavior, examining the effects of different parameter settings such as temperature on the emergent behavior of LLM-generated agents, investigating the capabilities of more advanced LLMs like GPT-4, and inventing new social dilemma games with corresponding task descriptions.",simple,"[{'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}
 {'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}]",TRUE
"What is the topic of the paper ""Learning predictive models from observation and interaction""?","['arXiv:1612.06699, 2016.\n[45] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and\nJ. Bohg, “Concept2Robot: Learning manipulation con-\ncepts from instructions and human demonstrations,” in\nProceedings of Robotics: Science and Systems (RSS),\n2020.\n[46] A. S. Chen, S. Nair, and C. Finn, “Learning generaliz-\nable robotic reward functions from “in-the-wild” hu-\nman videos,” arXiv preprint arXiv:2103.16817, 2021.\n[47] S. Kumar, J. Zamora, N. Hansen, R. Jangir, and\nX. Wang, “Graph inverse reinforcement learning from\ndiverse videos,” in Conference on Robot Learning.\nPMLR, 2023, pp. 55–66.\n[48] M. Alakuijala, G. Dulac-Arnold, J. Mairal, J. Ponce,\nand C. Schmid, “Learning reward functions for robotic\nmanipulation by observing humans,” in 2023 IEEE\nInternational Conference on Robotics and Automation\n(ICRA).\nIEEE, 2023, pp. 5006–5012.\n[49] Y. Zhou, Y. Aytar, and K. Bousmalis, “Manipulator-\nindependent representations for visual imitation,”\n2021.\n[50] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu,\nY. Zhu, and A. Anandkumar, “Mimicplay: Long-\nhorizon imitation learning by watching human play,”\nin Conference on Robot Learning, 2023.\n[51] K. Schmeckpeper, A. Xie, O. Rybkin, S. Tian,\nK. Daniilidis, S. Levine, and C. Finn, “Learning pre-\ndictive models from observation and interaction,” in\nEuropean Conference on Computer Vision.\nSpringer,\n2020, pp. 708–725.\n[52] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and\nA. Gupta, “R3m: A universal visual representation for\nrobot manipulation,” in CoRL, 2022.\n[53] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik,\n“Masked visual pre-training for motor control,” arXiv\npreprint arXiv:2203.06173, 2022.\n[54] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Ma-\nlik, and T. Darrell, “Real-world robot learning with\nmasked visual pre-training,” in Conference on Robot\nLearning, 2022.\n[55] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani,\nV. Kumar, and A. Zhang, “Vip: Towards universal\nvisual reward and representation via value-implicit\npre-training,” arXiv preprint arXiv:2210.00030, 2022.\n[56] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen,\nS. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik\net al., “Where are we in the search for an artificial vi-\nsual cortex for embodied intelligence?” arXiv preprint\narXiv:2303.18240, 2023.\n[57] S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn,\nD. Sadigh, and P. Liang, “Language-driven represen-\ntation learning for robotics,” Robotics: Science and\nSystems (RSS), 2023.\n[58] Y. Mu, S. Yao, M. Ding, P. Luo, and C. Gan, “EC2:\nEmergent communication for embodied control,” in\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023, pp. 6704–\n6714.\n[59] S. Bahl, R. Mendonca, L. Chen, U. Jain']",The topic of the paper 'Learning predictive models from observation and interaction' is learning predictive models from observation and interaction.,simple,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
What is the significance of data flow analysis in programming and compiler optimizations?,"['ciech Zaremba. Evaluating Large Language Models Trained on Code. ArXiv preprint, abs/2107.03374,\n2021b. URL https://arxiv.org/abs/2107.03374.\nChris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O’Boyle, and Hugh\nLeather. ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Op-\ntimizations. In ICLR, 2021.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau\nYih, Luke Zettlemoyer, and Mike Lewis. Incoder: A Generative Model for Code Infilling and Synthesis.\nArXiv preprint, abs/2204.05999, 2022. URL https://arxiv.org/abs/2204.05999.\nSpandan Garg, Roshanak Zilouchian Moghaddam, Colin B. Clement, Neel Sundaresan, and Chen Wu.\nDeepPERF: A Deep Learning-Based Approach For Improving Software Performance, 2022.\nURL\nhttps://arxiv.org/abs/2206.13619.\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P Bigham. Instructdial:\nimproving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pp. 505–525, 2022.\nPatrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to\nprogram better. arXiv preprint arXiv:2207.14502, 2022.\nYoussef Hamadi and Youssef Hamadi. Autonomous Search. Combinatorial Search: From Algorithms to\nSystems, 2013.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nChangwu Huang, Yuanxiang Li, and Xin Yao.\nA Survey of Automatic Parameter Tuning Methods for\nMetaheuristics. IEEE transactions on evolutionary computation, 2019.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data, 7(3):535–547, 2019.\nSam Kaufman, Phitchaya Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Roy, Amit Sabne, and Mike\nBurrows. A Learned Performance Model for Tensor Processing Units. Proceedings of Machine Learning\nand Systems, 2021.\nPascal Kerschke, Holger H Hoos, Frank Neumann, and Heike Trautmann. Automated Algorithm Selection:\nSurvey and Perspectives. Evolutionary computation, 2019.\n11\nPreprint. Under review.\nLars Kotthoff. Algorithm Selection for Combinatorial Search Problems: A Survey. Data mining and con-\nstraint programming: Foundations of a cross-disciplinary approach, 2016.\nCharles E Leiserson, Neil C Thompson, Joel S Emer, Bradley C Kuszmaul, Butler W Lampson, Daniel\nSanchez, and Tao B Schardl. There’s plenty of room at the top: What will drive computer performance\nafter moore’s law? Science, 368(6495):eaam9744, 2020.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-']",ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations,simple,"[{'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}]",TRUE
How do LLMs in-context learn differently to enable a robot to walk?,"['hypothesize that a description prompt provides a context\nfor LLMs to interpret the observations and actions properly.\nWhile we provide a prompt example for robot walking, the\nprompt design for robot motions is still under-explored.\nB. LLMs In-Context Learn Differently\nOur experiments demonstrate that LLMs in-context learn\nto prompt a robot to walk. Initially, we hypothesized that\nLLMs might learn a robot walking behavior in a manner akin\nto behavior cloning [48]. However, as shown in Fig. 4, the\njoint trajectories generated by the LLM policy are sufficiently\ndifferent from those generated by an RL policy. Moreover,\nthe LLM policy shows a more regular pattern which is not\npresent in the RL policy. If we pay attention to the left calf\nTime\nFig. 8: Robot Walking Visualization. Top: A1 robot is\nprompted to walk on uneven terrain in MuJoCo, where the\nLLM policy can make it recover from terrain disturbance.\nBottom: ANYmal robot is prompted to walk on flat ground\nin Isaac Gym using the same approach.\njoint trajectory, the pattern coincides with the biomechanics\nstudy of animal walking [13]. Thus, we believe that LLMs\nin-context learn differently to enable a robot to walk.\nC. Limitations\nWhile this work takes us closer towards utilizing LLMs\nfor robot walking control, there are some limitations in the\ncurrent framework. First, the current prompt design is fragile.\nMinor alterations in the prompt can dramatically affect the\nwalking performance, as described in our experiments. In\ngeneral, we still lack a good understanding of how to design\na reliable prompt for robot walking. Secondly, as we design\nand test the prompt based on a specific initialization policy,\nour prompt design inevitably becomes biased towards this\npolicy. Although we have tested our framework with several\ndifferent RL initialization policies, it is possible that some\ninitialization policies do not work with our prompt.\nAnother major limitation is that we are only able to carry\nout simulation experiments instead of hardware experiments.\nOne reason is the low inference speed of GPT-4. Our pipeline\nrequires LLMs to be queried at 10 Hz, which is much faster\nthan the actual inference speed through OpenAI API. Thus,\nwe have to pause the simulation to wait for the output of\nGPT-4. Furthermore, due to the limited token size, we have\nto choose a low-frequency policy, i.e., 10 Hz, to maximize\nthe time horizon of the context. As a side note for future\nresearch, this work is expensive and roughly costed $2, 000\nUS dollars for querying OpenAI API to test the prompt.\nV. CONCLUSIONS\nIn this paper, we presented an approach for prompting a\nrobot to walk. We use LLMs with text prompts, consisting of\na description prompt and an observation and action prompt\ncollected from the physical environment, without any task-\nspecific fine-tuning. Our experiments demonstrate that LLMs\ncan serve as low-level feedback controllers for dynamic\nmotion control even in high-dimensional robotic systems.']",LLMs in-context learn differently to enable a robot to walk by generating joint trajectories that are sufficiently different from those generated by an RL policy. The LLM policy shows a more regular pattern that coincides with the biomechanics study of animal walking.,simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What is the significance of RL-based robot walking controllers in the context of grounding LLMs in a physical environment?,"['LLM Prompt:\nDescription Prompt\nObservation and \nAction Prompt\nFig. 3: Text Prompt. We design a text prompt that includes two parts: a description prompt and an observation and action\nprompt. In the description prompt, we have the following subparts: PT D: task description, PIO: meaning of input and output\nspace, PJO: joint order, PCP : full control pipeline, and PAI: additional illustration. In the observation and action prompt,\nwe have PHist: historical observations and actions. The LLM outputs normalized target joint positions.\nto understand the context of the inputs and actions. Then,\nan explicit enumeration of the joint order of our robot is\nprovided in PJO to guide the LLM to comprehend the robot\nconfiguration. Additionally, we specify in the prompt PAI\nthat all the values LLMs encounter are not raw data. Instead,\nthese numerical values have been normalized. Lastly, the\nprompt offers an overview of the entire control pipeline\nin PCP , granting the LLM a macro perspective on how\nindividual components enabling it to process and interlink. It\nis crucial to highlight that, unlike classic learning-based and\nmodel-based walking controllers, text serves an important\nrole in the LLM policy.\nObservation and Action Prompt. A sequence of observa-\ntion and action pairs PHist are used as prompts. These pairs\nare generated from the recent history of the robot walking\ntrajectory. This procedure is widely used in RL-based robot\nwalking controllers, where it allows the neural network to\ninfer the dynamics as well as the privileged environment\ninformation. With a sequence of observation and action\nprompts, LLMs can in-context learn the dynamics and infer a\nreactive control action, where the observation prompt serves\nas the feedback signal. Note that both observation and action\nare converted to text format to interface with LLMs.\nLLMs often struggle to comprehend the significance of\nnumeric values, particularly floating point and negative num-\nbers. Inspired by the prompt design in [26], we adopt a\nnormalization approach for numerical values. Specifically,\nwe use a linear transformation to map all the potential\nnumeric values into non-negative integers, ranging from 0\nto 200. We hypothesize that LLMs are mostly trained with\ntext tokens, thus they are not sensitive enough to numerical\nvalues for robot control.\nC. Grounding LLMs\nIn order to make LLMs useful for robot walking control,\nwe need to ground them in a physical environment. We now\nintroduce the pipeline to allow LLMs to interact with a robot\nand an environment. We use a physics-based simulator where\nLLMs can get observations and send actions. The observa-\ntions are from the physics-based simulation. The output of\nthe LLM is the target joint positions, which are tracked by a\nset of joint Proportional-Derivative (PD) controllers running\nat a higher frequency. This joint-level PD control design is\nstandard for learning-based robot walking control. While this']","RL-based robot walking controllers are significant in grounding LLMs in a physical environment because they allow LLMs to interact with a robot and an environment. LLMs can receive observations from the physics-based simulation and send actions to control the robot. The output of the LLM is the target joint positions, which are tracked by joint PD controllers. This allows LLMs to learn the dynamics and infer reactive control actions in a physical environment.",simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
"What factors contribute to LLM-generated agents' behavior in social dilemmas, and how do architecture, training, and partner strategies affect it?","['the emergent behavior of LLM-generated agents in social dilemmas, both within and beyond the Prisoner’s Dilemma.\n9\nInvestigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics\nThis broader research program could involve exploring the potential for more reﬁned or complex prompts to elicit\na wider range of cooperative behaviors in various experimental economics scenarios, such as the ultimatum game,\nthe dictator game, and the public goods game, among others. Examining the role of model architecture and training\nparameters in shaping agent behaviors, as well as analyzing the impact of various partner strategies on agent behavior\nin these different contexts, could shed light on the model’s adaptability and alignment with human values.\nIn future studies, it would be valuable to examine other parameter settings, such as temperature, to explore their\neffects on the emergent behavior of LLM-generated agents. Additionally, as more advanced LLMs like GPT-4 become\navailable, it would be interesting to investigate whether they exhibit similar limitations or are capable of more nuanced\ncooperative behaviors in a wider array of social dilemmas. Another potential limitation of the current study is that the\nLLM has been exposed to a vast literature on the iterated Prisoner’s Dilemma in its training data, and it is unclear how\nwould it perform in more ecologically valid task environments that it has no prior exposure to. This limitation could\nbe addressed by inventing new social dilemma games with corresponding task descriptions which are not vignettes\nfrom the existing literature.\nBy addressing these questions, we hope to collectively build a deeper understanding of AI alignment in the context of\ncomplex, non-zero-sum interactions across various experimental economics settings, ultimately fostering the develop-\nment of AI systems that better adhere to human values and social norms.\nReferences\n[0xk23]\n0XK1G0: ChatGPT DAN. https://github.com/0xk1h0/ChatGPT_DAN. Version: 2023\n[Axe97]\nAXELROD, R.: The Complexity of Cooperation: Agent-based Models of Competition and Collaboration.\nPrinceton University Press, 1997\n[Ber23]\nBEREN:\nScaffolded LLMs as natural language computers. https://www.lesswrong.com/posts/\n43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers. Version: 2023\n[BS23]\nBINZ, Marcel ; SCHULZ, Eric: Using cognitive psychology to understand GPT-3. In: Proceedings of the\nNational Academy of Sciences 120 (2023), Nr. 6, S. e2218523120\n[DMS19]\nDIXIT, Avinash ; MCADAMS, David ; SKEATH, Susan: ’We Haven’t Got But One More Day’: The Cuban\nMissile Crisis as a Dynamic Chicken Game. In: Available at SSRN 3406265 (2019)\n[FF04]\nFEHR, Ernst ; FISCHBACHER, Urs: Third-party punishment and social norms. In: Evolution and human\nbehavior 25 (2004), Nr. 2, S. 63–87\n[Goo23]\nGOOGLE: BIG-bench. https://github.com/google/BIG-bench. Version: 2023\n[Jan23]\nJANUS:\nSimulators.\nhttps://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators.'
 'INVESTIGATING EMERGENT GOAL-LIKE BEHAVIOUR IN LARGE\nLANGUAGE MODELS USING EXPERIMENTAL ECONOMICS\nSteve Phelps1 and Yvan I. Russell2\n1University College London, Computer Science, steve.phelps@ucl.ac.uk\n2Middlesex University, Psychology, Y.Russell@mdx.ac.uk\nMay 16, 2023\nABSTRACT\nIn this study, we investigate the capacity of large language models (LLMs), speciﬁcally GPT-3.5,\nto operationalise natural language descriptions of cooperative, competitive, altruistic, and self-\ninterested behavior in social dilemmas. Our focus is on the iterated Prisoner’s Dilemma, a classic\nexample of a non-zero-sum interaction, but our broader research program encompasses a range of\nexperimental economics scenarios, including the ultimatum game, dictator game, and public goods\ngame. Using a within-subject experimental design, we instantiated LLM-generated agents with var-\nious prompts that conveyed different cooperative and competitive stances. We then assessed the\nagents’ level of cooperation in the iterated Prisoner’s Dilemma, taking into account their responsive-\nness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs\ncan translate natural language descriptions of altruism and selﬁshness into appropriate behaviour to\nsome extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. The\nobserved pattern of increased cooperation with defectors and decreased cooperation with coopera-\ntors highlights potential constraints in the LLM’s ability to generalize its knowledge about human\nbehavior in social dilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array of social dilemmas,\nexamining the impact of model architecture, training parameters, and various partner strategies on\nagent behavior. As more advanced LLMs like GPT-4 become available, it is crucial to investigate\nwhether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ul-\ntimately fostering the development of AI systems that better align with human values and social\nnorms.\n1\nMotivation and background\nThe concept of agency and goal-directed behavior in large language models (LLMs) has been a topic of ongoing\ndebate and investigation within the AI alignment community. While there are a diverse set of opinions on the subject,\na challenge for researchers is that the internal processing of large language models is largely opaque, and in the case\nof recent models such as GPT-4 the training procedures themselves are also subject to a degree of secrecy. Therefore,\nobjective assessment of the capabilities of large-language models cannot be conducted through inductive reasoning\nstarting from ﬁrst principles, but instead is a matter of empirical investigation, with experiments being the ultimate\narbiter of what they can or can’t do, e.g. [Goo23].']","The factors that contribute to LLM-generated agents' behavior in social dilemmas include model architecture, training parameters, and various partner strategies. It is important to examine how these factors impact the behavior of the agents and their adaptability to different contexts. As more advanced LLMs like GPT-4 become available, it is crucial to investigate whether they exhibit similar limitations or are capable of more nuanced cooperative behaviors. This research aims to foster the development of AI systems that better align with human values and social norms.",reasoning,"[{'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}
 {'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}]",TRUE
How does prompt content affect cooperation in LLM-generated agents when paired with different partners in the Prisoner's Dilemma?,"['2.7.5\nHypothesis 5 (H5)\nSimulacra instantiated with altruistic prompts will exhibit a higher degree of cooperation when paired with an un-\nconditionally cooperating partner, compared to when they are paired with an unconditionally defecting partner or a\ntit-for-tat partner.\n2.7.6\nHypothesis 6 (H6)\nSimulacra instantiated with self-interested prompts will exhibit a lower degree of cooperation when paired with an\nunconditionally cooperating partner, compared to when they are paired with an unconditionally defecting partner or a\ntit-for-tat partner.\n2.7.7\nHypothesis 7 (H7)\nSimulacra instantiated with cooperative or altruistic prompts will exhibit higher cooperation rates when paired with a\ntit-for-tat partner initiating with cooperation compared to when they are paired with a tit-for-tat partner initiating with\ndefection.\n2.7.8\nHypothesis 8 (H8)\nSimulacra instantiated with competitive or self-interested prompts will exhibit lower cooperation rates when paired\nwith a tit-for-tat partner initiating with cooperation compared to when they are paired with a tit-for-tat partner initiating\nwith defection.\n3\nResults\nThe data consists of a total of N = 1800 cases. Each case corresponds to a single play of the iterated PD over six\nrounds. For each case, we record the following ﬁelds:\n• Group\n• Participant\n• Condition\n• Score\n• Cooperation frequency\n• Choices\n• Transcript\nTables 1 to 5 give descriptive statistics showing the relationship between the prompt content and emergent behavior in\nLLM-generated agents, while Figures 1 and 2 show corresponding box-plots.\nAs can be seen from Figure 1a, without having to resort to statistical tests, our results provide clear support for\nhypothesis 1 through 3 (see section 2.7), demonstrating that simulacra instantiated with cooperative, competitive,\naltruistic, and self-interested prompts exhibit distinct levels of cooperation in the iterated Prisoner’s Dilemma. This\nindicates that LLMs can operationalise natural language descriptions of cooperative and competitive behavior to some\n5\nInvestigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics\nGroup.Competitive\nGroup.Altruistic\nGroup.Selfish\nGroup.Mixed\nGroup.Control\n0\n0.2\n0.4\n0.6\n0.8\n1\nGroup\nCooperation frequency\n(a) Cooperation frequency by group\nunconditional cooperate unconditional defect\ntit for tat C\ntit for tat D\n0\n0.2\n0.4\n0.6\n0.8\n1\nCondition\nCooperation frequency\n(b) Cooperation frequency by condition\nFigure 1: Cooperation frequnecy by group (a), and condition (b)\n6\nInvestigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\nGroup\nGroup.Altruistic\n360.00\n0.90\n0.17\n0.33\n0.83\n1.00\n1.00\n1.00\nGroup.Competitive\n360.00\n0.14\n0.16\n0.00\n0.00\n0.17\n0.33\n0.50\nGroup.Control\n345.00\n0.53\n0.24\n0.00\n0.50\n0.50\n0.67\n1.00\nGroup.Mixed\n360.00\n0.52\n0.25\n0.00\n0.50\n0.50\n0.50\n1.00\nGroup.Selﬁsh\n360.00\n0.15\n0.17\n0.00\n0.00\n0.00\n0.33\n0.50\nTable 1: Cooperation frequency by group\ncount'
 '1. Unconditional defect - the partner always chooses to defect.\n2. Unconditional cooperation - the partner always cooperates.\n3. Tit-for-tat (C) - the partner cooperates on the move, and thereafter the previous choice of the simulacrum.\n4. Tit-for-tat (D) - the partner defects on the move, and thereafter the previous choice of the simulacrum.\n2.5\nParameters and experimental protocol\nWe used the OpenAI chat completion API to interact with the model [Ope23a]. The language model’s temperature\nwas set to 0.2 and the maximum number of tokens per request-completion was set to 100. These parameters were\nconstant across samples and experimental conditions (future work will examine the sensitivity of our results to these\nparameters).\nEach simulacrum was instantiated using a message supplied in the user role at the beginning of the chat. The exper-\niment was then described to the simulacrum using a prompt in the user role, and thereafter the rounds of play were\nconducted by alternating messages supplied in the assistant and user roles for the choices made by the participant\nand their simulated partner respectively.\nThe full set of prompts and sample transcripts are given in the appendices (Sections 5.1 and 5.2), and the complete\nPython code used to conduct the experiment can be found in the code repository.\n2.6\nData Collection and Analysis\nWe collected and recorded data on the communication between the LLM-generated simulacra and their simulated\npartner during each round of the game. Each chat transcript was analysed using a simple regular expression to extract\nthe choices made by each simulacrum and their partner in each round. The total score was tallied after all rounds had\nbeen played. We recorded the mean and standard deviation of the ﬁnal score across all N chat samples.\n2.7\nHypotheses\nPrior to analysing the experimental results we formulated the following testable hypotheses in order to ascertain the\ncapabilities of large-language models are able to operationalise natural language descriptions of selﬁsh versus altruistic\nbehaviour.\n2.7.1\nHypothesis 1 (H1)\nSimulacra instantiated with altruistic prompts will exhibit higher cooperation rates compared to those instantiated with\nself-interested prompts.\n4\nInvestigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics\n2.7.2\nHypothesis 2 (H2)\nSimulacra in the mixed-motivation group will exhibit cooperation rates that fall between those of the cooperative and\ncompetitive groups.\n2.7.3\nHypothesis 3 (H3)\nSimulacra in all groups will exhibit cooperation rates that are different from the control group.\n2.7.4\nHypothesis 4 (H4)\nHypothesis 5 (H5) Simulacra instantiated with competitive prompts will demonstrate a greater tendency to defect,\nregardless of their partner’s behavior, compared to other groups.\n2.7.5\nHypothesis 5 (H5)\nSimulacra instantiated with altruistic prompts will exhibit a higher degree of cooperation when paired with an un-']",The prompt content affects cooperation in LLM-generated agents when paired with different partners in the Prisoner's Dilemma. Simulacra instantiated with altruistic prompts exhibit a higher degree of cooperation when paired with an unconditionally cooperating partner compared to when they are paired with an unconditionally defecting partner or a tit-for-tat partner. Simulacra instantiated with self-interested prompts exhibit a lower degree of cooperation when paired with an unconditionally cooperating partner compared to when they are paired with an unconditionally defecting partner or a tit-for-tat partner. Simulacra instantiated with cooperative or altruistic prompts exhibit higher cooperation rates when paired with a tit-for-tat partner initiating with cooperation compared to when they are paired with a tit-for-tat partner initiating with defection. Simulacra instantiated with competitive or self-interested prompts exhibit lower cooperation rates when paired with a tit-for-tat partner initiating with cooperation compared to when they are paired with a tit-for-tat partner initiating with defection.,reasoning,"[{'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}
 {'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}]",TRUE
What determines an object's transparency?,"['Mass\n94.2\n58.8\nFragility\n93.6\n53.1\nDeformability\n90.5\n48.1\nMaterial\n93.7\n59.4\nTransparency\n97.0\n72.5\nContents\n90.4\n49.8\nCan Contain Liquid\n99.3\n64.2\nIs Sealed\n98.2\n74.7\nDensity (held-out)\n93.3\n50.7\nLiquid Capacity (held-out)\n89.1\n46.0\nTABLE XI: Agreement among crowd-workers per concept\ncrowd-sourced data is also the bounding box image presented\nfor annotation. For crowd-sourced validation data, we filter\nour data to only include examples with at least 2/3 majority\nagreement among annotators, and only use the majority label.\nWe do not apply this filtering for training data. For preference\npair annotations, we remove data annotated with unclear.\nDataset Balancing. We construct sub-datasets for dataset\nbalancing purposes. For the categorical concepts except is\nsealed, we combine the crowd-sourced and automatically\nannotated data for each concept into one sub-dataset per\nconcept. For the other concepts, we keep separate sub-\ndatasets for crowd-sourced and automatically annotated data.\nWe keep separate sub-datasets for is sealed because for its\ncrowd-sourced data, we only train using the bounding box\nimage for the object that was presented for annotation, rather\nthan randomly sampling one of its bounding box images\n(as described in the below sub-section), as values for this\nconcept may change for the same object instance. We keep\nseparate datasets for the continuous concepts because there\nis a large imbalance between the number of crowd-sourced\nand automatically annotated examples for these concepts. To\nbalance these sub-datasets, we sample from each of them\nduring training at a rate proportional to the square root of\nthe number of annotations in the sub-dataset, as proposed in\nInstructBLIP for instruction tuning.\nAdditional Training Details. For most objects, each time\nwe sample one for training, we randomly sample one of its\nbounding box images as input to the model, as a form of\ndata augmentation. We do not do this with crowd-sourced\ndata for the contents and is sealed concepts, because labels\nfor these concepts may vary across different images of the\nsame object. Instead, we only use the bounding box image\nthat was presented for annotation.\nTo promote robustness to different queries to the VLM, we\ninclude object category labels in the question prompt for half\nof the training examples (e.g., asking “Is this bottle heavy?”),\nHyperparameter\nValue\nMax fine-tuning steps\n10000\nWarmup steps\n1000\nLearning rate\n1e-5\nBatch size\n128\nAdamW β\n(0.9, 0.999)\nWeight decay\n0.05\nImage resolution\n224\nPrompt template\nQuestion: {} Respond unknown if you are not sure. Short answer:\nTABLE XII: Hyperparameters for fine-tuning InstructBLIP\nConcept\nQuestion Prompt\nMass\nIs this object heavy?\nFragility\nIs this object fragile?\nDeformability\nIs this object deformable?\nMaterial\nWhat material is this object made of?\nTransparency\nIs this object transparent, translucent, or opaque?\nContents\nWhat is inside this container?\nCan Contain Liquid'
 'Mass\n94.2\n58.8\nFragility\n93.6\n53.1\nDeformability\n90.5\n48.1\nMaterial\n93.7\n59.4\nTransparency\n97.0\n72.5\nContents\n90.4\n49.8\nCan Contain Liquid\n99.3\n64.2\nIs Sealed\n98.2\n74.7\nDensity (held-out)\n93.3\n50.7\nLiquid Capacity (held-out)\n89.1\n46.0\nTABLE XI: Agreement among crowd-workers per concept\ncrowd-sourced data is also the bounding box image presented\nfor annotation. For crowd-sourced validation data, we filter\nour data to only include examples with at least 2/3 majority\nagreement among annotators, and only use the majority label.\nWe do not apply this filtering for training data. For preference\npair annotations, we remove data annotated with unclear.\nDataset Balancing. We construct sub-datasets for dataset\nbalancing purposes. For the categorical concepts except is\nsealed, we combine the crowd-sourced and automatically\nannotated data for each concept into one sub-dataset per\nconcept. For the other concepts, we keep separate sub-\ndatasets for crowd-sourced and automatically annotated data.\nWe keep separate sub-datasets for is sealed because for its\ncrowd-sourced data, we only train using the bounding box\nimage for the object that was presented for annotation, rather\nthan randomly sampling one of its bounding box images\n(as described in the below sub-section), as values for this\nconcept may change for the same object instance. We keep\nseparate datasets for the continuous concepts because there\nis a large imbalance between the number of crowd-sourced\nand automatically annotated examples for these concepts. To\nbalance these sub-datasets, we sample from each of them\nduring training at a rate proportional to the square root of\nthe number of annotations in the sub-dataset, as proposed in\nInstructBLIP for instruction tuning.\nAdditional Training Details. For most objects, each time\nwe sample one for training, we randomly sample one of its\nbounding box images as input to the model, as a form of\ndata augmentation. We do not do this with crowd-sourced\ndata for the contents and is sealed concepts, because labels\nfor these concepts may vary across different images of the\nsame object. Instead, we only use the bounding box image\nthat was presented for annotation.\nTo promote robustness to different queries to the VLM, we\ninclude object category labels in the question prompt for half\nof the training examples (e.g., asking “Is this bottle heavy?”),\nHyperparameter\nValue\nMax fine-tuning steps\n10000\nWarmup steps\n1000\nLearning rate\n1e-5\nBatch size\n128\nAdamW β\n(0.9, 0.999)\nWeight decay\n0.05\nImage resolution\n224\nPrompt template\nQuestion: {} Respond unknown if you are not sure. Short answer:\nTABLE XII: Hyperparameters for fine-tuning InstructBLIP\nConcept\nQuestion Prompt\nMass\nIs this object heavy?\nFragility\nIs this object fragile?\nDeformability\nIs this object deformable?\nMaterial\nWhat material is this object made of?\nTransparency\nIs this object transparent, translucent, or opaque?\nContents\nWhat is inside this container?\nCan Contain Liquid']","Transparency is determined by whether an object is transparent, translucent, or opaque.",multi_context,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
How do sophisticated prompts affect cooperative behaviors in experimental economics scenarios with LLM-generated agents in social dilemmas?,"['the emergent behavior of LLM-generated agents in social dilemmas, both within and beyond the Prisoner’s Dilemma.\n9\nInvestigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics\nThis broader research program could involve exploring the potential for more reﬁned or complex prompts to elicit\na wider range of cooperative behaviors in various experimental economics scenarios, such as the ultimatum game,\nthe dictator game, and the public goods game, among others. Examining the role of model architecture and training\nparameters in shaping agent behaviors, as well as analyzing the impact of various partner strategies on agent behavior\nin these different contexts, could shed light on the model’s adaptability and alignment with human values.\nIn future studies, it would be valuable to examine other parameter settings, such as temperature, to explore their\neffects on the emergent behavior of LLM-generated agents. Additionally, as more advanced LLMs like GPT-4 become\navailable, it would be interesting to investigate whether they exhibit similar limitations or are capable of more nuanced\ncooperative behaviors in a wider array of social dilemmas. Another potential limitation of the current study is that the\nLLM has been exposed to a vast literature on the iterated Prisoner’s Dilemma in its training data, and it is unclear how\nwould it perform in more ecologically valid task environments that it has no prior exposure to. This limitation could\nbe addressed by inventing new social dilemma games with corresponding task descriptions which are not vignettes\nfrom the existing literature.\nBy addressing these questions, we hope to collectively build a deeper understanding of AI alignment in the context of\ncomplex, non-zero-sum interactions across various experimental economics settings, ultimately fostering the develop-\nment of AI systems that better adhere to human values and social norms.\nReferences\n[0xk23]\n0XK1G0: ChatGPT DAN. https://github.com/0xk1h0/ChatGPT_DAN. Version: 2023\n[Axe97]\nAXELROD, R.: The Complexity of Cooperation: Agent-based Models of Competition and Collaboration.\nPrinceton University Press, 1997\n[Ber23]\nBEREN:\nScaffolded LLMs as natural language computers. https://www.lesswrong.com/posts/\n43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers. Version: 2023\n[BS23]\nBINZ, Marcel ; SCHULZ, Eric: Using cognitive psychology to understand GPT-3. In: Proceedings of the\nNational Academy of Sciences 120 (2023), Nr. 6, S. e2218523120\n[DMS19]\nDIXIT, Avinash ; MCADAMS, David ; SKEATH, Susan: ’We Haven’t Got But One More Day’: The Cuban\nMissile Crisis as a Dynamic Chicken Game. In: Available at SSRN 3406265 (2019)\n[FF04]\nFEHR, Ernst ; FISCHBACHER, Urs: Third-party punishment and social norms. In: Evolution and human\nbehavior 25 (2004), Nr. 2, S. 63–87\n[Goo23]\nGOOGLE: BIG-bench. https://github.com/google/BIG-bench. Version: 2023\n[Jan23]\nJANUS:\nSimulators.\nhttps://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators.'
 'INVESTIGATING EMERGENT GOAL-LIKE BEHAVIOUR IN LARGE\nLANGUAGE MODELS USING EXPERIMENTAL ECONOMICS\nSteve Phelps1 and Yvan I. Russell2\n1University College London, Computer Science, steve.phelps@ucl.ac.uk\n2Middlesex University, Psychology, Y.Russell@mdx.ac.uk\nMay 16, 2023\nABSTRACT\nIn this study, we investigate the capacity of large language models (LLMs), speciﬁcally GPT-3.5,\nto operationalise natural language descriptions of cooperative, competitive, altruistic, and self-\ninterested behavior in social dilemmas. Our focus is on the iterated Prisoner’s Dilemma, a classic\nexample of a non-zero-sum interaction, but our broader research program encompasses a range of\nexperimental economics scenarios, including the ultimatum game, dictator game, and public goods\ngame. Using a within-subject experimental design, we instantiated LLM-generated agents with var-\nious prompts that conveyed different cooperative and competitive stances. We then assessed the\nagents’ level of cooperation in the iterated Prisoner’s Dilemma, taking into account their responsive-\nness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs\ncan translate natural language descriptions of altruism and selﬁshness into appropriate behaviour to\nsome extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. The\nobserved pattern of increased cooperation with defectors and decreased cooperation with coopera-\ntors highlights potential constraints in the LLM’s ability to generalize its knowledge about human\nbehavior in social dilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array of social dilemmas,\nexamining the impact of model architecture, training parameters, and various partner strategies on\nagent behavior. As more advanced LLMs like GPT-4 become available, it is crucial to investigate\nwhether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ul-\ntimately fostering the development of AI systems that better align with human values and social\nnorms.\n1\nMotivation and background\nThe concept of agency and goal-directed behavior in large language models (LLMs) has been a topic of ongoing\ndebate and investigation within the AI alignment community. While there are a diverse set of opinions on the subject,\na challenge for researchers is that the internal processing of large language models is largely opaque, and in the case\nof recent models such as GPT-4 the training procedures themselves are also subject to a degree of secrecy. Therefore,\nobjective assessment of the capabilities of large-language models cannot be conducted through inductive reasoning\nstarting from ﬁrst principles, but instead is a matter of empirical investigation, with experiments being the ultimate\narbiter of what they can or can’t do, e.g. [Goo23].']","This broader research program could involve exploring the potential for more reﬁned or complex prompts to elicit a wider range of cooperative behaviors in various experimental economics scenarios, such as the ultimatum game, the dictator game, and the public goods game, among others.",multi_context,"[{'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}
 {'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}]",TRUE
"What is the reason for the 1:1 split in training the RT-2-X model, considering the capabilities and generalization properties of RT-1 and VLMs?","['natural language instruction is transformed into a USE [120]\nembedding. The visual and language representations are then\ninterwoven via FiLM [116] layers, producing 81 vision-\nlanguage tokens. These tokens are fed into a decoder-only\nTransformer, which outputs the tokenized actions.\nRT-2 [9] is a family of large vision-language-action\nmodels (VLAs) trained on Internet-scale vision and language\ndata along with robotic control data. RT-2 casts the tokenized\nactions to text tokens, e.g., a possible action may be “1 128\n91 241 5 101 127”. As such, any pretrained vision-language\nmodel (VLM [121–123]) can be finetuned for robotic control,\nthus leveraging the backbone of VLMs and transferring some\nof their generalization properties. In this work, we focus on\nthe RT-2-PaLI-X variant [121] built on a backbone of a visual\nmodel, ViT [124], and a language model, UL2 [125], and\npretrained primarily on the WebLI [121] dataset.\nC. Training and inference details\nBoth models use a standard categorical cross-entropy\nobjective over their output space (discrete buckets for RT-\n1 and all possible language tokens for RT-2).\nWe define the robotics data mixture used across all of\nthe experiments as the data from 9 manipulators, and taken\nfrom RT-1 [8], QT-Opt [66], Bridge [95], Task Agnostic\nRobot Play [126, 127], Jaco Play [128], Cable Routing [129],\nRoboTurk [86], NYU VINN [130], Austin VIOLA [131],\nBerkeley Autolab UR5 [132], TOTO [133] and Language\nTable [91] datasets. RT-1-X is trained on only robotics\nmixture data defined above, whereas RT-2-X is trained via\nco-fine-tuning (similarly to the original RT-2 [9]), with an\nFig. 4: RT-1-X mean success rate is 50% higher than that of either the Original Method or RT-1. RT-1 and RT-1-X have the same network\narchitecture. Therefore the performance increase can be attributed to co-training on the robotics data mixture. The lab logos indicate the\nphysical location of real robot evaluation, and the robot pictures indicate the embodiment used for the evaluation.\nEvaluation Setting\nBridge\nBridge\nRT-1 paper 6 skills\nEvaluation Location\nIRIS (Stanford)\nRAIL Lab (UCB)\nGoogle Robotic Lab\nRobot Embodiment\nWidowX\nWidowX\nGoogle Robot\nOriginal Method\nLCBC [95]\nLCBC [95]\n-\nOriginal Method\n13%\n13%\n-\nRT-1\n40%\n30%\n92%\nRT-1-X\n27%\n27%\n73%\nRT-2-X (55B)\n50%\n30%\n91%\nTABLE I: Parameter count scaling experiment to assess the impact\nof capacity on absorbing large-scale diverse embodiment data. For\nthese large-scale datasets (Bridge and RT-1 paper data), RT-1-X\nunderfits and performs worse than the Original Method and RT-1.\nRT-2-X model with significantly many more parameters can obtain\nstrong performance in these two evaluation scenarios.\napproximately one to one split of the original VLM data\nand the robotics data mixture. Note that the robotics data\nmixture used in our experiments includes 9 embodiments\nwhich is fewer than the entire Open X-Embodiment dataset\n(22) – the practical reason for this difference is that we have'
 'remains whether it is possible to train a model in the field of\nrobotics that can absorb knowledge from other fields. Could\nthe model demonstrate zero-shot generalization capabilities\nfor new tasks? Robotics Transformer 1 (RT-1) [10] was\nproposed to address the aforementioned question. RT-1 is\ncapable of encoding high-dimensional input and output data,\nincluding images and instructions, into compact tokens that\ncan be efficiently processed by Transformer [131]. It exhibits\nreal-time operation characteristics, making it suitable for ap-\nplications that require rapid processing and response times.\nIn experimental evaluations, RT-1 demonstrated strong gen-\neralization. The structure of RT-1 is composed of FiLM\n[96], conditioned EfficientNet [124], a TokenLearner [107],\nand Transformer [131]. However, RT-1 is not an end-to-end\nmodel.\nRobotics transformer 2. Can we pre-train a vision-\nlanguage model (VLM) [22, 34] that can be seamlessly inte-\ngrated into low-level robot control? Hereby enhancing VLM\ngeneralization capabilities? We can achieve this by training\nthe robot’s trajectory to be represented as a sequence of to-\nkens, effectively mapping natural language instructions into\na series of robot actions. To create an end-to-end model that\ncan directly map robot observations into actions, DeepMind\nemploys a collaborative fine-tuning approach. Combining\nstate-of-the-art VLMs with network-scale visual-language\ntasks on robot trajectory data, Robot Transformer 2 (RT-2)\n[9] is a model that leverages fine-tuning of a VLM. RT-2 is\ntrained on a web-scale dataset to achieve direct possession of\ngeneralization ability and semantic awareness for new tasks.\nThrough fine-tuning a VLM, it is adapted to generate actions\nbased on text encoding. Specifically, the model is trained\non a dataset that incorporates action-related text tokens.\nThis type of model can be called a visual-language-action\nmodel (VLA) [9]. RT-2 builds upon the policy trained by\nRobotic Transformer 1 (RT-1) [10], leveraging the same\ndataset and an expanded VLA to significantly enhance the\nmodel’s generalization capabilities for new tasks.\nRobotics transformer X. In robot learning, it is com-\nmon to train a separate large model for each application\nor environment. However, this approach can be limiting, as\nit may not allow for adaptability across different robots or\nenvironments. Can we develop a robot policy that is versatile\nand can be applied across various robots and environments?\nWith the advancements in large models, it is within the realm\nof possibility to train a versatile model that exhibits strong\ngeneralization capabilities for a specific task. Inspired by\nthese large models, X-embodiment training3 is proposed,\nwhich involves using robot data from diverse platforms for\ntraining. This approach enables the model to better adapt to\nchanges in both the robot and the environment, leading to\nimproved performance and versatility. Robotics Transformer']","The reason for the 1:1 split in training the RT-2-X model is to combine the original VLM data with the robotics data mixture, allowing the model to absorb knowledge from both fields and enhance its generalization capabilities for new tasks.",multi_context,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are the results of the retrieval-based few-shot prompting experiments for the GPT4 model?,"['For generation, we use a temperature of 1.0 and use top-p sampling with p = 0.9 For each prompt, we\ntry attempt to take n = 5 samples. We chose these samples after doing a sweep of 6 configurations of\n17\nPreprint. Under review.\nDataset\nPairs\nTrain\n77,967\nVal\n2,544\nTest\n982\nTable 5: Number of pairs.\nDataset\nMean src\nMean tgt\nMedian src\nMedian tgt\nTrain\n675.00\n616.44\n417\n372\nVal\n644.74\n471.47\n180\n110\nTest\n429.12\n398.78\n363\n318.5\nTable 6: GPT-2 Tokenizer lengths.\ngeneration parameters, each attempting to generate 200 programs. We found this configuration to be the\nmost cost-effective per new-sample with relatively promising rates of novelty.\nWe found that after attempting to generate 10,000 new programs through the prompting strategy, 6,553 were\nnot in the training/validation/test set of PIE. We keep track of equivalent programs of the ones generated,\nand of these 6,553 generations we found 3,314 equivalence sets. In total, this required executing over 1.4\nmillion binary input pairs. Parallelized on a 24-core Intel 13900k processor with 64GB of RAM, this took\nless than 72 hours to complete.\nA.5\nABLATION OF RETRIEVAL-BASED FEW-SHOT PROMPTING CONFIGURATION\nFor our retrieval-based prompting experiment we tried multiple configurations for the number of retrieved\nprompts where of K = {1, 2, 4} of the K closest retrieved prompts.\nTable 7: Retrieval-based few-shot prompting experiments: Results for various models and datasets configu-\nrations. *We note that for GPT4, we report only with Best@4, given resource constraints.\nBest@1\nBest@8\nMethod\nModel\n%Opt\nSpeedup\nCorrect\n%Opt\nSpeedup\nCorrect\nDynamic Retrieval, K=1\nCodeLlama7B\n3.39%\n1.09\n17.29%\n16.33%\n1.52\n52.49%\nDynamic Retrieval, K=1\nCodeLlama13B\n5.62%\n1.17\n22.59%\n23.22%\n1.74\n65.43%\nDynamic Retrieval, K=1\nCodeLlama34B\n10.51%\n1.26\n32.06%\n35.67%\n2.26\n72.61%\nDynamic Retrieval, K=2\nCodeLlama7B\n4.60%\n1.14\n21.21%\n17.35%\n1.52\n56.74%\nDynamic Retrieval, K=2\nCodeLlama13B\n9.40%\n1.36\n29.47%\n28.74%\n1.99\n66.25%\nDynamic Retrieval, K=2\nCodeLlama34B\n12.67%\n1.33\n31.87%\n42.16%\n2.57\n77.92%\nDynamic Retrieval, K=4\nCodeLlama7B\n5.54%\n1.19\n23.72%\n18.81%\n1.63\n60.08%\nDynamic Retrieval, K=4\nCodeLlama13B\n9.61%\n1.30\n27.69%\n29.05%\n2.07\n64.26%\nDynamic Retrieval, K=4\nCodeLlama34B\n12.12%\n1.35\n31.45%\n43.68%\n2.47\n75.44%\nDynamic Retrieval, K=2\nGPT3.5\n26.28%\n1.58\n80.47%\n48.16%\n2.14\n97.96%\nDynamic Retrieval, K=2\nGPT4\n50.15%\n2.61\n80.82%\n69.03%*\n3.56*\n95.90%*\n18\nPreprint. Under review.\nusing namespace std;\ntypedef long long ll;\ninline void getInt(int* p);\nconst int maxn=1000010;\nconst int inf=0x3f3f3f3f;\nll n;\nll dp[maxn];\nll a[maxn];\nint main()\n{\ngbt']","Dynamic Retrieval, K=2
GPT4
50.15%
2.61
80.82%
69.03%*
3.56*
95.90%*",simple,"[{'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}]",TRUE
What approach does Shypula et al. (2021) take to superoptimize assembly programs after compilation?,"['highly effective at achieving strong optimization abilities in LLMs. When allowing a model to take 8\nsamples and filtering for correctness and execution time, our fine-tuned performance-conditioned version\nof CODELLAMA 13B can achieve an average speedup of 5.65× on our test set, and a fine-tuned version of\nGPT-3.5 augmented with synthetic data via self-play achieves an average speedup of 6.86×, whereas the\nfastest human solutions we found achieve an average speedup of 4.06×. In summary, our contributions are:\n• We introduce a new code dataset of more than 77 K C++ program pairs, named PIE, with execution time\nannotations collected from the gem5 simulator. PIE enables reproducible evaluation of LLMs for program\noptimization and reliable performance annotations for training.\n2\nPreprint. Under review.\n• Enabled by our benchmark, we evaluate different prompting and fine-tuning approaches for adapting pre-\ntrained LLMs to optimize programs. Our results indicate that pre-trained code LLMs are limited in their\nability to optimize code without a dataset like PIE.\n• We develop three effective strategies for adapting LLMs for code optimization: retrieval-based prompting,\nperformance-conditioning, and self-play. Overall, our best model, GPT-3.5 augmented with synthetic data\nobtained from self-play, achieves an average speedup of 6.86×, and optimizes 87.68% of the test set by at\nleast 10%.\nRelated work. Beyond the approaches described above, machine learning has been applied to improve\nperformance by refactoring code (Mens & Tourwé, 2004; Agnihotri & Chug, 2020), identify compiler trans-\nformations (Bacon et al., 1994; Talaashrafi, 2022), perform parameter search (Hamadi & Hamadi, 2013;\nHuang et al., 2019; Kaufman et al., 2021), auto-vectorize code (Nuzman et al., 2006; Mendis et al., 2019),\noptimize GPU code (Liou et al., 2020; Cummins et al., 2021), and automatically select algorithms (Kotthoff,\n2016; Kerschke et al., 2019). and room at the top (Leiserson et al., 2020; Sherry & Thompson, 2021). Deep-\nPERF (Garg et al., 2022) uses a transformer-based model fine-tuned to generate performance improvement\npatches for C# applications. Additionally, Chen et al. (2022) uses a discrete variational auto-encoder, each\nlatent representation maps to a different category of code edits, and canonicalized code representations to au-\ntomatically suggest performance improvements, Shypula et al. (2021) trains seq2seq models from scratch on\noptimization data to superoptimize assembly programs after compilation, Shi et al. (2019) trains tree-LSTM\nfrom scratch with RL to superoptimize halide IR, and MAGPIE (Blot & Petke, 2022) uses genetic algo-\nrithms for tasks including optimization. AlphaCode (Li et al., 2021) leverages language models to generate\nsolutions to competitive programming problems in natural language, but it does not attempt to improve the\nperformance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b;']",Shypula et al. (2021) trains seq2seq models from scratch on optimization data to superoptimize assembly programs after compilation.,simple,"[{'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}]",TRUE
How does a diverse dataset help develop generalized robotic policies?,"['Mohit Sharma4, Moo Jin Kim27, Naoaki Kanazawa30, Nicklas Hansen34, Nicolas Heess11, Nikhil J Joshi11, Niko Suenderhauf24, Ning Liu13,\nNorman Di Palo14, Nur Muhammad Mahi Shafiullah22, Oier Mees36, Oliver Kroemer4, Osbert Bastani40, Pannag R Sanketi11,\nPatrick ”Tree” Miller31, Patrick Yin44, Paul Wohlhart11, Peng Xu11, Peter David Fagan35, Peter Mitrano38, Pierre Sermanet11, Pieter Abbeel32,\nPriya Sundaresan27, Qiuyu Chen44, Quan Vuong11, Rafael Rafailov11,27, Ran Tian32, Ria Doshi32, Roberto Mart´\nın-Mart´\nın29,\nRohan Baijal44, Rosario Scalise44, Rose Hendrix1, Roy Lin32, Runjia Qian13, Ruohan Zhang27, Russell Mendonca4, Rutav Shah29,\nRyan Hoque32, Ryan Julian11, Samuel Bustamante10, Sean Kirmani11, Sergey Levine11,32, Shan Lin34, Sherry Moore11, Shikhar Bahl4,\nShivin Dass41,29, Shubham Sonawani2, Shuran Song5, Sichun Xu11, Siddhant Haldar22, Siddharth Karamcheti27, Simeon Adebola32, Simon Guist18,\nSoroush Nasiriany29, Stefan Schaal15, Stefan Welker11, Stephen Tian27, Subramanian Ramamoorthy35, Sudeep Dasari4, Suneel Belkhale27,\nSungjae Park17, Suraj Nair31, Suvir Mirchandani27, Takayuki Osa30, Tanmay Gupta1, Tatsuya Harada30,25, Tatsuya Matsushima30,\nTed Xiao11, Thomas Kollar31, Tianhe Yu11, Tianli Ding11, Todor Davchev11, Tony Z. Zhao27, Travis Armstrong11, Trevor Darrell32,\nTrinity Chung32, Vidhi Jain11,4, Vincent Vanhoucke11, Wei Zhan32, Wenxuan Zhou11,4, Wolfram Burgard42, Xi Chen11,\nXiaolong Wang34, Xinghao Zhu32, Xinyang Geng32, Xiyuan Liu13, Xu Liangwei13, Xuanlin Li34, Yao Lu11, Yecheng Jason Ma40, Yejin Kim1,\nYevgen Chebotar11, Yifan Zhou2, Yifeng Zhu29, Yilin Wu4, Ying Xu11, Yixuan Wang37, Yonatan Bisk4, Yoonyoung Cho17, Youngwoon Lee32,\nYuchen Cui27, Yue Cao13, Yueh-Hua Wu34, Yujin Tang11,30, Yuke Zhu29, Yunchu Zhang44, Yunfan Jiang27, Yunshuang Li40,\nYunzhu Li37, Yusuke Iwasawa30, Yutaka Matsuo30, Zehan Ma32, Zhuo Xu11, Zichen Jeff Cui22, Zichen Zhang1, Zipeng Fu27, Zipeng Lin32\nFig. 1: We propose an open, large-scale dataset for robot learning curated from 21 institutions across the globe. The dataset represents\ndiverse behaviors, robot embodiments and environments, and enables learning generalized robotic policies.\nAbstract— Large, high-capacity models trained on diverse\ndatasets have shown remarkable successes on efficiently tackling\ndownstream applications. In domains from NLP to Computer\nVision, this has led to a consolidation of pretrained models,\nwith general pretrained backbones serving as a starting point\nfor many applications. Can such a consolidation happen in\nrobotics? Conventionally, robotic learning methods train a\nseparate model for every application, every robot, and even\nevery environment. Can we instead train “generalist” X-robot\npolicy that can be adapted efficiently to new robots, tasks,\nand environments? In this paper, we provide datasets in\nstandardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation,']","Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications.",reasoning,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
How is the dataset for adapting code LLMs to performance optimization constructed from human programmers' edits in competitive programming tasks in CodeNet?,"['performance of existing solutions. In contrast, we focus on adapting pre-trained LLMs (Chen et al., 2021b;\nNijkamp et al., 2022; Tunstall et al., 2022; Xu et al., 2022; Fried et al., 2022) to performance optimization.\n2\nPERFORMANCE IMPROVING EDITS (PIE) DATASET\nWe construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing\nprogram execution time. Our dataset is constructed based on performance-improving edits (PIE) made by\nhuman programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a\nproblem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu\n1 , yu\n2 , ...] be\na chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs\nthat were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests)\nor take more than the allowed time to run, resulting in a trajectory of programs Y∗= [y∗\n1, y∗\n2, . . . , y∗\nn].\nFor each trajectory Y∗, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which\n(time(yi)−time(y>i))\ntime(yi)\n> 10% where time (y) is the measured latency of program y (i.e., the relative time\nimprovement is more than 10%). The CodeNet dataset includes CPU time, but we found the information to\nbe inconsistent (see Appendix A.3). Thus, we relabel the execution time using gem5 as described below; to\ncreate these annotated runtimes, we performed over 42.8 million simulations in our gem5 environment.\nWe split the resulting dataset of pairs P into train/validation/test sets, ensuring that any particular competitive\nprogramming problem only appears in one of them. We obtain a training set of 77,967 pairs from 1,474\nproblems, a validation set of 2,544 pairs from 77 problems, and a test set of 982 pairs from 41 problems.\nFor each pair in the test set, we also record the fastest human submission execution time for that problem; in\nSection 4, we include this running time as a comparison point.\nTest cases. Our goal is to improve performance while ensuring correctness. We evaluate correctness through\nunit tests; we reject the program if a single test fails. CodeNet includes an average of 4 test cases per problem.\nTo improve coverage, we include additional test cases from AlphaCode (Li et al., 2021) generated with a\nfine-tuned LLM. A small set of test cases would lead to substantial timeouts above 2 minutes in gem5;\n3\nPreprint. Under review.\nafter excluding them, we obtain a median of 82.5 test cases per problem in our training set, 75 test cases\nper problem in our validation set, and 104 test cases per problem for our test set. See Appendix A.2 for\nadditional details.\nPerformance measurement using gem5. Benchmarking program performance is notoriously difficult. For\ninstance, code instrumentation introduces overhead, and there is substantial variance across executions due']","We construct a dataset targeted at adapting code LLMs to performance optimization, focusing on optimizing program execution time. Our dataset is constructed based on performance-improving edits (PIE) made by human programmers in a range of competitive programming tasks from CodeNet (Puri et al., 2021). Given a problem, programmers typically write an initial solution and iteratively improve it. Let Yu = [yu 1 , yu 2 , ...] be a chronologically sorted series of programs, written by user u for problem x. From Yu, we remove programs that were not accepted by the automated system, eliminating incorrect programs (fail one or more unit tests) or take more than the allowed time to run, resulting in a trajectory of programs Y∗= [y∗ 1, y∗ 2, . . . , y∗ n]. For each trajectory Y∗, we construct pairs P = (y1, y2), (y1, y3), (y2, y3) . . ., and keep only pairs for which (time(yi)−time(y>i)) time(yi) > 10% where time (y) is the measured latency of program y (i.e., the relative time improvement is more than 10%).",reasoning,"[{'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}]",TRUE
"How does PaLM-SayCan use the value function to perform tasks given by humans, utilizing LLM's semantic abilities and physical embodiment?","['robotics. Through interaction with language mod-\nels, robots can possess higher-level intelligence and\ncomprehension abilities, opening up new avenues for\nresearch and development in robotics.\n2.3. Robotics Based on LLMs\nIn this subsection, we introduce the smart robotics based\non LLMs in recent years. LLMs are used as brains in the part\nof robotics. First, we summarize the models in recent years\nin Table 2.\n2.3.1. PaLM-SayCan\nWith the increasing popularity of LLMs, people have\nbegun to wonder whether these models can be used to\nassist robots in performing various daily tasks. However,\nthere are challenges in enabling robots to extract knowledge\nfrom LLMs and interact with the physical world. LLMs\ncontain valuable semantic information about the real world,\naiding robots in understanding natural language. Nonethe-\nless, giving LLMs a physical form capable of interacting\nand making real-world decisions is challenging due to their\nlack of experience with physical objects and environments.\nPaLM-SayCan [1] can function as the physical embodiment\nof LLM, utilizing LLM’s semantic capabilities to process\nnatural language instructions. PaLM-SayCan enables robots\nto execute tasks assigned by humans through the value\nfunction. PaLM-SayCan features pre-trained meta-actions\ncontrolled by visual motors, while BC-Z [58] and MT-Opt\n[64] are employed to learn language-conditioned BC and\nRL policies, respectively. LLM can decompose received\nnatural language instructions into smaller, manageable tasks.\nBased on the current status, capabilities, and surrounding\nenvironment of the robot, actions can be flexibly executed.\nTo determine the feasibility of an action, PaLM-SayCan\nrelies on a logarithmic estimation of the value function and\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 4 of 19\nLarge Language Models for Robotics: A Survey\nTable 2\nLLMs for robot in recent years\nYear\nLLM-based robotics\nDescription\n2022\nPaLM-SayCan [1]\nPaLM-SayCan can function as the physical embodiment of LLM, utilizing LLM’s semantic capabilities to process\nnatural language instructions. Enabling robots to execute tasks assigned by humans through the value function.\n2023\nPaLM-E [34]\nPaLM-E boasts an LLM capable of integrating continuous sensory information from the real world, effectively\nbridging the gap between language and perception.\n2023\nLM-Nav [117]\nLM-Nav was developed, exploiting the advantages of language to facilitate effective communication between\nusers and robots. The LM-Nav system comprises three components: a vision-navigation model (VNM); a vision\nlanguage model (VLM); and a large language model (LLM).\n2023\nExpedition A11\nExpedition A1, developed by AGIBot, embodies the company’s commitment to seamlessly integrating advanced AI\ninto robotics and fostering harmonious collaboration between humans and machines.\nEmbodyAI / Agents\nCan you help\nme get an apple?\nWalking to the kitchen\n\xa0Opening the refrigerator\nObtaining the apple\nDelivering it to the requester']","PaLM-SayCan uses the value function to perform tasks given by humans by utilizing LLM's semantic abilities and physical embodiment. It processes natural language instructions using LLM's semantic capabilities and executes tasks assigned by humans through the value function. PaLM-SayCan features pre-trained meta-actions controlled by visual motors and learns language-conditioned BC and RL policies. It can decompose natural language instructions into smaller tasks and execute them based on the robot's current status, capabilities, and surrounding environment.",multi_context,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What labels does OWL-ViT provide and what objects are associated with them?,"['2). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nScene Image\nObject Detections\nTask Instructions\n1) clothing [green hoodie]\n2) towel\n3) clothing [striped shirt]\n4) bottle [sunscreen bottle]\n5) towel [socks]\n6) mouse [ear thermometer]\n7) suitcase\n8) bottle [hand sanitizer]\n9) hair dryer [dumbbell]\n10) clothing [blue shirt]\n1) Bring me the heaviest object. [S]\n2) Bring me all clear containers. [M]\n3) Bring me the hard plastic object.\n[M]\n4) Bring me the lightest piece of\nclothing. [S]\n5) Bring me the object I can pack my\nclothes into. [C]\n6) It is cold outside. Bring me some-\nthing that can keep me warm. [C]\n7) It is sunny outside. Bring me the\ncontainer of sunscreen. [C]\n1) facial tissue holder [paper towel\ndispenser]\n2) light switch [left electric outlet]\n3) light switch [right electric outlet]\n4) mixer\n5) toaster\n6) kettle\n7) paper towel\n8) water glass [plastic cup]\n9) salt and pepper shakers [salt]\n10) bottle [jam container]\n11) frying pan [baking pan]\n12) container\n[salmon-colored\ncon-\ntainer]\n13) salt and pepper shakers [pepper]\n14) countertop\n1) Bring me the heaviest object. [S]\n2) Bring me the heavier glass con-\ntainer. [M]\n3) Bring me something that is easy\nto tear. [C]\n4) Bring me the lightest container\nthat is empty but can be filled with\nwater. [M]\n5) Bring me the most deformable\ncontainer with a lid. [M]\n6) Bring me all metal containers that\ncan be used to carry water. [M]\n7) Bring me the object that can be\nused in an oven. [C]\nTABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 3 and\n4). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nScene Image\nObject Detections\nTask Instructions\n1) toaster\n2) light switch [electric outlet]\n3) envelope [napkin on microwave]\n4) light switch\n5) microwave oven [microwave]\n6) door [microwave door]\n7) bottle [glass sauce bottle]\n8) picnic basket [drying rack]\n9) soap dispenser\n10) bottle [plastic bottle with blue\nvanilla flavor]\n11) mug [dry mug]\n12) sink\n13) frying pan [dirty pan in sink]\n14) mug [dirty mug in sink]\n15) countertop\n16) waste container\n17) cupboard\n18) plastic bag [trashbag]\n1) Bring me the heaviest object that\nyou can carry. [S]\n2) Bring me an empty mug that I can\nuse to make tea. [C]\n3) Bring me the most deformable ob-\nject. [S]\n4) Bring me the glass object. [S]\n5) Bring me a metal pan that is in the\nsink. [C]\n6) Bring me the container that stores\ntrash. [C]'
 '2). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nScene Image\nObject Detections\nTask Instructions\n1) clothing [green hoodie]\n2) towel\n3) clothing [striped shirt]\n4) bottle [sunscreen bottle]\n5) towel [socks]\n6) mouse [ear thermometer]\n7) suitcase\n8) bottle [hand sanitizer]\n9) hair dryer [dumbbell]\n10) clothing [blue shirt]\n1) Bring me the heaviest object. [S]\n2) Bring me all clear containers. [M]\n3) Bring me the hard plastic object.\n[M]\n4) Bring me the lightest piece of\nclothing. [S]\n5) Bring me the object I can pack my\nclothes into. [C]\n6) It is cold outside. Bring me some-\nthing that can keep me warm. [C]\n7) It is sunny outside. Bring me the\ncontainer of sunscreen. [C]\n1) facial tissue holder [paper towel\ndispenser]\n2) light switch [left electric outlet]\n3) light switch [right electric outlet]\n4) mixer\n5) toaster\n6) kettle\n7) paper towel\n8) water glass [plastic cup]\n9) salt and pepper shakers [salt]\n10) bottle [jam container]\n11) frying pan [baking pan]\n12) container\n[salmon-colored\ncon-\ntainer]\n13) salt and pepper shakers [pepper]\n14) countertop\n1) Bring me the heaviest object. [S]\n2) Bring me the heavier glass con-\ntainer. [M]\n3) Bring me something that is easy\nto tear. [C]\n4) Bring me the lightest container\nthat is empty but can be filled with\nwater. [M]\n5) Bring me the most deformable\ncontainer with a lid. [M]\n6) Bring me all metal containers that\ncan be used to carry water. [M]\n7) Bring me the object that can be\nused in an oven. [C]\nTABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 3 and\n4). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nScene Image\nObject Detections\nTask Instructions\n1) toaster\n2) light switch [electric outlet]\n3) envelope [napkin on microwave]\n4) light switch\n5) microwave oven [microwave]\n6) door [microwave door]\n7) bottle [glass sauce bottle]\n8) picnic basket [drying rack]\n9) soap dispenser\n10) bottle [plastic bottle with blue\nvanilla flavor]\n11) mug [dry mug]\n12) sink\n13) frying pan [dirty pan in sink]\n14) mug [dirty mug in sink]\n15) countertop\n16) waste container\n17) cupboard\n18) plastic bag [trashbag]\n1) Bring me the heaviest object that\nyou can carry. [S]\n2) Bring me an empty mug that I can\nuse to make tea. [C]\n3) Bring me the most deformable ob-\nject. [S]\n4) Bring me the glass object. [S]\n5) Bring me a metal pan that is in the\nsink. [C]\n6) Bring me the container that stores\ntrash. [C]']","The labels provided by OWL-ViT are sometimes inaccurate or ambiguous. The objects associated with the labels in the given context are: 
1) clothing [green hoodie], clothing [striped shirt], clothing [blue shirt]
2) towel
3) bottle [sunscreen bottle], bottle [hand sanitizer], bottle [jam container], bottle [glass sauce bottle], bottle [plastic bottle with blue vanilla flavor]
4) mouse [ear thermometer]
5) suitcase
6) hair dryer [dumbbell]
7) facial tissue holder [paper towel dispenser]
8) light switch [left electric outlet], light switch [right electric outlet]
9) mixer
10) toaster
11) kettle
12) water glass [plastic cup]
13) salt and pepper shakers [salt], salt and pepper shakers [pepper]
14) frying pan [baking pan], frying pan [dirty pan in sink]
15) container [salmon-colored container]
16) paper towel
17) mug [dry mug], mug [dirty mug in sink]
18) countertop
19) waste container
20) cupboard
21) plastic bag [trashbag]
22) envelope [napkin on microwave]
23) microwave oven [microwave], door [microwave door]
24) picnic basket [drying rack]
25) soap dispenser
26) sink",multi_context,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
What is the purpose and application of the LAnguage Trajectory TransformEr in robotics?,"['exchanges utilizing authentic human language. They possess\nknowledge of other intelligent entities within their vicinity,\nand the generative agent framework dictates whether they\nproceed to interact or initiate a dialogue. These intelligent\nagents’ characters can exhibit quite realistic personal behav-\nior and social interactions. For example, when someone tells\none of the agents that they have a desire to organize and host\na festive gathering to celebrate Valentine’s Day, these agents\nwill spontaneously invite others to attend, meet each other,\ndate, and be on time for the party together. This innovative\narchitecture empowers generative agents with the ability to\nretain, recall, contemplate, engage with fellow agents, and\nstrategize amidst ever-changing circumstances.\n3.4.2. Language-based human-robot interaction\nThere are GUI (Graphical User Interface) and LUI (Lan-\nguage User Interface) for human-robot interaction. GUI\nrefers to a computer-operated user interface that is graphi-\ncally displayed and uses an interactive device to manage the\ninteraction with the system. Unlike GUI, LUI can directly\nuse natural human language for human-robot interaction,\nand the most representative LUI product is ChatGPT. Tradi-\ntionally, the task of simulating human-robot interaction us-\ning natural language has proven to be difficult due to the con-\nstraints imposed on users by rigid instructions, or the need\nfor intricate algorithms to manage numerous probability dis-\ntributions related to actions and target objects[4]. However,\nit is not easy to translate instructions into commands that\nrobots can understand in the real world, and traditionally,\nfixed collections of desired actions and directives have been\nused to enable robots to understand human language. How-\never, this can significantly limit the robot’s flexibility and has\nlimited generalizability across different hardware platforms.\nThe LAnguage Trajectory TransformEr [16] introduces a\nversatile language-driven framework that empowers users\nto customize and adapt the overall trajectories of robots.\nThe approach leverages pre-trained language models (e.g.,\nBERT [31] and CLIP [99]) to encode the user’s intention\nand target objects directly from unrestricted text inputs and\nscene images. It combines geometric features produced by a\nnetwork of transformer encoders and generates the trajectory\nusing a transformer decoder, eliminating the requirement for\nprior task-related or robot-specific information.\nConsidering the vagueness and ambiguity of natural\nlanguage, from the point of view of human-robot interaction,\nrobots should enhance the initiative of interaction in the\nfuture, that is to say, let the robot actively ask the user\nquestions through the large language model. If the robot feels\nthat the user’s words are problematic and is not sure what\nthey mean, it should ask you back what you mean or whether\nyou mean what you say.\n4. Applications of LLMs in Robotics']","The purpose and application of the LAnguage Trajectory TransformEr in robotics is to empower users to customize and adapt the overall trajectories of robots. It leverages pre-trained language models to encode the user's intention and target objects directly from unrestricted text inputs and scene images. It combines geometric features and generates the trajectory using a transformer decoder, eliminating the requirement for prior task-related or robot-specific information.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What is the purpose of asking questions about the objects in the scene before completing the task?,"['first providing the scene image to an OWL-ViT ViT-L/14\nopen-vocabulary object detector [44], which produces object\nbounding boxes and category labels from the EgoObjects\ncategories. We then provide the list of detected objects and\nthe task instruction to our LLM, which is GPT-4 [38] with\ntemperature 0. The LLM is additionally provided with the\nrobotic primitives, and a few-shot chain-of-thought prompt\n[45] with instructions to ask questions about objects in the\nscene to determine how to complete the task, and then pro-\nduce a plan using the primitives. There is no constraint on the\nquestions that the LLM can ask, except for encouragement\nin the prompt to ask questions that can be answered with\nyes or no. The same prompt is used for all scenes and tasks,\nwhich we provide in Listing 1.\nAfter the LLM asks a set of object-centric questions, a\nVLM answers each question prompted with the bounding\nbox of the object indicated by the LLM, and then provides\nthe LLM with its highest likelihood responses and their\nassociated likelihoods/confidence scores, as done in prior\nwork for VQA [30]. This continues until the LLM decides\nit has enough information, whereupon it either indicates that\nthe task is not possible, or produces a plan consisting of a list\nof primitives to execute for the task. The few-shot examples\nin Listing 1 illustrate how interaction between the LLM and\nVLM for planning is structured.\nPrimitives. We list the primitives for our real scene planning\nevaluation below:\n• go to object [X]\n• pick up object [X]\n• bring to human object [X]\n• put down object [X]\n• done\nThe primitives (except done) are parameterized by a letter\n(in place of [X]) that identifies each detected object in the\nscene. The assignment of letters is provided in the list of\nobject detections given to the LLM planner.\nScenes and Tasks. In Table XXII, we provide the scene\nimages in our evaluation, and the detected objects and task\ninstructions for each scene. We also indicate the task type\nfor each instruction.\nPrompts. We provide the prompts used by our LLM-based\nplanning framework for our scene planning evaluation. The\nversion with VLM interaction is in Listing 1 and the version\nwithout VLM interaction is in Listing 2. The parts of the\nprompts in square brackets are replaced with the correspond-\ning information specific to the task, in the same format as\nthe prompt example.\nListing 1: Prompt for LLM planner with VLM interaction.\nYou are a household robot. You are able to move most\nhousehold objects, but not large or heavy furniture.\nYou are to be safe and not break anything.\nYou will be given a list of objects in the scene.\nA human will give you a task instruction to perform.\nFirst, ask questions about the objects to learn more about\nthem to determine how to properly complete the task.\nIndicate the object letters before asking the question,\nand do not provide the answer.\nFormat each question like ""Question about object [A, B]:\nIs this object heavy?"".']",The purpose of asking questions about the objects in the scene before completing the task is to learn more about them and determine how to properly complete the task.,simple,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
What is the purpose of object instance IDs in the dataset?,"['of real images that was publicly released when constructing\nPHYSOBJECTS. The dataset consists of frames from egocen-\ntric videos in realistic household settings, which makes it par-\nticularly relevant for household robotics. It includes 117,424\nimages, 225,466 object bounding boxes with corresponding\ncategory labels from 277 object categories, and 4,203 object\ninstance IDs. PHYSOBJECTS consists of physical concept\nannotations for a large subset of this image data. 1\nWe construct random training, validation, and test sets\nbased on object instance IDs. We split the dataset per object\ncategory to ensure each object category is represented in\neach set when possible. Our training, validation, and test sets\nconsist of 73.0%, 14.8%, and 12.2% of objects, respectively.\nConcept\nDescription\nMass\nhow heavy an object is\nFragility\nhow easily an object can be broken/damaged\nDeformability\nhow easily an object can change shape without breaking\nMaterial\nwhat an object is primarily made of\nTransparency\nhow much can be seen through an object\nContents\nwhat is inside a container\nCan Contain Liquid\nif a container can be used to easily carry liquid\nIs Sealed\nif a container will not spill if rotated\nDensity (held-out)\nhow much mass per unit of volume of an object\nLiquid Capacity (held-out)\nhow much liquid a container can contain\nTABLE I: Our physical concepts and brief descriptions\nPhysical Concepts. We collect annotations for eight main\nphysical concepts and two additional concepts reserved for\nheld-out evaluation. We select concepts based on prior work\nand what we believe to be useful for robotic manipulation,\nbut do not consider all such concepts. For example, we do not\ninclude friction because this can be challenging to estimate\nwithout interaction, and we do not include volume because\nthis requires geometric reasoning, which we do not focus on.\nOf our main concepts, three are continuous-valued and\napplicable to all objects: mass, fragility, and deformability.\nTwo are also applicable to all objects, but are categorical:\nmaterial and transparency. Transparency could be consid-\nered continuous, but we use discrete values of transparent,\ntranslucent, and opaque. The other three are categorical and\napplicable only to container objects: contents, can contain\nliquid, and is sealed. We define which object categories are\ncontainers, resulting in 956 container object instances.\nOur two held-out concepts are density, which is continuous\nand applicable to all objects, and liquid capacity, which is\ncontinuous and applicable only to containers. We only collect\ntest data for these held-out concepts. We list all concepts and\ntheir brief descriptions in Table I.\nFor categorical concepts, we define a set of labels for each\nconcept. Annotations consist of a label specified for a given\nobject and concept. For the concepts material and contents,\nwhen crowd-sourcing, we allow for open-ended labels if\nnone of the pre-defined labels are applicable.']",The purpose of object instance IDs in the dataset is to identify and track individual objects in the images.,simple,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
What is the purpose of investigating emergent goal-like behavior in large language models using experimental economics?,"['choice for the next round?\nI choose ‘project blue’ for the fourth round.\nYour partner chose Project Green in that round. Now we will move on the next round. What is your\nchoice for the next round?\n12\nInvestigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics\nI choose ‘project blue’ for the ﬁfth round.\nYour partner chose Project Green in that round. Now we will move on the next round. What is your\nchoice for the next round?\nFor the ﬁnal round, I choose ‘project blue’.\n13']",The purpose of investigating emergent goal-like behavior in large language models using experimental economics is to understand and analyze the behavior of these models and how they interact with human decision-making processes.,simple,"[{'Authors': 'Steve Phelps, Yvan I. Russell', 'Published': '2023-05-13', 'Summary': ""In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms."", 'Title': 'Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics'}]",TRUE
Why does imitation learning require large amounts of data?,"['produces a policy that takes an image as input, and outputs\nactions that control the robot to perform desirable behav-\niors. Directly reasoning from images allows such methods to\nbe generally applied as they circumvent the need for task-\ndependent estimation of state or design of features.\nBut, there is no free lunch. The generality of learning vision-\nbased policies comes at the cost of needing a large number\nof demonstrations. MIME [59] uses 400 demonstrations per\ntask, while robomimic [36] uses 200 demonstrations to train\nmanipulation policies. This scale of data signiﬁcantly hampers\nour ability to train multiple skills in reasonable amounts\nof time. Furthermore, collecting large amounts of demon-\nstrations is physically and cognitively taxing on the human\ndemonstrators due to the nature of available teleoperation\nframeworks [5]. Hence, getting imitation learning to work\nwith few demonstrations is paramount for practical training\nof robotic skills.\nTo\nunderstand\nwhy\nimitation\nlearning\nrequires\nlarge\namounts of data, let us take a look at one common paradigm\n– ofﬂine imitation. Methods in this class such as Behavior\narXiv:2303.01497v1  [cs.RO]  2 Mar 2023\nCloning (BC) [47] or Nearest Neighbor retrieval (NN) [43] use\na supervised learning objective to maximize the likelihood of\ndemonstrated actions given observations in the demonstration.\nTo ensure that the resulting policy is generalizable to varying\nfactors in deployment (e.g. object conﬁgurations), the demon-\nstration set used in training will need to span these factors of\nvariation. Without sufﬁcient coverage, which is only possible\nwith large amounts of demonstration data, trained policies\noften suffer from distribution shift during deployment [54].\nTo address the large data requirements of ofﬂine imitation\nand instead imitate with few examples, a promising direction\nis to adapt policies that were trained ofﬂine with online\nRL [39, 25, 51]. The hope is that while the ofﬂine policy,\ntrained with few demonstrations, would fail in deployment,\nonline RL will allow the policy to improve and adapt to\ndeployment scenarios. But how does the RL algorithm get the\nrewards needed for adaptation? Constructing a task-speciﬁc\nreward function is one possibility [51, 48]. However, this\nstrategy may not be applicable in real-world scenarios where\nstates of objects are hard to estimate or reward functions are\nhard to create.\nIn this work, we present Fast Imitation of Skills from\nHumans (FISH), a new technique for robotic imitation, where\ngiven only a minute of demonstrations (between 1 to 3\ntrajectories), a robot can learn visual policies that both solve\nthe task and adapt to new object conﬁgurations through\nsubsequent online training. FISH operates in two phases. First,\na weak base policy is learned by ofﬂine imitation on the\nfew demonstrations. Second, a residual policy [61, 28, 76, 2]\nis trained to produce corrective offsets to the weak policy.']","Imitation learning requires large amounts of data because methods like Behavior Cloning or Nearest Neighbor retrieval use a supervised learning objective to maximize the likelihood of demonstrated actions given observations in the demonstration. To ensure that the resulting policy is generalizable to varying factors in deployment, the demonstration set used in training needs to span these factors of variation. Without sufficient coverage, which is only possible with large amounts of demonstration data, trained policies often suffer from distribution shift during deployment.",simple,"[{'Authors': 'Siddhant Haldar, Jyothish Pari, Anant Rai, Lerrel Pinto', 'Published': '2023-03-02', 'Summary': 'While imitation learning provides us with an efficient toolkit to train\nrobots, learning skills that are robust to environment variations remains a\nsignificant challenge. Current approaches address this challenge by relying\neither on large amounts of demonstrations that span environment variations or\non handcrafted reward functions that require state estimates. Both directions\nare not scalable to fast imitation. In this work, we present Fast Imitation of\nSkills from Humans (FISH), a new imitation learning approach that can learn\nrobust visual skills with less than a minute of human demonstrations. Given a\nweak base-policy trained by offline imitation of demonstrations, FISH computes\nrewards that correspond to the ""match"" between the robot\'s behavior and the\ndemonstrations. These rewards are then used to adaptively update a residual\npolicy that adds on to the base-policy. Across all tasks, FISH requires at most\ntwenty minutes of interactive learning to imitate demonstrations on object\nconfigurations that were not seen in the demonstrations. Importantly, FISH is\nconstructed to be versatile, which allows it to be used across robot\nmorphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g.\nthird-person, eye-in-hand). Our experimental evaluations on 9 different tasks\nshow that FISH achieves an average success rate of 93%, which is around 3.8x\nhigher than prior state-of-the-art methods.', 'Title': 'Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations'}]",TRUE
What are the limitations of the FISH algorithm?,"['provides a solution to this problem by learning a robust reward\nfunction through online interactions but suffers from sample\ninefﬁciency [32]. There has been some work on improving\nthe sample efﬁciency of IRL [32, 22, 70, 64], with some\nvisual extensions to these IRL approaches [25, 9, 66, 50, 12].\nThere has also been demonstration of such IRL approaches\nperforming complex tasks on real robots [1, 25, 33].\nOptimal Transport (OT) OT [68, 46] provides a tool for\ncomparing probability measures while including the geome-\ntry of the space. In imitation learning, OT can be used to\ncompute the alignment between a set of agent and expert\nobservations using distance metrics such as Sinkhorn [14],\nGromov-Wasserstein [45], GDTW [11], CO-OT [53] and Soft-\nDTW [15]. Many of these distance metrics have an associated\nIL algorithm - SIL [42] uses Sinkhorn, PWIL [16] uses greedy\nWasserstein, GDTW-IL [11] uses GDTW, and GWIL [20]\nusing Gromov-Wasserstein. Recent work by Cohen et al. [12]\nhas demonstrated that the Sinkhorn distance [42] produces the\nmost efﬁcient learning among the discussed metrics and can be\ncombined with ofﬂine pretraining to efﬁcient perform complex\ntasks in the real world [25]. OT has also seen use in the ﬁeld of\ncomputer vision [55, 4] to show improvements for Generative\nAdversarial Networks (GANs) [23]. In this work, we adopt\nthe Sinkhorn metric for online learning and combine it with\nnon-parametric IL approaches to perform precise tasks across\nthree robot morphologies.\nResidual RL for robotics Learning residuals through RL\nenables safe and robust online learning [61, 28, 76, 2]. Resid-\nual RL operates by applying offsets on top of a base policy.\nPrior works either use a hand-engineered controller [61, 28]\nor a policy learned from demonstrations [2] as the base\npolicy. In this work, we resort to the latter and use non-\nparametric base policies obtained from one minute of expert\ndemonstration. Prior works also assume the availability of\ntask-speciﬁc rewards for learning the online policy. However,\nwe differ from this and use OT matching to obtain rewards\nfrom the collected demonstration set.\nVI. CONCLUSION\nIn this work, we present a new algorithm for fast imitation\nlearning, FISH, that demonstrates improved performance com-\npared to prior state-of-the-art work on a variety of real robot\ntasks across three different robot morphologies. We demon-\nstrate that combining an imperfect base policy with a learned\nresidual policy can enable performing precise tasks with one\nminute of demonstration collection and limited environment\ninteractions. Further, we ablate over various design decisions\nof FISH, which shows the importance of learning stable\nrepresentations, choosing the right base policy, and performing\nguided exploration. While powerful, we recognize that FISH\nhas limitations (see Section IV-K).\nACKNOWLEDGMENTS\nWe thank Sridhar Pandian Arunachalam, David Brandfon-\nbrener, Zichen Jeff Cui, Venkatesh Pattabiraman, Ilija Ra-']",We recognize that FISH has limitations (see Section IV-K).,simple,"[{'Authors': 'Siddhant Haldar, Jyothish Pari, Anant Rai, Lerrel Pinto', 'Published': '2023-03-02', 'Summary': 'While imitation learning provides us with an efficient toolkit to train\nrobots, learning skills that are robust to environment variations remains a\nsignificant challenge. Current approaches address this challenge by relying\neither on large amounts of demonstrations that span environment variations or\non handcrafted reward functions that require state estimates. Both directions\nare not scalable to fast imitation. In this work, we present Fast Imitation of\nSkills from Humans (FISH), a new imitation learning approach that can learn\nrobust visual skills with less than a minute of human demonstrations. Given a\nweak base-policy trained by offline imitation of demonstrations, FISH computes\nrewards that correspond to the ""match"" between the robot\'s behavior and the\ndemonstrations. These rewards are then used to adaptively update a residual\npolicy that adds on to the base-policy. Across all tasks, FISH requires at most\ntwenty minutes of interactive learning to imitate demonstrations on object\nconfigurations that were not seen in the demonstrations. Importantly, FISH is\nconstructed to be versatile, which allows it to be used across robot\nmorphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g.\nthird-person, eye-in-hand). Our experimental evaluations on 9 different tasks\nshow that FISH achieves an average success rate of 93%, which is around 3.8x\nhigher than prior state-of-the-art methods.', 'Title': 'Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations'}]",TRUE
"What is the definition of deformability in the PHYSOBJECTS dataset, and what other physical concepts are included?","['of real images that was publicly released when constructing\nPHYSOBJECTS. The dataset consists of frames from egocen-\ntric videos in realistic household settings, which makes it par-\nticularly relevant for household robotics. It includes 117,424\nimages, 225,466 object bounding boxes with corresponding\ncategory labels from 277 object categories, and 4,203 object\ninstance IDs. PHYSOBJECTS consists of physical concept\nannotations for a large subset of this image data. 1\nWe construct random training, validation, and test sets\nbased on object instance IDs. We split the dataset per object\ncategory to ensure each object category is represented in\neach set when possible. Our training, validation, and test sets\nconsist of 73.0%, 14.8%, and 12.2% of objects, respectively.\nConcept\nDescription\nMass\nhow heavy an object is\nFragility\nhow easily an object can be broken/damaged\nDeformability\nhow easily an object can change shape without breaking\nMaterial\nwhat an object is primarily made of\nTransparency\nhow much can be seen through an object\nContents\nwhat is inside a container\nCan Contain Liquid\nif a container can be used to easily carry liquid\nIs Sealed\nif a container will not spill if rotated\nDensity (held-out)\nhow much mass per unit of volume of an object\nLiquid Capacity (held-out)\nhow much liquid a container can contain\nTABLE I: Our physical concepts and brief descriptions\nPhysical Concepts. We collect annotations for eight main\nphysical concepts and two additional concepts reserved for\nheld-out evaluation. We select concepts based on prior work\nand what we believe to be useful for robotic manipulation,\nbut do not consider all such concepts. For example, we do not\ninclude friction because this can be challenging to estimate\nwithout interaction, and we do not include volume because\nthis requires geometric reasoning, which we do not focus on.\nOf our main concepts, three are continuous-valued and\napplicable to all objects: mass, fragility, and deformability.\nTwo are also applicable to all objects, but are categorical:\nmaterial and transparency. Transparency could be consid-\nered continuous, but we use discrete values of transparent,\ntranslucent, and opaque. The other three are categorical and\napplicable only to container objects: contents, can contain\nliquid, and is sealed. We define which object categories are\ncontainers, resulting in 956 container object instances.\nOur two held-out concepts are density, which is continuous\nand applicable to all objects, and liquid capacity, which is\ncontinuous and applicable only to containers. We only collect\ntest data for these held-out concepts. We list all concepts and\ntheir brief descriptions in Table I.\nFor categorical concepts, we define a set of labels for each\nconcept. Annotations consist of a label specified for a given\nobject and concept. For the concepts material and contents,\nwhen crowd-sourcing, we allow for open-ended labels if\nnone of the pre-defined labels are applicable.'
 'of real images that was publicly released when constructing\nPHYSOBJECTS. The dataset consists of frames from egocen-\ntric videos in realistic household settings, which makes it par-\nticularly relevant for household robotics. It includes 117,424\nimages, 225,466 object bounding boxes with corresponding\ncategory labels from 277 object categories, and 4,203 object\ninstance IDs. PHYSOBJECTS consists of physical concept\nannotations for a large subset of this image data. 1\nWe construct random training, validation, and test sets\nbased on object instance IDs. We split the dataset per object\ncategory to ensure each object category is represented in\neach set when possible. Our training, validation, and test sets\nconsist of 73.0%, 14.8%, and 12.2% of objects, respectively.\nConcept\nDescription\nMass\nhow heavy an object is\nFragility\nhow easily an object can be broken/damaged\nDeformability\nhow easily an object can change shape without breaking\nMaterial\nwhat an object is primarily made of\nTransparency\nhow much can be seen through an object\nContents\nwhat is inside a container\nCan Contain Liquid\nif a container can be used to easily carry liquid\nIs Sealed\nif a container will not spill if rotated\nDensity (held-out)\nhow much mass per unit of volume of an object\nLiquid Capacity (held-out)\nhow much liquid a container can contain\nTABLE I: Our physical concepts and brief descriptions\nPhysical Concepts. We collect annotations for eight main\nphysical concepts and two additional concepts reserved for\nheld-out evaluation. We select concepts based on prior work\nand what we believe to be useful for robotic manipulation,\nbut do not consider all such concepts. For example, we do not\ninclude friction because this can be challenging to estimate\nwithout interaction, and we do not include volume because\nthis requires geometric reasoning, which we do not focus on.\nOf our main concepts, three are continuous-valued and\napplicable to all objects: mass, fragility, and deformability.\nTwo are also applicable to all objects, but are categorical:\nmaterial and transparency. Transparency could be consid-\nered continuous, but we use discrete values of transparent,\ntranslucent, and opaque. The other three are categorical and\napplicable only to container objects: contents, can contain\nliquid, and is sealed. We define which object categories are\ncontainers, resulting in 956 container object instances.\nOur two held-out concepts are density, which is continuous\nand applicable to all objects, and liquid capacity, which is\ncontinuous and applicable only to containers. We only collect\ntest data for these held-out concepts. We list all concepts and\ntheir brief descriptions in Table I.\nFor categorical concepts, we define a set of labels for each\nconcept. Annotations consist of a label specified for a given\nobject and concept. For the concepts material and contents,\nwhen crowd-sourcing, we allow for open-ended labels if\nnone of the pre-defined labels are applicable.']","Deformability in the PHYSOBJECTS dataset refers to how easily an object can change shape without breaking. Other physical concepts included in the dataset are mass, fragility, material, transparency, contents, can contain liquid, is sealed, density (held-out), and liquid capacity (held-out).",reasoning,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
What is the significance of Soft-dtw in ML and its relation to optimal transport and imitation learning?,"['tion of optimal transport. Advances in neural information\nprocessing systems, 26, 2013.\n[15] Marco Cuturi and Mathieu Blondel. Soft-dtw: a differ-\nentiable loss function for time-series. In International\nconference on machine learning, pages 894–903. PMLR,\n2017.\n[16] Robert Dadashi, L´\neonard Hussenot, Matthieu Geist, and\nOlivier Pietquin. Primal wasserstein imitation learning.\narXiv preprint arXiv:2006.04678, 2020.\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei.\nImagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer\nvision and pattern recognition, pages 248–255. Ieee,\n2009.\n[18] Yue Fan, Shilei Chu, Wei Zhang, Ran Song, and Yibin\nLi. Learn by observation: Imitation learning for drone\npatrolling from videos of a human navigator. In 2020\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 5209–5216. IEEE, 2020.\n[19] Bin Fang, Shidong Jia, Di Guo, Muhua Xu, Shuhuan\nWen, and Fuchun Sun. Survey of imitation learning for\nrobotic manipulation. International Journal of Intelligent\nRobotics and Applications, 3:362–369, 2019.\n[20] Arnaud Fickinger, Samuel Cohen, Stuart Russell, and\nBrandon Amos.\nCross-domain imitation learning via\noptimal transport.\narXiv preprint arXiv:2110.03684,\n2021.\n[21] Pete Florence, Corey Lynch, Andy Zeng, Oscar A\nRamirez, Ayzaan Wahid, Laura Downs, Adrian Wong,\nJohnny Lee, Igor Mordatch, and Jonathan Tompson.\nImplicit behavioral cloning.\nIn Conference on Robot\nLearning, pages 158–168. PMLR, 2022.\n[22] Justin Fu, Katie Luo, and Sergey Levine. Learning robust\nrewards with adversarial inverse reinforcement learning.\narXiv preprint arXiv:1710.11248, 2017.\n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio.\nGenerative adversarial networks.\nCommunications of the ACM, 63(11):139–144, 2020.\n[24] Jean-Bastien\nGrill,\nFlorian\nStrub,\nFlorent\nAltch´\ne,\nCorentin Tallec, Pierre Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mo-\nhammad Gheshlaghi Azar, et al.\nBootstrap your own\nlatent-a new approach to self-supervised learning. Ad-\nvances in neural information processing systems, 33:\n21271–21284, 2020.\n[25] Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and\nLerrel Pinto.\nWatch and match: Supercharging imita-\ntion with regularized optimal transport. arXiv preprint\narXiv:2206.15469, 2022.\n[26] Jonathan Ho and Stefano Ermon. Generative adversar-\nial imitation learning. Advances in neural information\nprocessing systems, 29, 2016.\n[27] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan,\nand Chrisina Jayne.\nImitation learning: A survey of\nlearning methods. ACM Computing Surveys (CSUR), 50\n(2):1–35, 2017.\n[28] Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan\nLuo, Avinash Kumar, Matthias Loskyll, Juan Aparicio\nOjea, Eugen Solowjow, and Sergey Levine.\nResidual\nreinforcement learning for robot control. In 2019 Interna-'
 '[55] Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Ja-\nson D Lee. On the convergence and robustness of training\ngans with regularized optimal transport.\nAdvances in\nNeural Information Processing Systems, 31, 2018.\n[56] Fumihiro Sasaki and Ryota Yamashina.\nBehavioral\ncloning from noisy demonstrations.\nIn International\nConference on Learning Representations, 2021.\n[57] Fabian Schilling, Julien Lecoeur, Fabrizio Schiano, and\nDario Floreano.\nLearning vision-based ﬂight in drone\nswarms by imitation.\nIEEE Robotics and Automation\nLetters, 4(4):4523–4530, 2019.\n[58] Nur Muhammad Mahi Shaﬁullah, Zichen Jeff Cui, Ari-\nuntuya Altanzaya, and Lerrel Pinto. Behavior transform-\ners: Cloning k modes with one stone.\narXiv preprint\narXiv:2206.11251, 2022.\n[59] Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Ab-\nhinav Gupta. Multiple interactions made easy (mime):\nLarge scale demonstrations data for imitation. In Con-\nference on robot learning, pages 906–915. PMLR, 2018.\n[60] David Silver, Guy Lever, Nicolas Heess, Thomas Degris,\nDaan Wierstra, and Martin Riedmiller.\nDeterministic\npolicy gradient algorithms. In International conference\non machine learning, pages 387–395. PMLR, 2014.\n[61] Tom Silver, Kelsey Allen, Josh Tenenbaum, and Leslie\nKaelbling.\nResidual policy learning.\narXiv preprint\narXiv:1812.06298, 2018.\n[62] Richard S Sutton and Andrew G Barto. Reinforcement\nlearning: An introduction. MIT press, 2018.\n[63] Faraz Torabi, Garrett Warnell, and Peter Stone.\nBe-\nhavioral cloning from observation.\narXiv preprint\narXiv:1805.01954, 2018.\n[64] Faraz Torabi, Garrett Warnell, and Peter Stone.\nGen-\nerative adversarial imitation from observation.\narXiv\npreprint arXiv:1807.06158, 2018.\n[65] Faraz Torabi, Garrett Warnell, and Peter Stone. Recent\nadvances in imitation learning from observation. arXiv\npreprint arXiv:1905.13566, 2019.\n[66] Sam Toyer, Rohin Shah, Andrew Critch, and Stuart\nRussell.\nThe magical benchmark for robust imitation.\nAdvances in Neural Information Processing Systems, 33:\n18284–18295, 2020.\n[67] Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu,\nMengyuan Yan, Jos´\nephine Simon, Matthew Bennice,\nChuyuan Fu, Cong Ma, Jiantao Jiao, et al. Jump-start re-\ninforcement learning. arXiv preprint arXiv:2204.02372,\n2022.\n[68] C´\nedric Villani. Optimal transport: old and new, volume\n338. Springer, 2009.\n[69] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng\nGao, Dinghan Shen, Yuan-Fang Wang, William Yang\nWang, and Lei Zhang. Reinforced cross-modal match-\ning and self-supervised imitation learning for vision-\nlanguage navigation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 6629–6638, 2019.\n[70] Huang Xiao, Michael Herman, Joerg Wagner, Sebastian\nZiesche, Jalal Etesami, and Thai Hong Linh.\nWasser-\nstein adversarial imitation learning.\narXiv preprint\narXiv:1906.08113, 2019.\n[71] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra\nMalik.\nMasked visual pre-training for motor control.\narXiv:2203.06173, 2022.']",Soft-dtw is a differentiable loss function for time-series in machine learning. It is related to optimal transport and imitation learning as it can be used in imitation learning algorithms that rely on optimal transport to match observed and generated trajectories.,multi_context,"[{'Authors': 'Siddhant Haldar, Jyothish Pari, Anant Rai, Lerrel Pinto', 'Published': '2023-03-02', 'Summary': 'While imitation learning provides us with an efficient toolkit to train\nrobots, learning skills that are robust to environment variations remains a\nsignificant challenge. Current approaches address this challenge by relying\neither on large amounts of demonstrations that span environment variations or\non handcrafted reward functions that require state estimates. Both directions\nare not scalable to fast imitation. In this work, we present Fast Imitation of\nSkills from Humans (FISH), a new imitation learning approach that can learn\nrobust visual skills with less than a minute of human demonstrations. Given a\nweak base-policy trained by offline imitation of demonstrations, FISH computes\nrewards that correspond to the ""match"" between the robot\'s behavior and the\ndemonstrations. These rewards are then used to adaptively update a residual\npolicy that adds on to the base-policy. Across all tasks, FISH requires at most\ntwenty minutes of interactive learning to imitate demonstrations on object\nconfigurations that were not seen in the demonstrations. Importantly, FISH is\nconstructed to be versatile, which allows it to be used across robot\nmorphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g.\nthird-person, eye-in-hand). Our experimental evaluations on 9 different tasks\nshow that FISH achieves an average success rate of 93%, which is around 3.8x\nhigher than prior state-of-the-art methods.', 'Title': 'Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations'}
 {'Authors': 'Siddhant Haldar, Jyothish Pari, Anant Rai, Lerrel Pinto', 'Published': '2023-03-02', 'Summary': 'While imitation learning provides us with an efficient toolkit to train\nrobots, learning skills that are robust to environment variations remains a\nsignificant challenge. Current approaches address this challenge by relying\neither on large amounts of demonstrations that span environment variations or\non handcrafted reward functions that require state estimates. Both directions\nare not scalable to fast imitation. In this work, we present Fast Imitation of\nSkills from Humans (FISH), a new imitation learning approach that can learn\nrobust visual skills with less than a minute of human demonstrations. Given a\nweak base-policy trained by offline imitation of demonstrations, FISH computes\nrewards that correspond to the ""match"" between the robot\'s behavior and the\ndemonstrations. These rewards are then used to adaptively update a residual\npolicy that adds on to the base-policy. Across all tasks, FISH requires at most\ntwenty minutes of interactive learning to imitate demonstrations on object\nconfigurations that were not seen in the demonstrations. Importantly, FISH is\nconstructed to be versatile, which allows it to be used across robot\nmorphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g.\nthird-person, eye-in-hand). Our experimental evaluations on 9 different tasks\nshow that FISH achieves an average success rate of 93%, which is around 3.8x\nhigher than prior state-of-the-art methods.', 'Title': 'Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations'}]",TRUE
"What does ""Contents"" mean in the PHYSOBJECTS dataset, and what objects does it apply to?","['of real images that was publicly released when constructing\nPHYSOBJECTS. The dataset consists of frames from egocen-\ntric videos in realistic household settings, which makes it par-\nticularly relevant for household robotics. It includes 117,424\nimages, 225,466 object bounding boxes with corresponding\ncategory labels from 277 object categories, and 4,203 object\ninstance IDs. PHYSOBJECTS consists of physical concept\nannotations for a large subset of this image data. 1\nWe construct random training, validation, and test sets\nbased on object instance IDs. We split the dataset per object\ncategory to ensure each object category is represented in\neach set when possible. Our training, validation, and test sets\nconsist of 73.0%, 14.8%, and 12.2% of objects, respectively.\nConcept\nDescription\nMass\nhow heavy an object is\nFragility\nhow easily an object can be broken/damaged\nDeformability\nhow easily an object can change shape without breaking\nMaterial\nwhat an object is primarily made of\nTransparency\nhow much can be seen through an object\nContents\nwhat is inside a container\nCan Contain Liquid\nif a container can be used to easily carry liquid\nIs Sealed\nif a container will not spill if rotated\nDensity (held-out)\nhow much mass per unit of volume of an object\nLiquid Capacity (held-out)\nhow much liquid a container can contain\nTABLE I: Our physical concepts and brief descriptions\nPhysical Concepts. We collect annotations for eight main\nphysical concepts and two additional concepts reserved for\nheld-out evaluation. We select concepts based on prior work\nand what we believe to be useful for robotic manipulation,\nbut do not consider all such concepts. For example, we do not\ninclude friction because this can be challenging to estimate\nwithout interaction, and we do not include volume because\nthis requires geometric reasoning, which we do not focus on.\nOf our main concepts, three are continuous-valued and\napplicable to all objects: mass, fragility, and deformability.\nTwo are also applicable to all objects, but are categorical:\nmaterial and transparency. Transparency could be consid-\nered continuous, but we use discrete values of transparent,\ntranslucent, and opaque. The other three are categorical and\napplicable only to container objects: contents, can contain\nliquid, and is sealed. We define which object categories are\ncontainers, resulting in 956 container object instances.\nOur two held-out concepts are density, which is continuous\nand applicable to all objects, and liquid capacity, which is\ncontinuous and applicable only to containers. We only collect\ntest data for these held-out concepts. We list all concepts and\ntheir brief descriptions in Table I.\nFor categorical concepts, we define a set of labels for each\nconcept. Annotations consist of a label specified for a given\nobject and concept. For the concepts material and contents,\nwhen crowd-sourcing, we allow for open-ended labels if\nnone of the pre-defined labels are applicable.'
 'of real images that was publicly released when constructing\nPHYSOBJECTS. The dataset consists of frames from egocen-\ntric videos in realistic household settings, which makes it par-\nticularly relevant for household robotics. It includes 117,424\nimages, 225,466 object bounding boxes with corresponding\ncategory labels from 277 object categories, and 4,203 object\ninstance IDs. PHYSOBJECTS consists of physical concept\nannotations for a large subset of this image data. 1\nWe construct random training, validation, and test sets\nbased on object instance IDs. We split the dataset per object\ncategory to ensure each object category is represented in\neach set when possible. Our training, validation, and test sets\nconsist of 73.0%, 14.8%, and 12.2% of objects, respectively.\nConcept\nDescription\nMass\nhow heavy an object is\nFragility\nhow easily an object can be broken/damaged\nDeformability\nhow easily an object can change shape without breaking\nMaterial\nwhat an object is primarily made of\nTransparency\nhow much can be seen through an object\nContents\nwhat is inside a container\nCan Contain Liquid\nif a container can be used to easily carry liquid\nIs Sealed\nif a container will not spill if rotated\nDensity (held-out)\nhow much mass per unit of volume of an object\nLiquid Capacity (held-out)\nhow much liquid a container can contain\nTABLE I: Our physical concepts and brief descriptions\nPhysical Concepts. We collect annotations for eight main\nphysical concepts and two additional concepts reserved for\nheld-out evaluation. We select concepts based on prior work\nand what we believe to be useful for robotic manipulation,\nbut do not consider all such concepts. For example, we do not\ninclude friction because this can be challenging to estimate\nwithout interaction, and we do not include volume because\nthis requires geometric reasoning, which we do not focus on.\nOf our main concepts, three are continuous-valued and\napplicable to all objects: mass, fragility, and deformability.\nTwo are also applicable to all objects, but are categorical:\nmaterial and transparency. Transparency could be consid-\nered continuous, but we use discrete values of transparent,\ntranslucent, and opaque. The other three are categorical and\napplicable only to container objects: contents, can contain\nliquid, and is sealed. We define which object categories are\ncontainers, resulting in 956 container object instances.\nOur two held-out concepts are density, which is continuous\nand applicable to all objects, and liquid capacity, which is\ncontinuous and applicable only to containers. We only collect\ntest data for these held-out concepts. We list all concepts and\ntheir brief descriptions in Table I.\nFor categorical concepts, we define a set of labels for each\nconcept. Annotations consist of a label specified for a given\nobject and concept. For the concepts material and contents,\nwhen crowd-sourcing, we allow for open-ended labels if\nnone of the pre-defined labels are applicable.']",Contents in the PHYSOBJECTS dataset refers to what is inside a container. It applies to container objects in the dataset.,multi_context,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
How can language models enhance robots' perception and processing abilities?,"['Research on robots’ autonomous perception capabilities is\ncrucial for improving our quality of life in the future.\n7. Conclusions\nIn this survey, we summarized the methods and tech-\nnologies currently used for large models in robots. First, we\nreviewed some basic concepts of large language models and\ncommon large models. We explain what improvements will\nbe brought to robots by using large models as brains. We\nalso introduce the representative LLM-based robot models\nproposed in recent years, such as LM-Nav [117], PaLM-\nSayCan [1], PaLM-E [34], etc. Next, we divide the robot into\nfour modules: perception, decision-making, control, and in-\nteraction. For each module, we discuss the relevant technolo-\ngies and their functions, including the perception module’s\nability to process the robot’s input from the surroundings;\nthe decision-making module’s capacity to understand human\ninstructions and plan; the control module’s role in processing\noutput actions; the interaction module’s ability to interact\nwith the environment. We also explore the potential applica-\ntion scenarios of current robots based on LLMs and discuss\nthe challenges, such as training, safety, shape, deployment,\nand long-term task performance. Finally, we consider the\nsocial and ethical implications of post-intelligent robots and\ntheir potential impact on human society.\nAs LLMs continue to evolve, robots may become in-\ncreasingly intelligent and capable of processing instructions\nand tasks more efficiently. With advancements in hardware,\nrobots may eventually become reliable assistants for hu-\nmans, as depicted in science fiction movies. However, we\nmust also be mindful of their potential impact on society and\naddress any concerns proactively. Embodied intelligence is\na new paradigm for the development of intelligent science\nand is of great significance in leading the development of\nthe future. LLM-based robotics represent a potential path\nto embodied intelligence. We hope this survey can provide\nsome inspiration to the community and facilitate research in\nrelated fields.\nAcknowledgment\nThis research was supported in part by the National\nNatural Science Foundation of China (Nos. 62002136 and\n62272196), the Natural Science Foundation of Guangdong\nProvince (No. 2022A1515011861), Engineering Research\nCenter of Trustworthy AI, Ministry of Education (Jinan Uni-\nversity), and Guangdong Key Laboratory of Data Security\nand Privacy Preserving.\nReferences\n[1] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David,\nB., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al., 2022.\nDo as i can, not as i say: Grounding language in robotic affordances,\nin: The Conference on Robot Learning, pp. 287–318.\n[2] Alam, A., 2022. Social robots in education for long-term human-\nrobot interaction: socially supportive behaviour of robotic tutor for\ncreating robo-tangible learning environment in a guided discovery\nlearning interaction. ECS Transactions 107, 12389.']","Language models can enhance robots' perception and processing abilities by improving their ability to understand and process human instructions, as well as their capacity to interact with the environment.",reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
How are question prompts used in the evaluation of PG-InstructBLIP and what impact do they have on the performance of the model?,"['to PG-InstructBLIP on VQAv2 [42] and OK-VQA [43].\nThese results suggest that existing systems using VLMs can\nbenefit from PHYSOBJECTS for physical reasoning, without\nsacrificing other reasoning abilities.\nWe perform VQA evaluation using the LAVIS library,\nusing their configurations for evaluation of BLIP-2. Although\nPG-InstructBLIP is fine-tuned without Q-Former text condi-\ntioning, we found that Q-Former text conditioning during\nVQA evaluation improved performance, so we report these\nresults. We believe this is because InstructBLIP was instruc-\ntion tuned with this text conditioning. We also experimented\nwith VQA evaluation on PG-InstructBLIP fine-tuned with\nQ-Former text conditioning, but found this to have worse\nresults, possibly due to overfitting on our limited variety of\nquestion prompts. We believe these issues can be mitigated\nby co-training on PHYSOBJECTS in combination with other\nvision and language datasets, which we leave for future work.\nMotivated by these VQA results, for our planning evalua-\ntions we also evaluate PG-InstructBLIP using Q-Former text\nconditioning, to avoid possible degradation when answering\nquestions that do not pertain concepts in PHYSOBJECTS.\nWe verified that evaluating PG-InstructBLIP using Q-Former\ntext conditioning did not significantly affect test accuracy on\nPHYSOBJECTS.\nIncluding Object Category Labels in Question Prompts.\nWe generally report evaluation results without ground-truth\nobject category labels in the question prompt. In Table XVI,\nwe compare including object category labels or not, and find\nthat all models are not extremely sensitive to this.\nIncluding Concept Definitions in Question Prompts.\nWhile we did not spend extensive effort designing the\nquestion prompts for each concept (shown in Table XIII), we\naimed for them to be concise while still eliciting the desired\nconcept. As seen in Table XVIII, the base InstructBLIP\nmodel achieves above chance performance on all concepts,\nsuggesting that these prompts do elicit the desired concept\nto some extent. However, these prompts do not contain\nour definitions for each concept provided to annotators, as\ndescribed in Appendix A. We analyze whether including\nconcept definitions in the question prompt would improve\nbase VLM performance in Table XVIII, which contains our\noriginal crowd-sourced test accuracy results, with additional\nevaluation of the base InstructBLIP model using modified\nprompts that contain concept definitions, which we pro-\nvide in Table XVII. We find that while including concept\ndefinitions improves performance for some concepts (mass,\ndeformability, contents, can contain liquid), this still does\nnot match PG-InstructBLIP on these concepts, and overall\nperformance in fact decreases compared to the original\nprompts. We believe this could be because InstructBLIP\ndoes not have strong enough language understanding to\nproperly incorporate the concept definitions when providing\nresponses. For this reason, and for simplicity, we use prompts']","We generally report evaluation results without ground-truth object category labels in the question prompt. In Table XVI, we compare including object category labels or not, and find that all models are not extremely sensitive to this. While we did not spend extensive effort designing the question prompts for each concept, we aimed for them to be concise while still eliciting the desired concept. The base InstructBLIP model achieves above chance performance on all concepts, suggesting that these prompts do elicit the desired concept to some extent. However, these prompts do not contain our definitions for each concept provided to annotators. We analyze whether including concept definitions in the question prompt would improve base VLM performance, but find that it does not match PG-InstructBLIP on these concepts, and overall performance actually decreases compared to the original prompts. We believe this could be because InstructBLIP does not have strong enough language understanding to properly incorporate the concept definitions when providing responses. For this reason, and for simplicity, we use prompts without concept definitions in the evaluation of PG-InstructBLIP.",simple,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
What is the focus of the monolingual track in ML-SUPERB?,"['ent SSL models on various speech-related tasks, universally.\nWhile SUPERB covers a wide range of speech tasks, it was\ndesigned primarily for English speech. However, there has been\ngrowing interest in applying SSL models to multilingual sce-\nnarios, such as training multilingual SSL models [6–8] or using\n∗\nEqual contribution, sorted in alphabetical order.\nSSL models in a cross-lingual manner [9–12]. To support fu-\nture research in these areas, we propose a new benchmark called\nmultilingual SUPERB (ML-SUPERB).\nML-SUPERB is designed to cover a wide range of lan-\nguages, including both high-resource languages like English\nand endangered languages such as Totonac. The benchmark pri-\nmarily focuses on evaluating SSL models for automatic speech\nrecognition (ASR) and language identification (LID). To ac-\ncommodate different use cases for SSL models, ML-SUPERB\nincludes two tracks with four different tasks: the monolingual\ntrack (monolingual ASR), and the multilingual track (multilin-\ngual ASR, LID, joint multilingual ASR/LID). Similar to SU-\nPERB, ML-SUPERB employs frozen SSL models as feature\nextractors and a lightweight downstream model that can be fine-\ntuned for different tracks to achieve high training efficiency.\nSeveral existing benchmarks also include multilingual SSL\nmodels [13–15]. Lebenchmark primarily evaluates speech tasks\nin French [13]; IndicSUPERB focuses mostly on Indian lan-\nguages [14]. XTREME-S focuses on multilingual speech rep-\nresentation benchmarks, including ASR, speech translation,\nspeech classification, and speech retrieval [15].\nThere are\nthree main differences between XTREME-S and ML-SUPERB.\nFirstly, ML-SUPERB covers a wider range of languages, with\n143 languages compared to XTREME-S’s 102. Secondly, ML-\nSUPERB focuses on ASR and LID, while XTREME-S covers\nfour different tasks. However, ML-SUPERB expands the tasks\nby evaluating them in four common multilingual research sce-\nnarios, while XTREME-S considers multilingual training only.\nFinally, ML-SUPERB is designed for efficiency, using smaller\nbenchmark datasets and downstream models, and does not in-\nclude fine-tuning. This lightweight setup allows us to conduct\nexperiments for a dozen of popular speech SSL models, trained\nwith various sizes and pre-training sets, and compare their per-\nformances across the proposed tracks. We expect ML-SUPERB\nwould be a valuable complement to existing benchmarks.\n2. Benchmark Details\n2.1. Data Collection\nML-SUPERB gathers data from a wide range of multilingual\nspeech corpora, including Multilingual Librispeech [16], Com-\nmonvoice [17], Voxforge [18], Voxpopuli [19], Googlei18n\nopen-source project [20–22], Nordic Language Technology\nASR corpora [23], Fleurs [24], NCHLT Speech [25], Spoken\nWikipedia corpus [26], Mexican endangered languages [10, 27,\n28], M-AILab multilingual corpora [29], Living Audio dataset\n[30], ALFFA corpus [31]. All corpora are with either Creative\nCommons, MIT, GNU, or Free-BSD licenses, which are avail-']",The focus of the monolingual track in ML-SUPERB is monolingual automatic speech recognition (ASR).,simple,"[{'Authors': 'Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe', 'Published': '2023-08-11', 'Summary': 'Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard\nto benchmark the performance of Self-Supervised Learning (SSL) models on\nvarious speech processing tasks. However, SUPERB largely considers English\nspeech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),\ncovering 143 languages (ranging from high-resource to endangered), and\nconsidering both automatic speech recognition and language identification.\nFollowing the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and\nemploys a simple framework for multilingual tasks by learning a shallow\ndownstream model. Similar to the SUPERB benchmark, we find speech SSL models\ncan significantly improve performance compared to FBANK features. Furthermore,\nwe find that multilingual models do not always perform better than their\nmonolingual counterparts. We will release ML-SUPERB as a challenge with\norganized datasets and reproducible training scripts for future multilingual\nrepresentation research.', 'Title': 'ML-SUPERB: Multilingual Speech Universal PERformance Benchmark'}]",TRUE
How do LLMs in-context learn differently to enable a robot to walk?,"['hypothesize that a description prompt provides a context\nfor LLMs to interpret the observations and actions properly.\nWhile we provide a prompt example for robot walking, the\nprompt design for robot motions is still under-explored.\nB. LLMs In-Context Learn Differently\nOur experiments demonstrate that LLMs in-context learn\nto prompt a robot to walk. Initially, we hypothesized that\nLLMs might learn a robot walking behavior in a manner akin\nto behavior cloning [48]. However, as shown in Fig. 4, the\njoint trajectories generated by the LLM policy are sufficiently\ndifferent from those generated by an RL policy. Moreover,\nthe LLM policy shows a more regular pattern which is not\npresent in the RL policy. If we pay attention to the left calf\nTime\nFig. 8: Robot Walking Visualization. Top: A1 robot is\nprompted to walk on uneven terrain in MuJoCo, where the\nLLM policy can make it recover from terrain disturbance.\nBottom: ANYmal robot is prompted to walk on flat ground\nin Isaac Gym using the same approach.\njoint trajectory, the pattern coincides with the biomechanics\nstudy of animal walking [13]. Thus, we believe that LLMs\nin-context learn differently to enable a robot to walk.\nC. Limitations\nWhile this work takes us closer towards utilizing LLMs\nfor robot walking control, there are some limitations in the\ncurrent framework. First, the current prompt design is fragile.\nMinor alterations in the prompt can dramatically affect the\nwalking performance, as described in our experiments. In\ngeneral, we still lack a good understanding of how to design\na reliable prompt for robot walking. Secondly, as we design\nand test the prompt based on a specific initialization policy,\nour prompt design inevitably becomes biased towards this\npolicy. Although we have tested our framework with several\ndifferent RL initialization policies, it is possible that some\ninitialization policies do not work with our prompt.\nAnother major limitation is that we are only able to carry\nout simulation experiments instead of hardware experiments.\nOne reason is the low inference speed of GPT-4. Our pipeline\nrequires LLMs to be queried at 10 Hz, which is much faster\nthan the actual inference speed through OpenAI API. Thus,\nwe have to pause the simulation to wait for the output of\nGPT-4. Furthermore, due to the limited token size, we have\nto choose a low-frequency policy, i.e., 10 Hz, to maximize\nthe time horizon of the context. As a side note for future\nresearch, this work is expensive and roughly costed $2, 000\nUS dollars for querying OpenAI API to test the prompt.\nV. CONCLUSIONS\nIn this paper, we presented an approach for prompting a\nrobot to walk. We use LLMs with text prompts, consisting of\na description prompt and an observation and action prompt\ncollected from the physical environment, without any task-\nspecific fine-tuning. Our experiments demonstrate that LLMs\ncan serve as low-level feedback controllers for dynamic\nmotion control even in high-dimensional robotic systems.']",LLMs in-context learn differently to enable a robot to walk by generating joint trajectories that are sufficiently different from those generated by an RL policy. The LLM policy shows a more regular pattern that coincides with the biomechanics study of animal walking.,simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What is the application of CodeBERT in automated program repair of Java simple bugs?,"['Engineering Conference and Symposium on the Foundations of Software Engineering. 1571–1575.\n[44] Tsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, and Shing-Chi Cheung. 2023. Finding Failure-Inducing\nTest Cases with ChatGPT. arXiv preprint arXiv:2304.11686 (2023).\n[45] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of\nthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, 4582–4597.\nhttps://doi.org/10.18653/v1/2021.acl-long.353\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nIs ChatGPT the Ultimate Programming Assistant - How far is it?\n21\n[46] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling,\nFelix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624\n(2022), 1092–1097.\n[47] Yi Li, Shaohua Wang, and Tien N Nguyen. 2022. Dear: A novel deep learning-based approach for automated program\nrepair. In Proceedings of the 44th International Conference on Software Engineering. 511–523.\n[48] Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin. 2020. A self-attentional neural architecture for code\ncompletion with multi-task learning. In Proceedings of the 28th International Conference on Program Comprehension.\n37–47.\n[49] Stephen MacNeil, Andrew Tran, Arto Hellas, Joanne Kim, Sami Sarsa, Paul Denny, Seth Bernstein, and Juho Leinonen.\n2023. Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development\nE-Book. In Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1. 931–937.\n[50] Riya Madaan. 2023. ChatGPT GPT-4 Response Quality And Performance Drop Issues To Be Investigated.\nhttps://\npiunikaweb.com/2023/07/14/chatgpt-gpt-4-response-quality-performance-drop-issues/ Accessed: 11 August 2023.\n[51] Henry B Mann and Donald R. Whitney. 1947. On a Test of Whether One of Two Random Variables Is Stochastically\nLarger than the Other. The Annals of Mathematical Statistics 18, 1 (1947), 50–60.\nhttps://doi.org/10.1214/aoms/\n1177730491\n[52] Ehsan Mashhadi and Hadi Hemmati. 2021. Applying codebert for automated program repair of java simple bugs. In\n2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 505–509.\n[53] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana\nHeintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A\nsurvey. Comput. Surveys (2021).\n[54] Manish Motwani, Mauricio Soto, Yuriy Brun, Rene Just, and Claire Le Goues. 2020. Quality of automated program\nrepair on real-world defects. IEEE Transactions on Software Engineering 48, 2 (2020), 637–661.']",The application of CodeBERT in automated program repair of Java simple bugs is mentioned in [52].,simple,"[{'Authors': 'Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, Tegawendé F. Bissyandé', 'Published': '2023-08-31', 'Summary': ""Recently, the ChatGPT LLM has received great attention: it can be used as a\nbot for discussing source code, prompting it to suggest changes, provide\ndescriptions or even generate code. Typical demonstrations generally focus on\nexisting benchmarks, which may have been used in model training (i.e., data\nleakage). To assess the feasibility of using an LLM as a useful assistant bot\nfor programmers, we must assess its realistic capabilities on unseen problems\nas well as its capabilities on various tasks. In this paper, we present an\nempirical study of ChatGPT's potential as a fully automated programming\nassistant, focusing on the tasks of code generation, program repair, and code\nsummariziation. The study investigates ChatGPT's performance on common\nprogramming problems and compares it with state-of-the-art approaches on two\nbenchmarks. Among several findings, our study shows that ChatGPT is effective\nin dealing with common programming problems. However, our experiments also\nreveal limitations in terms of its attention span: detailed descriptions will\nconstrain the focus of ChatGPT and prevent it from leveraging its vast\nknowledge to solve the actual problem. Surprisingly, we have identified the\nability of ChatGPT to reason the original intention of the code. We expect\nfuture work to build on this insight for dealing with the open question of the\noracle problem. Our findings contribute interesting insights to the development\nof LLMs for programming assistance, notably by demonstrating the importance of\nprompt engineering, and providing a better understanding of ChatGPT's practical\napplications for software engineering."", 'Title': 'Is ChatGPT the Ultimate Programming Assistant -- How far is it?'}]",TRUE
What are some limitations of traditional vision techniques in reasoning about physical objects?,"['How can we use vision to reason about physical object\nconcepts? Prior work has studied this problem using more\ntraditional vision techniques, such as self-supervised learning\non object interaction data. However, object interaction data\n1Stanford University, 2Google DeepMind, 3Princeton University. Con-\ntact: jenseng@stanford.edu.\ncan be challenging to collect when scaling up beyond a\nsmall set of objects in well-defined settings. While precise\nestimation of physical properties may sometimes be impos-\nsible without interaction data, humans can use their visual\nperception to reason at a high level about physical concepts\nwithout object interactions. For example, humans can reason\nthat a glass cup is more fragile than a plastic bottle, and\nthat it would be easier to use a bowl to hold water than a\nshallow plate. This reasoning is often based on prior semantic\nknowledge of visually similar objects, and can be done from\nstatic visual appearance alone.\nSimilarly, VLMs pre-trained using large-scale data have\ndemonstrated broad visual reasoning abilities and generaliza-\ntion [8]–[13], and thus have the potential to physically reason\nabout objects in a similar fashion as humans. Therefore, we\npropose to leverage VLMs as a scalable way of providing\nthe kind of high-level physical reasoning that humans use to\ninteract with the world, which can benefit a robotic planner,\nwithout the need for interaction data. The general and flexible\nnature of VLMs also removes the need to use separate task-\nspecific vision models for physical reasoning. VLMs have\nalready been commonly incorporated into robotic planning\nsystems [3]–[7], [13], making them a natural solution for\nendowing physical reasoning into robotic planning.\nHowever, while modern VLMs have improved signifi-\ncantly on tasks such as visual question answering (VQA),\nand there has been evidence of their potential for object-\ncentric physical reasoning [14], we show in this work that\ntheir out-of-the-box performance for this still leaves much\nto be desired. Although VLMs have been trained on broad\ninternet-scale data, this data does not contain many ex-\namples of object-centric physical reasoning. This motivates\nincorporating a greater variety and amount of such data\nwhen training VLMs. Unfortunately, prior visual datasets\nfor physical reasoning are not well-suited for understanding\ncommon real-world objects, which is desirable for robotics.\nTo address this, we propose PHYSOBJECTS, an object-\ncentric dataset with human physical concept annotations of\ncommon household objects. Our annotations include categor-\nical labels (e.g., object X is made of plastic) and preference\npairs (e.g., object X is heavier than object Y).\nOur main contributions are PHYSOBJECTS, a dataset of\n39.6K crowd-sourced and 417K automated physical concept\nannotations of real household objects, and demonstrating that\nusing it to fine-tune a VLM significantly improves physical\nreasoning. We show that our physically grounded VLM']","Traditional vision techniques for reasoning about physical objects have limitations in terms of collecting object interaction data on a large scale. It can be challenging to collect such data beyond a small set of objects in well-defined settings. Additionally, precise estimation of physical properties may sometimes be impossible without interaction data.",simple,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
"How do reasoning abilities impact the problem-solving and decision-making of LLMs, and what is the role of planning in this process?","['number of memories grows. As the robot’s mem-\nory burden increases over time, it must be able to\neffectively manage and retrieve memories to avoid\ncatastrophic forgetfulness [68].\nReasoning. Reasoning serves as a foundational element\nin human cognition, playing a crucial role in problem-\nsolving, decision-making, and the analytical examination of\ninformation [135, 136]. Reasoning plays a crucial role in en-\nabling LLMs to solve complex tasks. Reasoning capabilities\nallow LLMs to break down problems into smaller, manage-\nable steps and solve them starting from the current status\nand known conditions. There is ongoing debate about how\nLLMs acquire their reasoning abilities, with some arguing\nthat it is a result of pre-training or fine-tuning [54], while\nothers believe that it emerges only at a certain scale [137].\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 9 of 19\nLarge Language Models for Robotics: A Survey\nResearch has shown that Chain-of-Thought (CoT) [136] can\nhelp LLMs reveal their reasoning capabilities, and some\nstudies suggest that inference abilities may stem from the\nlocal static structure of the training data.\nPlanning. Humans plan when faced with complex chal-\nlenges. Planning can help people organize their thoughts,\nset goals, and decide what they should do in the current\nsituation [45, 130]. In this case, they can gradually approach\ntheir goals. The core of planning is reasoning. The agent can\nuse reasoning capabilities to deconstruct the received high-\nlevel abstract instructions into executable subtasks and make\nreasonable plans for each subtask [26, 112]. For example,\nLM-Nav uses ChatGpt to process received natural language\ninstructions [117]. PaLM-E directly implements end-to-end\nprocessing, converting the received multi-modal input into\nmulti-modal sentences for LLM processing [34]. Agents\nmay also be able to reasonably update task planning based on\nthe current situation through multiple rounds of dialogue and\nself-questioning and answering in the future. Many studies\nhave proposed methods of dividing the execution tasks into\nmany executable small tasks during the planning process.\nFor example, directly break down the execution task into\nmany small tasks and execute them sequentially [103, 145].\nCoT only processes one sub-task at a time and can adaptively\ncomplete the task, which has a certain degree of flexibility\n[69, 138]. There are also some vertical planning methods\nthat divide tasks into tree diagrams [49, 148].\n3.3. Control\nHere, we argue that the control module is the key compo-\nnent responsible for regulating robotic actions. This module\nplays a crucial role in ensuring that the robot’s actions are\nexecuted accurately and successfully, with a focus on the\nhardware aspects of action execution.\n3.3.1. How to learn language-conditioned behavior\nMuch of the previous work has focused on enabling\nrobots and other agents to comprehend and execute nat-\nural language instructions [19, 35, 81]. There are various']","Reasoning abilities play a crucial role in problem-solving, decision-making, and the analytical examination of information for LLMs. It allows LLMs to break down problems into smaller, manageable steps and solve them based on the current status and known conditions. Planning, which is closely related to reasoning, helps LLMs organize their thoughts, set goals, and make reasonable plans for each subtask. Reasoning and planning together enable LLMs to solve complex tasks.",reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are the evaluation results of using the smaller Flan-T5 XL as the base LLM in the InstructBLIP model compared to the Flan-T5 XXL used in other experiments?,"['does not have strong enough language understanding to\nproperly incorporate the concept definitions when providing\nresponses. For this reason, and for simplicity, we use prompts\nwithout concept definitions in the rest of our experiments.\nUsing a Smaller VLM. To analyze the effect of VLM size\non physical reasoning, in Table XIX we provide evaluation\nresults using the InstructBLIP version with the smaller Flan-\nT5 XL as its base LLM, compared to the Flan-T5 XXL\nversion used in all other experiments. We find that while\nthe smaller Flan-T5 XL version generally has worse base\nInstructBLIP\nSingle Concept FT (ours)\nPG-InstructBLIP (ours)\nCategory Labels\nYes\nNo\nYes\nNo\nYes\nNo\nMass\n60.0\n62.2\n84.4\n80.0\n80.0\n80.0\nFragility\n75.7\n78.4\n91.2\n91.2\n97.3\n94.6\nDeformability\n69.8\n67.4\n88.4\n95.3\n90.7\n93.0\nMaterial\n73.3\n67.1\n86.8\n83.7\n85.7\n84.6\nTransparency\n84.5\n85.8\n89.1\n89.4\n89.8\n90.1\nContents\n34.2\n35.1\n80.7\n81.6\n82.5\n83.3\nCan Contain Liquid\n57.8\n59.4\n84.4\n84.4\n82.8\n87.5\nIs Sealed\n71.0\n74.2\n80.6\n80.6\n87.1\n87.1\nAverage\n65.8\n66.2\n85.7\n85.8\n87.0\n87.5\nTABLE XVI: Test accuracy for main concepts on crowd-sourced PHYSOBJECTS, with and without object category labels\nConcept\nQuestion Prompt\nMass\nThe heaviness of an object refers to its mass. It includes the contents of the\nobject if it has something inside it. Is this object heavy?\nFragility\nFragility refers to how easily an object can be broken or damaged. Is this\nobject fragile?\nDeformability\nDeformability refers to how easily an object can change shape without\nbreaking. Is this object deformable?\nMaterial\nThe material of an object refers to what material makes up the largest portion\nof the object that is visible. It does not refer to the contents of a container.\nWhat material is this object made of?\nTransparency\nTransparency refers to how much can be seen through an object. A transparent\nobject can be clearly seen through, almost as if it was not there. A translucent\nobject can be seen through with some details, but not as clearly as if it was\ntransparent. An opaque object cannot be seen through at all. The transparency\nof an object does not refer to the transparency of its contents if it has anything\ninside it. Is this object transparent, translucent, or opaque? If different portions\nof the object have different levels of transparency, respond with the level that\napplies to the largest visible portion of the object.\nContents\nWhat is inside this container? Only respond with contents that are clearly\nvisible and identifiable.\nCan Contain Liquid\nA container can contain liquid if it can be used to transport a liquid across a\nroom without a person needing to be particularly careful about not spilling it.\nCan this container contain liquid?\nIs Sealed\nA container is sealed if it can be rotated by any amount in any direction\nwithout spilling its contents if it has anything inside. Is this container sealed?\nTABLE XVII: Question prompts with definitions for each main concept, without object category labels\nInstructBLIP']","We find that while the smaller Flan-T5 XL version generally has worse base InstructBLIP performance compared to the Flan-T5 XXL version used in other experiments, it still achieves competitive results across various concepts.",reasoning,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
"How does Q-Former text conditioning affect PG-InstructBLIP performance in VQA evaluation, considering PHYSOBJECTS and the inclusion of object category labels and concept definitions in question prompts?","['to PG-InstructBLIP on VQAv2 [42] and OK-VQA [43].\nThese results suggest that existing systems using VLMs can\nbenefit from PHYSOBJECTS for physical reasoning, without\nsacrificing other reasoning abilities.\nWe perform VQA evaluation using the LAVIS library,\nusing their configurations for evaluation of BLIP-2. Although\nPG-InstructBLIP is fine-tuned without Q-Former text condi-\ntioning, we found that Q-Former text conditioning during\nVQA evaluation improved performance, so we report these\nresults. We believe this is because InstructBLIP was instruc-\ntion tuned with this text conditioning. We also experimented\nwith VQA evaluation on PG-InstructBLIP fine-tuned with\nQ-Former text conditioning, but found this to have worse\nresults, possibly due to overfitting on our limited variety of\nquestion prompts. We believe these issues can be mitigated\nby co-training on PHYSOBJECTS in combination with other\nvision and language datasets, which we leave for future work.\nMotivated by these VQA results, for our planning evalua-\ntions we also evaluate PG-InstructBLIP using Q-Former text\nconditioning, to avoid possible degradation when answering\nquestions that do not pertain concepts in PHYSOBJECTS.\nWe verified that evaluating PG-InstructBLIP using Q-Former\ntext conditioning did not significantly affect test accuracy on\nPHYSOBJECTS.\nIncluding Object Category Labels in Question Prompts.\nWe generally report evaluation results without ground-truth\nobject category labels in the question prompt. In Table XVI,\nwe compare including object category labels or not, and find\nthat all models are not extremely sensitive to this.\nIncluding Concept Definitions in Question Prompts.\nWhile we did not spend extensive effort designing the\nquestion prompts for each concept (shown in Table XIII), we\naimed for them to be concise while still eliciting the desired\nconcept. As seen in Table XVIII, the base InstructBLIP\nmodel achieves above chance performance on all concepts,\nsuggesting that these prompts do elicit the desired concept\nto some extent. However, these prompts do not contain\nour definitions for each concept provided to annotators, as\ndescribed in Appendix A. We analyze whether including\nconcept definitions in the question prompt would improve\nbase VLM performance in Table XVIII, which contains our\noriginal crowd-sourced test accuracy results, with additional\nevaluation of the base InstructBLIP model using modified\nprompts that contain concept definitions, which we pro-\nvide in Table XVII. We find that while including concept\ndefinitions improves performance for some concepts (mass,\ndeformability, contents, can contain liquid), this still does\nnot match PG-InstructBLIP on these concepts, and overall\nperformance in fact decreases compared to the original\nprompts. We believe this could be because InstructBLIP\ndoes not have strong enough language understanding to\nproperly incorporate the concept definitions when providing\nresponses. For this reason, and for simplicity, we use prompts'
 'to PG-InstructBLIP on VQAv2 [42] and OK-VQA [43].\nThese results suggest that existing systems using VLMs can\nbenefit from PHYSOBJECTS for physical reasoning, without\nsacrificing other reasoning abilities.\nWe perform VQA evaluation using the LAVIS library,\nusing their configurations for evaluation of BLIP-2. Although\nPG-InstructBLIP is fine-tuned without Q-Former text condi-\ntioning, we found that Q-Former text conditioning during\nVQA evaluation improved performance, so we report these\nresults. We believe this is because InstructBLIP was instruc-\ntion tuned with this text conditioning. We also experimented\nwith VQA evaluation on PG-InstructBLIP fine-tuned with\nQ-Former text conditioning, but found this to have worse\nresults, possibly due to overfitting on our limited variety of\nquestion prompts. We believe these issues can be mitigated\nby co-training on PHYSOBJECTS in combination with other\nvision and language datasets, which we leave for future work.\nMotivated by these VQA results, for our planning evalua-\ntions we also evaluate PG-InstructBLIP using Q-Former text\nconditioning, to avoid possible degradation when answering\nquestions that do not pertain concepts in PHYSOBJECTS.\nWe verified that evaluating PG-InstructBLIP using Q-Former\ntext conditioning did not significantly affect test accuracy on\nPHYSOBJECTS.\nIncluding Object Category Labels in Question Prompts.\nWe generally report evaluation results without ground-truth\nobject category labels in the question prompt. In Table XVI,\nwe compare including object category labels or not, and find\nthat all models are not extremely sensitive to this.\nIncluding Concept Definitions in Question Prompts.\nWhile we did not spend extensive effort designing the\nquestion prompts for each concept (shown in Table XIII), we\naimed for them to be concise while still eliciting the desired\nconcept. As seen in Table XVIII, the base InstructBLIP\nmodel achieves above chance performance on all concepts,\nsuggesting that these prompts do elicit the desired concept\nto some extent. However, these prompts do not contain\nour definitions for each concept provided to annotators, as\ndescribed in Appendix A. We analyze whether including\nconcept definitions in the question prompt would improve\nbase VLM performance in Table XVIII, which contains our\noriginal crowd-sourced test accuracy results, with additional\nevaluation of the base InstructBLIP model using modified\nprompts that contain concept definitions, which we pro-\nvide in Table XVII. We find that while including concept\ndefinitions improves performance for some concepts (mass,\ndeformability, contents, can contain liquid), this still does\nnot match PG-InstructBLIP on these concepts, and overall\nperformance in fact decreases compared to the original\nprompts. We believe this could be because InstructBLIP\ndoes not have strong enough language understanding to\nproperly incorporate the concept definitions when providing\nresponses. For this reason, and for simplicity, we use prompts']",Q-Former text conditioning during VQA evaluation improves PG-InstructBLIP performance. Including object category labels and concept definitions in question prompts does not significantly affect test accuracy on PHYSOBJECTS.,multi_context,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
What was the objective of the DARPA-funded Robotic Systems project in 2018 for enhancing the development of robotic systems in complex underground environments?,"['search Projects Agency (DARPA) in 2018. Its goal was to expedite development of robotic systems to rapidly\nmap, navigate, and search complex underground environments such as human-made tunnel systems, urban\nunderground and natural cave networks. Three Circuit events were held focusing on the aforementioned un-\nderground environments to qualify for the Finals event in Louisville, KY which combined all environments\ninto a single, purpose-built course. Eight qualiﬁed teams, six of which were DARPA funded, competed on\nSeptember 21-23, 2021 to explore the unknown course to ﬁnd, locate and identify a number of artifacts. For\nevery artifact which was identiﬁed correctly and localized accurately, a point was awarded. Two preliminary\nrounds of 30 minutes were followed by the all-deciding 60 minute Prize Run. Only a single human supervisor\nwas allowed to remotely interact with the robots once they entered the competition area and communications\nwere severely limited, requiring a high level of autonomy from the robots. The composition of the robot\nteams was not prescribed by DARPA, and approaches changed over the four years of the competition. In the\nend, all funded teams brought at least one legged robot to the Finals. Our team CERBERUS (Tranzatto\net al., 2022b), which won the competition, focused their ground robot eﬀorts exclusively on legged robots\nfrom the beginning and brought four ANYmal-C (Hutter et al., 2016) quadrupeds to the Finals.\n1.3.1\nNavigation Planners in the SubT Challenge\nThe approaches for ground robot navigation used during SubT are diverse. Team CoStar used a 2D multi-\nlayer risk map to assess the terrain and planned the robot base path using a risk-aware kinodynamic MPC\nplanner (Fan et al., 2021; Thakker et al., 2021). Team CSIRO Data61 used heuristic height map features\nto assess terrain traversability. They also used the concept of virtual surfaces (see Section 2.4.1) to com-\npute a lower bound for terrain inclination which was used to avoid negative obstacles (Hines et al., 2021).\nAdditionally, they used a deep reinforcement learning policy to control their tracked robots through narrow\ngaps (Tidd et al., 2021).\nTeam Explorer developed a kinodynamic local planner (Chao et al., 2021) as\nwell as a viewpoint-based planner using a polygonal representation of the environment (Yang et al., 2021b).\nHowever, both works do not explicitly state how the traversable regions and obstacles were computed. Team\nCTU-CRAS-NORLAB computed traversable regions in a height map based on the neighboring cell height\ndiﬀerence and planned using the A* algorithm (Bayer and Faigl, 2019; Bayer and Faigl, 2020). Team MAR-\nBLE used an Octree representation of the environment and computed traversability based on the surface\nnormal of extracted ground voxels (Ohradzansky et al., 2021). They employed a graph-based global planner\ncombined with a reactive local controller. All other funded teams besides team CERBERUS used a team of']","The objective of the DARPA-funded Robotic Systems project in 2018 was to expedite the development of robotic systems to rapidly map, navigate, and search complex underground environments such as human-made tunnel systems, urban underground, and natural cave networks.",multi_context,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How can large models and robots be applied in agriculture and farm mechanization?,"['Robots can analyze medical images, patient data, and\nclinical records, aiding in disease detection, surgical\nplanning, and personalized therapy. They can also pro-\nvide physical assistance and rehabilitation exercises\nfor mobility-impaired patients.\n• Environmental monitoring and exploration. Large\nmodels can be combined with robot platforms for\nmonitoring and exploration in various environments,\nsuch as oceans, forests, and disaster sites. These robots\ncan analyze sensor data, satellite imagery, and other\nenvironmental data to monitor pollution levels, detect\nnatural disasters, and explore uncharted territories.\n• Agriculture and farm mechanization. Large models\nand robots can be applied in agriculture and farm\nmechanization, optimizing crop management, mon-\nitoring plant health, and automating labor-intensive\ntasks. Robots equipped with sensors and cameras can\ncollect data from farmlands, and analyze soil condi-\ntions, climate changes, and crop requirements, provid-\ning farmers with decision support to enhance agricul-\ntural productivity and sustainability.\n• Education and learning assistance. Large models\nand robots can provide personalized tutoring and\nlearning support in the field of education. Robots\ncan interact with students, and then offer personal-\nized learning materials and guidance based on their\nabilities and needs [2]. Leveraging the semantic un-\nderstanding and knowledge reasoning capabilities of\nlarge models, robots can answer questions, explain\nconcepts, and help students deepen their understand-\ning of knowledge.\nIn summary, the combination of large models and robotics\nholds tremendous potential across various domains, in-\ncluding autonomous navigation, speech interaction, visual\nperception, human-robot collaboration, industrial automa-\ntion, healthcare, environmental monitoring, agriculture, and\neducation. It can bring convenience and innovation to human\nlife and work.\n5. Challenges\n5.1. Datasets\nIn the realm of Web 3.0 [39], big data [123], AI-\nGenerated Content (AIGC) [140], and machine learning,\ncollecting datasets has always been a challenge. Currently,\ntraining LLMs require vast amounts of data to support their\ncapabilities, particularly high-quality datasets that consume\nconsiderable resources. In the field of robotics, collecting\ndatasets is even more difficult. While LLM like ChatGPT re-\nlies on text data for pre-training [14], VLM uses a combina-\ntion of text and image data [99]. Robotics, however, requires\na combination of both, with the addition of multimodal\ndata, such as text, images, and touch, to serve as the robot’s\nsensory input. These diverse datasets need to be processed\nin a unified format [34], allowing the robot’s brain to plan\nand divide tasks effectively. Unfortunately, there is a lack\nof ready-made, multi-modal datasets, and collecting them\nrequires a significant time investment. Moreover, policy\ncontrol is necessary, which includes the interaction between']","Large models and robots can be applied in agriculture and farm mechanization by optimizing crop management, monitoring plant health, and automating labor-intensive tasks. Robots equipped with sensors and cameras can collect data from farmlands, analyze soil conditions, climate changes, and crop requirements, providing farmers with decision support to enhance agricultural productivity and sustainability.",simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What other objects are commonly found near a sink?,"['chair, measuring cup, pencil, picture\nframe,\ncomputer\nkeyboard,\ngame\ncontroller/pad, tea cup, tin can, salt\nand pepper shakers, television, cof-\nfeemaker, stapler, tablet computer,\nkettle, vase, coffee cup, mixing bowl,\ncomputer monitor, stool, ring, alarm\nclock, light switch, saucer, printer,\nscrewdriver, guitar, camera, jug, gas\nstove, baseball bat, humidifier, chest\nof drawers, sink, can opener, night-\nstand, hair dryer\nTABLE VIII: Object category assignments to high and low tiers for continuous concepts\nConcept\nLabel\nCategories\nMaterial\nPlastic\nremote control, computer mouse, computer keyboard,\ngame controller/pad, plastic bag\nGlass\nwater glass, wine glass\nMetal\ntin can, kitchen knife, can opener\nPaper\nbook, paper, paper towel\nFabric\nclothing, towel, blanket, scarf, sock\nFood\nbanana, apple, orange, cookie, pear, bread\nTransparency\nTransparent\nwine glass\nOpaque\nbook, pillow, remote control, clothing, laptop, mobile\nphone, towel, headphones, spatula, chair, frying pan,\nbanana, wallet, flowerpot, scissors, apple, houseplant,\nhouse/car key, pencil, computer keyboard, tin can,\nwhisk, dumbbell, orange, belt, cutting board, toaster,\nteddy bear, tablet computer, cookie, pear, computer mon-\nitor, stool, light switch, bread, pressure cooker, scarf,\nlaptop charger, guitar, camera, yoga mat, shirt, baseball\nbat, paper towel, kitchen knife, sink, chest of drawers,\ncan opener, boot, nightstand, hair dryer\nCan Contain Liquid\nYes\nbottle, mug, water glass, measuring cup, wine glass,\ntea cup, kettle, coffee cup, mixing bowl, jug, pitcher\n(container), tin can\nNo\npicnic basket, serving tray\nIs Sealed\nNo\nplate, bowl, mug, water glass, measuring cup, wine\nglass, tea cup, frying pan, flowerpot, kettle, vase, coffee\ncup, mixing bowl, saucer, jug, serving tray, pitcher\n(container), picnic basket\nTABLE IX: Concept label assignments of object categories for categorical concepts\nIn the annotation user interface, for each object example,\nthe object is shown in the context of its surrounding scene,\nwith the object indicated by its bounding box. We also\nprovide the object’s category label to help clarify which\nobject is to be annotated. Crowd-workers can choose an\nannotation label by clicking on an associated button, or\ntyping an associated keyboard key. We also provide a back\noption to go to the previous example to correct mistakes.\nFor the concepts material and contents, the user may choose\nother as an option, whereupon they are presented with a text\nbox to type an open-ended label. We do not annotate objects\nfrom the categories pet, cat, and dog, to omit objects that\nare living.\nWe provide instructions to annotators that are specific to\neach concept, to encourage annotations that agree with our\nconcept definitions. We provide an image of the instruction\npage provided to annotators for the fragility concept, which\nalso includes an example of the annotation user interface, in\nFig. 7. The instructions for how to annotate each property are']","chest of drawers, can opener, nightstand, hair dryer",simple,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
How have researchers worked towards understanding natural language directions in the field of robotics?,"['action in route instructions,” in Proceedings of the\nTwenty-First AAAI Conference on Artificial Intelli-\ngence, 2006.\n[98] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward\nunderstanding natural language directions,” in 2010\n5th ACM/IEEE International Conference on Human-\nRobot Interaction (HRI), 2010, pp. 259–266.\n[99] D. L. Chen and R. J. Mooney, “Learning to interpret\nnatural language navigation instructions from observa-\ntions,” in Proceedings of the Twenty-Fifth AAAI Con-\nference on Artificial Intelligence, 2011, p. 859–865.\n[100] F. Duvallet, J. Oh, A. Stentz, M. Walter, T. Howard,\nS. Hemachandra, S. Teller, and N. Roy, “Inferring\nmaps and behaviors from natural language instruc-\ntions,” in International Symposium on Experimental\nRobotics (ISER), 2014.\n[101] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foer-\nster, J. Andreas, E. Grefenstette, S. Whiteson, and\nT. Rockt¨\naschel, “A survey of reinforcement learning\ninformed by natural language,” in IJCAI, 2019.\n[102] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee,\nC. Baral, and H. Ben Amor, “Language-conditioned\nimitation learning for robot manipulation tasks,” Ad-\nvances in Neural Information Processing Systems,\nvol. 33, pp. 13 139–13 150, 2020.\n[103] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn\net al., “Learning language-conditioned robot behavior\nfrom offline data and crowd-sourced annotation,” in\nConference on Robot Learning.\nPMLR, 2022, pp.\n1303–1315.\n[104] O.\nMees,\nL.\nHermann,\nE.\nRosete-Beas,\nand\nW. Burgard, “CALVIN: A benchmark for language-\nconditioned policy learning for long-horizon robot\nmanipulation tasks,” IEEE Robotics and Automation\nLetters, 2022.\n[105] O. Mees, L. Hermann, and W. Burgard, “What matters\nin language conditioned robotic imitation learning\nover unstructured data,” IEEE Robotics and Automa-\ntion Letters, vol. 7, no. 4, pp. 11 205–11 212, 2022.\n[106] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver-\nactor: A multi-task transformer for robotic manipu-\nlation,” Conference on Robot Learning (CoRL), 2022.\n[107] F. Hill, S. Mokra, N. Wong, and T. Harley, “Human\ninstruction-following with deep reinforcement learn-\ning via transfer-learning from text,” arXiv preprint\narXiv:2005.09382, 2020.\n[108] C. Lynch and P. Sermanet, “Grounding language in\nplay,” Robotics: Science and Systems (RSS), 2021.\n[109] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes,\nB. David, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog et al., “Do as I can, not as I say: Grounding\nlanguage in robotic affordances,” Conference on Robot\nLearning (CoRL), 2022.\n[110] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou,\nY. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and\nL. Fan, “VIMA: General robot manipulation with\nmultimodal prompts,” International Conference on\nMachine Learning (ICML), 2023.\n[111] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor,\n“ChatGPT for robotics: Design principles and model\nabilities,” Microsoft Auton. Syst. Robot. Res, vol. 2,\np.'
 'com/science/article/pii/0010028572900023\n[97] M. MacMahon, B. Stankiewicz, and B. Kuipers,\n“Walk the talk: Connecting language, knowledge, and\naction in route instructions,” in Proceedings of the\nTwenty-First AAAI Conference on Artificial Intelli-\ngence, 2006.\n[98] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward']","Researchers have worked towards understanding natural language directions in the field of robotics through various approaches, including learning to interpret natural language navigation instructions from observations, inferring maps and behaviors from natural language instructions, and language-conditioned imitation learning for robot manipulation tasks. They have also explored reinforcement learning informed by natural language and learning language-conditioned robot behavior from offline data and crowd-sourced annotation.",simple,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
What are some recent advancements in imitation learning for robot control?,"['Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio\nOjea, Eugen Solowjow, and Sergey Levine.\nResidual\nreinforcement learning for robot control. In 2019 Interna-\ntional Conference on Robotics and Automation (ICRA),\npages 6023–6029. IEEE, 2019.\n[29] Edward Johns. Coarse-to-ﬁne imitation learning: Robot\nmanipulation from a single demonstration. In 2021 IEEE\ninternational conference on robotics and automation\n(ICRA), pages 4613–4619. IEEE, 2021.\n[30] Haresh Karnan, Garrett Warnell, Xuesu Xiao, and Peter\nStone.\nVoila: Visual-observation-only imitation learn-\ning for autonomous navigation.\nIn 2022 International\nConference on Robotics and Automation (ICRA), pages\n2497–2503. IEEE, 2022.\n[31] Philip A Knight. The sinkhorn–knopp algorithm: conver-\ngence and applications. SIAM Journal on Matrix Analysis\nand Applications, 30(1):261–275, 2008.\n[32] Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta\nDwibedi,\nSergey\nLevine,\nand\nJonathan\nTompson.\nDiscriminator-actor-critic:\nAddressing\nsample\ninefﬁ-\nciency and reward bias in adversarial imitation learning.\narXiv preprint arXiv:1809.02925, 2018.\n[33] Sateesh Kumar, Jonathan Zamora, Nicklas Hansen,\nRishabh Jangir, and Xiaolong Wang. Graph inverse rein-\nforcement learning from diverse videos. arXiv preprint\narXiv:2207.14299, 2022.\n[34] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,\nNicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. Continuous control with deep reinforce-\nment learning. arXiv preprint arXiv:1509.02971, 2015.\n[35] Zhao Mandi, Fangchen Liu, Kimin Lee, and Pieter\nAbbeel.\nTowards more generalizable one-shot visual\nimitation learning.\nIn 2022 International Conference\non Robotics and Automation (ICRA), pages 2434–2444.\nIEEE, 2022.\n[36] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush\nNasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,\nSilvio Savarese, Yuke Zhu, and Roberto Mart´\nın-Mart´\nın.\nWhat matters in learning from ofﬂine human demon-\nstrations\nfor\nrobot\nmanipulation.\narXiv\npreprint\narXiv:2108.03298, 2021.\n[37] Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAndrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. Human-level control through deep rein-\nforcement learning. nature, 518(7540):529–533, 2015.\n[38] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wo-\njciech Zaremba, and Pieter Abbeel.\nOvercoming ex-\nploration in reinforcement learning with demonstrations.\nIn 2018 IEEE international conference on robotics and\nautomation (ICRA), pages 6292–6299. IEEE, 2018.\n[39] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and\nSergey Levine.\nAwac: Accelerating online reinforce-\nment learning with ofﬂine datasets.\narXiv preprint\narXiv:2006.09359, 2020.\n[40] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea\nFinn, and Abhinav Gupta.\nR3m: A universal visual\nrepresentation for robot manipulation.\narXiv preprint\narXiv:2203.12601, 2022.\n[41] Andrew Y Ng, Stuart J Russell, et al.\nAlgorithms']","Recent advancements in imitation learning for robot control include residual reinforcement learning, coarse-to-fine imitation learning, visual-observation-only imitation learning, graph inverse reinforcement learning from diverse videos, and learning from offline human demonstrations.",simple,"[{'Authors': 'Siddhant Haldar, Jyothish Pari, Anant Rai, Lerrel Pinto', 'Published': '2023-03-02', 'Summary': 'While imitation learning provides us with an efficient toolkit to train\nrobots, learning skills that are robust to environment variations remains a\nsignificant challenge. Current approaches address this challenge by relying\neither on large amounts of demonstrations that span environment variations or\non handcrafted reward functions that require state estimates. Both directions\nare not scalable to fast imitation. In this work, we present Fast Imitation of\nSkills from Humans (FISH), a new imitation learning approach that can learn\nrobust visual skills with less than a minute of human demonstrations. Given a\nweak base-policy trained by offline imitation of demonstrations, FISH computes\nrewards that correspond to the ""match"" between the robot\'s behavior and the\ndemonstrations. These rewards are then used to adaptively update a residual\npolicy that adds on to the base-policy. Across all tasks, FISH requires at most\ntwenty minutes of interactive learning to imitate demonstrations on object\nconfigurations that were not seen in the demonstrations. Importantly, FISH is\nconstructed to be versatile, which allows it to be used across robot\nmorphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g.\nthird-person, eye-in-hand). Our experimental evaluations on 9 different tasks\nshow that FISH achieves an average success rate of 93%, which is around 3.8x\nhigher than prior state-of-the-art methods.', 'Title': 'Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations'}]",TRUE
How is language-conditioned behavior learned in robotics?,"['number of memories grows. As the robot’s mem-\nory burden increases over time, it must be able to\neffectively manage and retrieve memories to avoid\ncatastrophic forgetfulness [68].\nReasoning. Reasoning serves as a foundational element\nin human cognition, playing a crucial role in problem-\nsolving, decision-making, and the analytical examination of\ninformation [135, 136]. Reasoning plays a crucial role in en-\nabling LLMs to solve complex tasks. Reasoning capabilities\nallow LLMs to break down problems into smaller, manage-\nable steps and solve them starting from the current status\nand known conditions. There is ongoing debate about how\nLLMs acquire their reasoning abilities, with some arguing\nthat it is a result of pre-training or fine-tuning [54], while\nothers believe that it emerges only at a certain scale [137].\nF. Zeng et al.: Preprint submitted to Elsevier\nPage 9 of 19\nLarge Language Models for Robotics: A Survey\nResearch has shown that Chain-of-Thought (CoT) [136] can\nhelp LLMs reveal their reasoning capabilities, and some\nstudies suggest that inference abilities may stem from the\nlocal static structure of the training data.\nPlanning. Humans plan when faced with complex chal-\nlenges. Planning can help people organize their thoughts,\nset goals, and decide what they should do in the current\nsituation [45, 130]. In this case, they can gradually approach\ntheir goals. The core of planning is reasoning. The agent can\nuse reasoning capabilities to deconstruct the received high-\nlevel abstract instructions into executable subtasks and make\nreasonable plans for each subtask [26, 112]. For example,\nLM-Nav uses ChatGpt to process received natural language\ninstructions [117]. PaLM-E directly implements end-to-end\nprocessing, converting the received multi-modal input into\nmulti-modal sentences for LLM processing [34]. Agents\nmay also be able to reasonably update task planning based on\nthe current situation through multiple rounds of dialogue and\nself-questioning and answering in the future. Many studies\nhave proposed methods of dividing the execution tasks into\nmany executable small tasks during the planning process.\nFor example, directly break down the execution task into\nmany small tasks and execute them sequentially [103, 145].\nCoT only processes one sub-task at a time and can adaptively\ncomplete the task, which has a certain degree of flexibility\n[69, 138]. There are also some vertical planning methods\nthat divide tasks into tree diagrams [49, 148].\n3.3. Control\nHere, we argue that the control module is the key compo-\nnent responsible for regulating robotic actions. This module\nplays a crucial role in ensuring that the robot’s actions are\nexecuted accurately and successfully, with a focus on the\nhardware aspects of action execution.\n3.3.1. How to learn language-conditioned behavior\nMuch of the previous work has focused on enabling\nrobots and other agents to comprehend and execute nat-\nural language instructions [19, 35, 81]. There are various']",Much of the previous work has focused on enabling robots and other agents to comprehend and execute natural language instructions.,simple,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are some recent studies on the detection and assessment of Covid-19 using deep learning and chest CT scans?,"['ings, Part VII. Springer, 2023, pp. 691–708.\n[14] Chuansheng Zheng, Xianbo Deng, Qing Fu, Qiang\nZhou, Jiapei Feng, Hui Ma, Wenyu Liu, and Xinggang\nWang,\n“Deep learning-based detection for covid-19\nfrom chest ct using weak label,” MedRxiv, 2020.\n[15] Lu Huang, Rui Han, Tao Ai, Pengxin Yu, Han Kang,\nQian Tao, and Liming Xia, “Serial quantitative chest\nct assessment of covid-19: a deep learning approach,”\nRadiology: Cardiothoracic Imaging, vol. 2, no. 2, pp.\ne200075, 2020.']","There are two recent studies on the detection and assessment of Covid-19 using deep learning and chest CT scans. One study is titled 'Deep learning-based detection for covid-19 from chest ct using weak label' by Chuansheng Zheng, Xianbo Deng, Qing Fu, Qiang Zhou, Jiapei Feng, Hui Ma, Wenyu Liu, and Xinggang Wang. The other study is titled 'Serial quantitative chest ct assessment of covid-19: a deep learning approach' by Lu Huang, Rui Han, Tao Ai, Pengxin Yu, Han Kang, Qian Tao, and Liming Xia.",simple,"[{'Authors': 'Dimitrios Kollias, Anastasios Arsenos, Stefanos Kollias', 'Published': '2024-03-10', 'Summary': ""The paper presents the DEF-AI-MIA COV19D Competition, which is organized in\nthe framework of the 'Domain adaptation, Explainability, Fairness in AI for\nMedical Image Analysis (DEF-AI-MIA)' Workshop of the 2024 Computer Vision and\nPattern Recognition (CVPR) Conference. The Competition is the 4th in the\nseries, following the first three Competitions held in the framework of ICCV\n2021, ECCV 2022 and ICASSP 2023 International Conferences respectively. It\nincludes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain\nAdaptation. The Competition use data from COV19-CT-DB database, which is\ndescribed in the paper and includes a large number of chest CT scan series.\nEach chest CT scan series consists of a sequence of 2-D CT slices, the number\nof which is between 50 and 700. Training, validation and test datasets have\nbeen extracted from COV19-CT-DB and provided to the participants in both\nChallenges. The paper presents the baseline models used in the Challenges and\nthe performance which was obtained respectively."", 'Title': 'Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans'}]",TRUE
How does chunking data in RAG systems help with hallucinated responses from LLMs and linking sources/references to generated responses?,"['Seven Failure Points When Engineering a Retrieval Augmented Generation System\nCAIN 2024, April 2024, Lisbon, Portugal\nFP\nLesson\nDescription\nCase Studies\nFP4\nLarger context get better results (Context refers to a\nparticular setting or situation in which the content\noccurs)\nA larger context enabled more accurate responses\n(8K vs 4K). Contrary to prior work with GPT-3.5 [13]\nAI Tutor\nFP1\nSemantic caching drives cost and latency down\nRAG systems struggle with concurrent users due to\nrate limits and the cost of LLMs. Prepopulate the\nsemantic cache with frequently asked questions [1].\nAI Tutor\nFP5-7\nJailbreaks bypass the RAG system and hit the safety\ntraining.\nResearch suggests fine-tuning LLMs reverses safety\ntraining [11], test all fine-tuned LLMs for RAG sys-\ntem.\nAI Tutor\nFP2, FP4\nAdding meta-data improves retrieval.\nAdding the file name and chunk number into the\nretrieved context helped the reader extract the re-\nquired information. Useful for chat dialogue.\nAI Tutor\nFP2, FP4-7\nOpen source embedding models perform better for\nsmall text.\nOpensource sentence embedding models performed\nas well as closed source alternatives on small text.\nBioASQ, AI Tutor\nFP2-7\nRAG systems require continuous calibration.\nRAG systems receive unknown input at runtime\nrequiring constant monitoring.\nAI Tutor, BioASQ\nFP1, FP2\nImplement a RAG pipeline for configuration.\nA RAG system requires calibrating chunk size,\nembedding strategy, chunking strategy, retrieval\nstrategy, consolidation strategy, context size, and\nprompts.\nCognitive Reviewer,\nAI Tutor, BioASQ\nFP2, FP4\nRAG pipelines created by assembling bespoke solu-\ntions are suboptima.\nEnd-to-end training enhances domain adaptation\nin RAG systems [18].\nBioASQ, AI Tutor\nFP2-7\nTesting performance characteristics are only possi-\nble at runtime.\nOffline evaluation techniques such as G-Evals [14]\nlook promising but are premised on having access\nto labelled question and answer pairs.\nCognitive Reviewer,\nAI Tutor\nTable 2: The lessons learned from the three case studies with key takeaways for future RAG implementations\nsort out the security/privacy (who can access what). Furthermore,\nas the foundation model itself evolves or you get new data to add to\nthe model, you will need to run finetuning again. On the other side,\nRAG systems seem to offer a pragmatic solution allowing you to\nchunk your data as needed and only use relevant chunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-']","Chunking data in RAG systems helps with hallucinated responses from LLMs by allowing the user to control what chunks of information are included in the context. This helps prevent the LLM from generating responses based on irrelevant or incorrect information. Additionally, chunking data allows for linking sources/references to generated responses, ensuring that the information provided is accurate and can be traced back to reliable sources.",reasoning,"[{'Authors': 'Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek', 'Published': '2024-01-11', 'Summary': 'Software engineers are increasingly adding semantic search capabilities to\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\nRAG system involves finding documents that semantically match a query and then\npassing the documents to a large language model (LLM) such as ChatGPT to\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\nproblem of hallucinated responses from LLMs, b) link sources/references to\ngenerated responses, and c) remove the need for annotating documents with\nmeta-data. However, RAG systems suffer from limitations inherent to information\nretrieval systems and from reliance on LLMs. In this paper, we present an\nexperience report on the failure points of RAG systems from three case studies\nfrom separate domains: research, education, and biomedical. We share the\nlessons learned and present 7 failure points to consider when designing a RAG\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\nsystem is only feasible during operation, and 2) the robustness of a RAG system\nevolves rather than designed in at the start. We conclude with a list of\npotential research directions on RAG systems for the software engineering\ncommunity.', 'Title': 'Seven Failure Points When Engineering a Retrieval Augmented Generation System'}]",TRUE
Can you recommend any research papers on language-conditioned policy learning for robotic manipulation tasks?,"['action in route instructions,” in Proceedings of the\nTwenty-First AAAI Conference on Artificial Intelli-\ngence, 2006.\n[98] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward\nunderstanding natural language directions,” in 2010\n5th ACM/IEEE International Conference on Human-\nRobot Interaction (HRI), 2010, pp. 259–266.\n[99] D. L. Chen and R. J. Mooney, “Learning to interpret\nnatural language navigation instructions from observa-\ntions,” in Proceedings of the Twenty-Fifth AAAI Con-\nference on Artificial Intelligence, 2011, p. 859–865.\n[100] F. Duvallet, J. Oh, A. Stentz, M. Walter, T. Howard,\nS. Hemachandra, S. Teller, and N. Roy, “Inferring\nmaps and behaviors from natural language instruc-\ntions,” in International Symposium on Experimental\nRobotics (ISER), 2014.\n[101] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foer-\nster, J. Andreas, E. Grefenstette, S. Whiteson, and\nT. Rockt¨\naschel, “A survey of reinforcement learning\ninformed by natural language,” in IJCAI, 2019.\n[102] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee,\nC. Baral, and H. Ben Amor, “Language-conditioned\nimitation learning for robot manipulation tasks,” Ad-\nvances in Neural Information Processing Systems,\nvol. 33, pp. 13 139–13 150, 2020.\n[103] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn\net al., “Learning language-conditioned robot behavior\nfrom offline data and crowd-sourced annotation,” in\nConference on Robot Learning.\nPMLR, 2022, pp.\n1303–1315.\n[104] O.\nMees,\nL.\nHermann,\nE.\nRosete-Beas,\nand\nW. Burgard, “CALVIN: A benchmark for language-\nconditioned policy learning for long-horizon robot\nmanipulation tasks,” IEEE Robotics and Automation\nLetters, 2022.\n[105] O. Mees, L. Hermann, and W. Burgard, “What matters\nin language conditioned robotic imitation learning\nover unstructured data,” IEEE Robotics and Automa-\ntion Letters, vol. 7, no. 4, pp. 11 205–11 212, 2022.\n[106] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver-\nactor: A multi-task transformer for robotic manipu-\nlation,” Conference on Robot Learning (CoRL), 2022.\n[107] F. Hill, S. Mokra, N. Wong, and T. Harley, “Human\ninstruction-following with deep reinforcement learn-\ning via transfer-learning from text,” arXiv preprint\narXiv:2005.09382, 2020.\n[108] C. Lynch and P. Sermanet, “Grounding language in\nplay,” Robotics: Science and Systems (RSS), 2021.\n[109] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes,\nB. David, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog et al., “Do as I can, not as I say: Grounding\nlanguage in robotic affordances,” Conference on Robot\nLearning (CoRL), 2022.\n[110] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou,\nY. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and\nL. Fan, “VIMA: General robot manipulation with\nmultimodal prompts,” International Conference on\nMachine Learning (ICML), 2023.\n[111] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor,\n“ChatGPT for robotics: Design principles and model\nabilities,” Microsoft Auton. Syst. Robot. Res, vol. 2,\np.'
 'com/science/article/pii/0010028572900023\n[97] M. MacMahon, B. Stankiewicz, and B. Kuipers,\n“Walk the talk: Connecting language, knowledge, and\naction in route instructions,” in Proceedings of the\nTwenty-First AAAI Conference on Artificial Intelli-\ngence, 2006.\n[98] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward']","Yes, here are some research papers on language-conditioned policy learning for robotic manipulation tasks: 
- ""Language-conditioned imitation learning for robot manipulation tasks"" by S. Stepputtis et al. (2020)
- ""Learning language-conditioned robot behavior from offline data and crowd-sourced annotation"" by S. Nair et al. (2022)
- ""CALVIN: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks"" by O. Mees et al. (2022)
- ""What matters in language conditioned robotic imitation learning over unstructured data"" by O. Mees et al. (2022)
- ""Perceiver-actor: A multi-task transformer for robotic manipulation"" by M. Shridhar et al. (2022)
- ""Human instruction-following with deep reinforcement learning via transfer-learning from text"" by F. Hill et al. (2020)
- ""Grounding language in play"" by C. Lynch and P. Sermanet (2021)
- ""Do as I can, not as I say: Grounding language in robotic affordances"" by M. Ahn et al. (2022)
- ""VIMA: General robot manipulation with multimodal prompts"" by Y. Jiang et al. (2023)
- ""ChatGPT for robotics: Design principles and model abilities"" by S. Vemprala et al. (2022)",reasoning,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
What does the PHYSOBJECTS dataset enhance in vision-language models for robotic manipulation?,"['Physically Grounded Vision-Language Models for Robotic Manipulation\nJensen Gao1, Bidipta Sarkar1, Fei Xia2, Ted Xiao2, Jiajun Wu1,\nBrian Ichter2, Anirudha Majumdar2,3, Dorsa Sadigh1,2\nAbstract— Recent\nadvances\nin\nvision-language\nmodels\n(VLMs) have led to improved performance on tasks such as\nvisual question answering and image captioning. Consequently,\nthese models are now well-positioned to reason about the\nphysical world, particularly within domains such as robotic\nmanipulation. However, current VLMs are limited in their\nunderstanding of the physical concepts (e.g., material, fragility)\nof common objects, which restricts their usefulness for robotic\nmanipulation tasks that involve interaction and physical reason-\ning about such objects. To address this limitation, we propose\nPHYSOBJECTS, an object-centric dataset of 39.6K crowd-\nsourced and 417K automated physical concept annotations of\ncommon household objects. We demonstrate that fine-tuning a\nVLM on PHYSOBJECTS improves its understanding of physical\nobject concepts, including generalization to held-out concepts,\nby capturing human priors of these concepts from visual\nappearance. We incorporate this physically grounded VLM in\nan interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on\ntasks that require reasoning about physical object concepts,\ncompared to baselines that do not leverage physically grounded\nVLMs. We additionally illustrate the benefits of our physically\ngrounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and\nvisualizations of our results at https://iliad.stanford.\nedu/pg-vlm/.\nI. INTRODUCTION\nLarge language models (LLMs) have shown great promise\nfor converting language instructions into task plans for em-\nbodied agents [1], [2]. The fundamental challenge in apply-\ning LLMs for this is grounding them to the physical world,\nthrough sensory input such as vision. Prior work has made\nprogress towards grounding LLMs by using vision-language\nmodels (VLMs) to indicate the presence of objects in a\nscene, or to provide feedback about occurrences in a scene\n[3]–[7]. However, vision could be used to further improve\ngrounding by extracting more detailed scene information.\nFor robotic manipulation, understanding physical concepts of\nobjects, such as their material composition or their fragility,\nwould help planners identify relevant objects to interact with,\nand affordances based on physical or safety constraints. For\nexample, if a human wants a robot to get a cup of water,\nthe robot should be able to determine if a cup already has\nwater or something else in it. Also, the robot should handle\nthe cup with greater caution if it is more fragile.\nHow can we use vision to reason about physical object\nconcepts? Prior work has studied this problem using more\ntraditional vision techniques, such as self-supervised learning'
 'Physically Grounded Vision-Language Models for Robotic Manipulation\nJensen Gao1, Bidipta Sarkar1, Fei Xia2, Ted Xiao2, Jiajun Wu1,\nBrian Ichter2, Anirudha Majumdar2,3, Dorsa Sadigh1,2\nAbstract— Recent\nadvances\nin\nvision-language\nmodels\n(VLMs) have led to improved performance on tasks such as\nvisual question answering and image captioning. Consequently,\nthese models are now well-positioned to reason about the\nphysical world, particularly within domains such as robotic\nmanipulation. However, current VLMs are limited in their\nunderstanding of the physical concepts (e.g., material, fragility)\nof common objects, which restricts their usefulness for robotic\nmanipulation tasks that involve interaction and physical reason-\ning about such objects. To address this limitation, we propose\nPHYSOBJECTS, an object-centric dataset of 39.6K crowd-\nsourced and 417K automated physical concept annotations of\ncommon household objects. We demonstrate that fine-tuning a\nVLM on PHYSOBJECTS improves its understanding of physical\nobject concepts, including generalization to held-out concepts,\nby capturing human priors of these concepts from visual\nappearance. We incorporate this physically grounded VLM in\nan interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on\ntasks that require reasoning about physical object concepts,\ncompared to baselines that do not leverage physically grounded\nVLMs. We additionally illustrate the benefits of our physically\ngrounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and\nvisualizations of our results at https://iliad.stanford.\nedu/pg-vlm/.\nI. INTRODUCTION\nLarge language models (LLMs) have shown great promise\nfor converting language instructions into task plans for em-\nbodied agents [1], [2]. The fundamental challenge in apply-\ning LLMs for this is grounding them to the physical world,\nthrough sensory input such as vision. Prior work has made\nprogress towards grounding LLMs by using vision-language\nmodels (VLMs) to indicate the presence of objects in a\nscene, or to provide feedback about occurrences in a scene\n[3]–[7]. However, vision could be used to further improve\ngrounding by extracting more detailed scene information.\nFor robotic manipulation, understanding physical concepts of\nobjects, such as their material composition or their fragility,\nwould help planners identify relevant objects to interact with,\nand affordances based on physical or safety constraints. For\nexample, if a human wants a robot to get a cup of water,\nthe robot should be able to determine if a cup already has\nwater or something else in it. Also, the robot should handle\nthe cup with greater caution if it is more fragile.\nHow can we use vision to reason about physical object\nconcepts? Prior work has studied this problem using more\ntraditional vision techniques, such as self-supervised learning']",The PHYSOBJECTS dataset enhances the understanding of physical object concepts in vision-language models for robotic manipulation.,multi_context,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
"What research has been done on understanding natural language directions in robotics, and what are the key findings and approaches?","['action in route instructions,” in Proceedings of the\nTwenty-First AAAI Conference on Artificial Intelli-\ngence, 2006.\n[98] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward\nunderstanding natural language directions,” in 2010\n5th ACM/IEEE International Conference on Human-\nRobot Interaction (HRI), 2010, pp. 259–266.\n[99] D. L. Chen and R. J. Mooney, “Learning to interpret\nnatural language navigation instructions from observa-\ntions,” in Proceedings of the Twenty-Fifth AAAI Con-\nference on Artificial Intelligence, 2011, p. 859–865.\n[100] F. Duvallet, J. Oh, A. Stentz, M. Walter, T. Howard,\nS. Hemachandra, S. Teller, and N. Roy, “Inferring\nmaps and behaviors from natural language instruc-\ntions,” in International Symposium on Experimental\nRobotics (ISER), 2014.\n[101] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foer-\nster, J. Andreas, E. Grefenstette, S. Whiteson, and\nT. Rockt¨\naschel, “A survey of reinforcement learning\ninformed by natural language,” in IJCAI, 2019.\n[102] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee,\nC. Baral, and H. Ben Amor, “Language-conditioned\nimitation learning for robot manipulation tasks,” Ad-\nvances in Neural Information Processing Systems,\nvol. 33, pp. 13 139–13 150, 2020.\n[103] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn\net al., “Learning language-conditioned robot behavior\nfrom offline data and crowd-sourced annotation,” in\nConference on Robot Learning.\nPMLR, 2022, pp.\n1303–1315.\n[104] O.\nMees,\nL.\nHermann,\nE.\nRosete-Beas,\nand\nW. Burgard, “CALVIN: A benchmark for language-\nconditioned policy learning for long-horizon robot\nmanipulation tasks,” IEEE Robotics and Automation\nLetters, 2022.\n[105] O. Mees, L. Hermann, and W. Burgard, “What matters\nin language conditioned robotic imitation learning\nover unstructured data,” IEEE Robotics and Automa-\ntion Letters, vol. 7, no. 4, pp. 11 205–11 212, 2022.\n[106] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver-\nactor: A multi-task transformer for robotic manipu-\nlation,” Conference on Robot Learning (CoRL), 2022.\n[107] F. Hill, S. Mokra, N. Wong, and T. Harley, “Human\ninstruction-following with deep reinforcement learn-\ning via transfer-learning from text,” arXiv preprint\narXiv:2005.09382, 2020.\n[108] C. Lynch and P. Sermanet, “Grounding language in\nplay,” Robotics: Science and Systems (RSS), 2021.\n[109] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes,\nB. David, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog et al., “Do as I can, not as I say: Grounding\nlanguage in robotic affordances,” Conference on Robot\nLearning (CoRL), 2022.\n[110] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou,\nY. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and\nL. Fan, “VIMA: General robot manipulation with\nmultimodal prompts,” International Conference on\nMachine Learning (ICML), 2023.\n[111] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor,\n“ChatGPT for robotics: Design principles and model\nabilities,” Microsoft Auton. Syst. Robot. Res, vol. 2,\np.'
 'com/science/article/pii/0010028572900023\n[97] M. MacMahon, B. Stankiewicz, and B. Kuipers,\n“Walk the talk: Connecting language, knowledge, and\naction in route instructions,” in Proceedings of the\nTwenty-First AAAI Conference on Artificial Intelli-\ngence, 2006.\n[98] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward']","Several research studies have been conducted on understanding natural language directions in robotics. Some key findings and approaches include: 
- MacMahon et al. (2006) proposed a method for connecting language, knowledge, and action in route instructions.
- Kollar et al. (2010) explored the understanding of natural language directions in human-robot interaction.
- Chen and Mooney (2011) focused on learning to interpret natural language navigation instructions from observations.
- Duvallet et al. (2014) worked on inferring maps and behaviors from natural language instructions.
- Luketina et al. (2019) conducted a survey on reinforcement learning informed by natural language.
- Stepputtis et al. (2020) developed language-conditioned imitation learning for robot manipulation tasks.
- Nair et al. (2022) studied learning language-conditioned robot behavior from offline data and crowd-sourced annotation.
- Mees et al. (2022) introduced CALVIN, a benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks.
- Shridhar et al. (2022) proposed the Perceiver-actor, a multi-task transformer for robotic manipulation.
- Hill et al. (2020) explored human instruction-following with deep reinforcement learning via transfer-learning from text.
- Lynch and Sermanet (2021) focused on grounding language in play.
- Ahn et al. (2022) worked on grounding language in robotic affordances.
- Jiang et al. (2023) introduced VIMA, a general robot manipulation system with multimodal prompts.
- Vemprala et al. (2023) discussed ChatGPT for robotics, including design principles and model abilities.",multi_context,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
"How is the height map representation used in the planning process, considering the implementation of erosion and dilation techniques, safety threshold, and filtering based on rising height?","['ure 6\n. It is implemented using image erosion on the foothold score layer of the height map, which reduces\nthe steppable map region by a safety margin. This has the additional beneﬁt of also removing small isolated\nsteppable patches from the map, which could only be overcome by solving a stepping-stone problem. To\navoid unnecessarily inﬂating small obstacles like rails, which the robot can easily step over, we do not inﬂate\nunsteppable regions below a certain size. In practice this is done by performing an image dilation of smaller\nradius, before doing the erosion.\nThis safety threshold was crucial on the Subway Station of the Finals circuit, as shown in Section 3.2.2.\n2.4.3\nCeiling Point Filter\nWe use a 2.5D height map representation for planning. While this is suﬃcient for ground robot navigation\nin most environments, it can be problematic in the tight underground spaces encountered during SubT. Low\nceilings mean that they are frequently observed by the depth sensors, which causes spikes in the height map,\nas shown in Figure 2(b). However, we cannot simply discard all points above a ﬁxed height, since this would\neither prevent us from planning up slopes or from passing underneath low overhangs.\nWe therefore use a rising height threshold to ﬁlter points (Miki et al., 2022b), shown in Figure 6\n. It\nﬁlters points just above robot height close to the robot, and linearly increases the height threshold up to a\nmaximum at larger distances. This setup caused map spikes in parts of the course with low ceilings which\nslowed us down, but these crucially never stopped us from exploring. It allowed us to pass underneath very\nlow overhangs, and to plan up slopes, even when encountered together, as detailed in Section 3.3.1.\n3\nExperimental Results\nArtPlanner was deployed on all four ANYmal-C ground robots of team CERBERUS during all runs of the\nSubT Finals. It used a height map of size 8 m×8 m with a 4 cm resolution. We only cover results related\nto the navigation planner presented in this work. For further details on the general performance we refer to\nour overview article (Tranzatto et al., 2022a).\nWe deployed all four ground robots during the Prize Run, which were directed by the supervisor to explore\ndiﬀerent areas of the course. All ground robots successfully made it to the end of the competition and\nwe did not observe a single path planning or locomotion failure, which could have been provoked by bad\npath planning. Our planner was active for 90 minutes between all robots which accounts for 88.94% of all\nrobot motion. We gracefully navigated the narrow doorways and small rooms in the Urban section, passed\nthrough the Tunnel section with obscuring fog, and made it through the narrowest and roughest part of the\nCave section. The only case where ArtPlanner did not follow the exploration path over traversable terrain\nhappened at the stairs leading to the subway station. This resulted from our operational decision to use']","The height map representation is used in the planning process by implementing erosion and dilation techniques. Erosion is used to reduce the steppable map region by a safety margin, which also removes small isolated steppable patches. Dilation is performed with a smaller radius to avoid inflating small obstacles like rails. A safety threshold is used to avoid unnecessarily inflating small obstacles. The height map is also filtered based on rising height, where points just above robot height close to the robot are filtered, and the height threshold linearly increases up to a maximum at larger distances.",reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
What are the results of the experiments conducted with the RT-2-X model on tasks involving objects and skills from the Bridge dataset?,"['Emergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like\nthe ones shown in Fig. 5. These tasks involve objects and\nskills that are not present in the RT-2 dataset but occur in the\nBridge dataset [95] for a different robot (the WidowX robot).\nResults are shown in Table II, Emergent Skills Evaluation\ncolumn. Comparing rows (1) and (2), we find that RT-2-X\noutperforms RT-2 by ∼3×, suggesting that incorporating\ndata from other robots into the training improves the range\nof tasks that can be performed even by a robot that already\nhas large amounts of data available. Our results suggest that\nco-training with data from other platforms imbues the RT-2-\nX controller with additional skills for the platform that are\nnot present in that platform’s original dataset.\nOur next ablation involves removing the Bridge dataset\nfrom RT-2-X training: Row (3) shows the results for RT-2-\nX that includes all data used for RT-2-X except the Bridge\ndataset. This variation significantly reduces performance on\nthe hold-out tasks, suggesting that transfer from the WidowX\ndata may indeed be responsible for the additional skills that\ncan be performed by RT-2-X with the Google Robot.\nC. Design decisions\nLastly, we perform ablations to measure the influence of\ndifferent design decisions on the generalization capabilities\nof our most performant RT-2-X model, which are presented\nin Table II. We note that including a short history of im-\nages significantly improves generalization performance (row\n(4) vs row (5)). Similarly to the conclusions in the RT-2\npaper [9], Web-based pre-training of the model is critical\nto achieving a high performance for the large models (row\n(4) vs row (6)). We also note that the 55B model has\nsignificantly higher success rate in the Emergent Skills com-\npared to the 5B model (row (2) vs row (4)), demonstrating\nthat higher model capacity enables higher degree of transfer\nacross robotic datasets. Contrary to previous RT-2 findings,\nco-fine-tuning and fine-tuning have similar performance in\nboth the Emergent Skills and Generalization Evaluation (row\n(4) vs row (7)), which we attribute to the fact that the robotics\ndata used in RT-2-X is much more diverse than the previously\nused robotics datasets.\nFig. 5: To assess transfer between embodiments, we evaluate the\nRT-2-X model on out-of-distribution skills. These skills are in\nthe Bridge dataset, but not in the Google Robot dataset (the\nembodiment they are evaluated on).\nVI. DISCUSSION, FUTURE WORK, AND OPEN PROBLEMS\nWe presented a consolidated dataset that combines data\nfrom 22 robotic embodiments collected through a collab-\noration between 21 institutions, demonstrating 527 skills\n(160266 tasks). We also presented an experimental demon-\nstration that Transformer-based policies trained on this data\ncan exhibit significant positive transfer between the different'
 'dataset. This constitutes a reasonable baseline insofar as\nit can be expected that the model has been optimized to\nwork well with the associated data; we refer to this baseline\nmodel as the Original Method model. (2) An RT-1 model\ntrained on the dataset in isolation; this baseline allows us to\nassess whether the RT-X model architectures have enough\ncapacity to represent policies for multiple different robot\nplatforms simultaneously, and whether co-training on multi-\nembodiment data leads to higher performance.\nSmall-scale dataset domains (Fig. 4). RT-1-X outper-\nforms Original Method trained on each of the robot-specific\ndatasets on 4 of the 5 datasets, with a large average im-\nprovement, demonstrating domains with limited data benefit\nsubstantially from co-training on X-embodiment data.\nLarge-scale dataset domains (Table I). In the large-\ndataset setting, the RT-1-X model does not outperform\nthe RT-1 baseline trained on only the embodiment-specific\ndataset, which indicates underfitting for that model class.\nHowever, the larger RT-2-X model outperforms both the\nOriginal Method and RT-1 suggesting that X-robot training\ncan improve performance in the data-rich domains, but only\nwhen utilizing a sufficiently high-capacity architecture.\nB. Improved generalization to out-of-distribution settings\nWe now examine how X-embodiment training can enable\nbetter generalization to out-of-distribution settings and more\ncomplex and novel instructions. These experiments focus on\nthe high-data domains, and use the RT-2-X model.\nRow\nModel\nSize\nHistory Length\nDataset\nCo-Trained w/ Web\nInitial Checkpoint\nEmergent Skills Evaluation\nRT-2 Generalization Evaluation\n(1)\nRT-2\n55B\nnone\nGoogle Robot action\nYes\nWeb-pretrained\n27.3%\n62%\n(2)\nRT-2-X\n55B\nnone\nRobotics data\nYes\nWeb-pretrained\n75.8%\n61%\n(3)\nRT-2-X\n55B\nnone\nRobotics data except Bridge\nYes\nWeb-pretrained\n42.8%\n54%\n(4)\nRT-2-X\n5B\n2\nRobotics data\nYes\nWeb-pretrained\n44.4%\n52%\n(5)\nRT-2-X\n5B\nnone\nRobotics data\nYes\nWeb-pretrained\n14.5%\n30%\n(6)\nRT-2-X\n5B\n2\nRobotics data\nNo\nFrom scratch\n0%\n1%\n(7)\nRT-2-X\n5B\n2\nRobotics data\nNo\nWeb-pretrained\n48.7%\n47%\nTABLE II: Ablations to show the impact of design decisions on generalization (to unseen objects, backgrounds, and environments) and\nemergent skills (skills from other datasets on the Google Robot), showing the importance of Web-pretraining, model size, and history.\nUnseen objects, backgrounds and environments. We\nfirst conduct the same evaluation of generalization properties\nas proposed in [9], testing for the ability to manipulate\nunseen objects in unseen environments and against unseen\nbackgrounds. We find that RT-2 and RT-2-X perform roughly\non par (Table II, rows (1) and (2), last column). This is not\nunexpected, since RT-2 already generalizes well (see [9])\nalong these dimensions due to its VLM backbone.\nEmergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like']","Results are shown in Table II, Emergent Skills Evaluation column. Comparing rows (1) and (2), we find that RT-2-X outperforms RT-2 by ∼3×, suggesting that incorporating data from other robots into the training improves the range of tasks that can be performed even by a robot that already has large amounts of data available.",simple,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
What is the purpose of creating a catalogue of failure points in RAG systems?,"['is an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperiment involved 15,000 documents and 1000 question\narXiv:2401.05856v1  [cs.SE]  11 Jan 2024\nCAIN 2024, April 2024, Lisbon, Portugal\nScott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\nand answer pairs. We indexed all documents then ran the\nqueries and stored the generated responses using GPT-4. All\nquestion and answer pairs were then validated with OpenAI\nevals 1. Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience report from 3 case studies of implementing a\nRAG system. Two currently running at Deakin University.\n• A research direction for RAG systems based on the lessons\nlearned from the 3 case studies.\n2\nRELATED WORK\nRetrieval augmented generation encompasses using documents\nto augment large language models through pre-training and at\ninference time [7, 9, 12]. Due to the compute cost, data preparation\ntime and required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existing work by presenting challenges faced during the\nimplementation of a RAG system with a focus on practitioners.\nErrors and failures that arise from RAG systems overlap with\nother information retrieval systems including 1) no metrics for\nquery rewriting, 2) document re-ranking, and 3) effective content\nsummarisation [19]. Our results confirm this The unique aspects\nare related to the semantic and generative nature of the use of large']",The purpose of creating a catalogue of failure points in RAG systems is to identify the patterns of failure and challenges faced during the implementation of RAG systems. This helps engineers understand the issues they may encounter and guides software engineering research to find solutions for improving RAG systems.,simple,"[{'Authors': 'Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek', 'Published': '2024-01-11', 'Summary': 'Software engineers are increasingly adding semantic search capabilities to\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\nRAG system involves finding documents that semantically match a query and then\npassing the documents to a large language model (LLM) such as ChatGPT to\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\nproblem of hallucinated responses from LLMs, b) link sources/references to\ngenerated responses, and c) remove the need for annotating documents with\nmeta-data. However, RAG systems suffer from limitations inherent to information\nretrieval systems and from reliance on LLMs. In this paper, we present an\nexperience report on the failure points of RAG systems from three case studies\nfrom separate domains: research, education, and biomedical. We share the\nlessons learned and present 7 failure points to consider when designing a RAG\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\nsystem is only feasible during operation, and 2) the robustness of a RAG system\nevolves rather than designed in at the start. We conclude with a list of\npotential research directions on RAG systems for the software engineering\ncommunity.', 'Title': 'Seven Failure Points When Engineering a Retrieval Augmented Generation System'}]",TRUE
What is the application of deep learning approach in traversability estimation?,"['deep learning approach for traversability estimation. In 2018 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 3044–3051. IEEE.\nHutter, M., Gehring, C., Jud, D., Lauber, A., Bellicoso, C. D., Tsounis, V., Hwangbo, J., Bodie, K.,\nFankhauser, P., Bloesch, M., et al. (2016). Anymal-a highly mobile and dynamic quadrupedal robot. In\nIROS, pages 38–44. IEEE.\nKim, D., Sun, J., Oh, S. M., Rehg, J. M., and Bobick, A. F. (2006). Traversability classiﬁcation using\nunsupervised on-line visual learning for outdoor robot navigation. In IEEE International Conference\non Robotics and Automation (ICRA), pages 518–525. IEEE.\nKr¨\nusi, P., Furgale, P., Bosse, M., and Siegwart, R. (2017).\nDriving on point clouds: Motion planning,\ntrajectory optimization, and terrain assessment in generic nonplanar environments. Journal of Field\nRobotics, 34(5):940–984.\nKulkarni, M., Dharmadhikari, M., Tranzatto, M., Zimmermann, S., Reijgwart, V., De Petris, P., Nguyen, H.,\nKhedekar, N., Papachristos, C., Ott, L., Siegwart, R., Hutter, M., and Alexis, K. (2022). Autonomous\nteamed exploration of subterranean environments using legged and aerial robots. In IEEE International\nConference on Robotics and Automation (ICRA).\nLin, Y.-C. and Berenson, D. (2017). Humanoid navigation in uneven terrain using learned estimates of\ntraversability. In 2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids),\npages 9–16. IEEE.\nMiki, T., Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., and Hutter, M. (2022a).\nLearning robust\nperceptive locomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822.\nMiki, T., Wellhausen, L., Grandia, R., Jenelten, F., Homberger, T., and Hutter, M. (2022b). Elevation\nmapping for locomotion and navigation using gpu. arXiv preprint arXiv:2204.12876.\nNorby, J. and Johnson, A. M. (2020). Fast global motion planning for dynamic legged robots. 2020 IEEE\nInternational Conference on Intelligent Robots and Systems (IROS).\nOhradzansky, M. T., Rush, E. R., Riley, D. G., Mills, A. B., Ahmad, S., McGuire, S., Biggie, H., Harlow,\nK., Miles, M. J., Frew, E. W., Heckman, C., and Humbert, J. S. (2021).\nMulti-agent autonomy:\nAdvancements and challenges in subterranean exploration. arXiv preprint arXiv:2110.04390.\nOleynikova, H., Taylor, Z., Fehr, M., Siegwart, R., and Nieto, J. (2017). Voxblox: Incremental 3d euclidean\nsigned distance ﬁelds for on-board mav planning. In IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 1366–1373. IEEE.\nOtsu, K., Ono, M., Fuchs, T. J., Baldwin, I., and Kubota, T. (2016). Autonomous terrain classiﬁcation with\nco-and self-training approach. IEEE Robotics and Automation Letters, 1(2):814–819.\nReid, W., Fitch, R., G¨\nokto˘\ngan, A. H., and Sukkarieh, S. (2020). Sampling-based hierarchical motion plan-\nning for a reconﬁgurable wheel-on-leg planetary analogue exploration rover. Journal of Field Robotics,\n37(5):786–811.']","The application of deep learning approach in traversability estimation is in the field of robotics and autonomous navigation, specifically for estimating the traversability of terrain for robots.",simple,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How does the performance of PG-InstructBLIP scale with dataset size in terms of PhysObjects?,"['88.4\nMaterial\n67.7\n83.4\nTransparency\n81.5\n83.8\nContents\n32.5\n81.6\nCan Contain Liquid\n56.3\n89.1\nIs Sealed\n71.0\n80.6\nAverage\n64.0\n84.1\nTABLE IV: Test accuracy for main concepts with\nparaphrased prompts\nIn Table IV, we report results for main concepts on unseen\nparaphrased question prompts. We find that PG-InstructBLIP\nstill outperforms InstructBLIP, with limited degradation from\nthe original prompts, suggesting robustness to question vari-\nety from using a large pre-trained VLM.\n10\n50\n100\n% Training Data\n65\n70\n75\n80\n85\n90\n% Accuracy\nPhysObjects Scaling\nInstructBLIP\nPG-InstructBLIP (ours)\nFig. 3: Performance scaling\nwith dataset size\nDataset Scaling. In Fig. 3,\nwe illustrate how average\nperformance\nscales\nwith\ndataset size, by fine-tuning\non different fractions of\ndata from PHYSOBJECTS.\nPerformance scales posi-\ntively, but the models still\nbenefit significantly from\nonly 10% of our dataset,\nsuggesting that the phys-\nical reasoning of VLMs\ncan be improved with rel-\natively small amounts of\nannotated data.\nAdditional Results. We include additional results in our Ap-\npendix (found on our website). These include showing that\nPG-InstructBLIP has limited degradation on general VQA\nbenchmarks compared to InstructBLIP, suggesting that ex-\nisting systems using VLMs can benefit from PHYSOBJECTS\nfor physical reasoning, without sacrificing other reasoning\nabilities. We also include results using different question\nprompts, using a smaller version of InstructBLIP, evaluating\non automatically annotated data, transfer to held-out con-\ncepts, and ablations on our fine-tuning process.\nB. Real Scene Planning Evaluation\nNext, we evaluate the efficacy of PG-InstructBLIP for\nrobotic planning on unseen images of real scenes. We provide\nan example scene in Fig. 4. We evaluate on tasks with\nlanguage instructions, and assume a library of primitive\nrobotic operations with language descriptions.\nFig. 4: Example scene in our\nplanning evaluation\nPlanning\nFramework.\nThe\nLLM\nused\nin\nour\nplanning\nframework\nis\nGPT-4\n[38].\nIt\nis\nfirst\ngiven\nobject\ndetections\nin the scene, a list of\nprimitives, and the task\ninstruction, and then asks\na VLM questions about\nobjects in the scene. There\nare no constraints on the\nquestions. Afterwards, the\nLLM either indicates the task is not possible, or produces a\nplan consisting of primitives to execute.\nTask Category\nNo\nVLM\nInstruct-\nBLIP\nPG-InstructBLIP\n(ours)\nSingle Concept\n36.8\n68.4\n84.1\nMulti-Concept\n27.8\n27.8\n94.4\nCommon Knowledge\n35.7\n78.6\n85.7\nOverall\n33.3\n56.9\n88.2\nTABLE V: Task plan accuracy on 51 real scenarios\nResults. We report task planning accuracy using Instruct-\nBLIP and PG-InstructBLIP in Table V. We also compare to\na planner that does not use VLM interaction for grounding.\nWe evaluate on 51 task scenarios across 8 scenes, using\na non-author human to evaluate task plans. We divide our\ntask scenarios into three categories. Single Concept requires\nidentifying objects using one physical concept, e.g., finding']","Performance scales positively with dataset size, by fine-tuning on different fractions of data from PHYSOBJECTS. The models benefit significantly from only 10% of the dataset, suggesting that the physical reasoning of VLMs can be improved with relatively small amounts of annotated data.",simple,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
How does the RT-2-X model perform on Bridge dataset tasks compared to the RT-2 model?,"['Emergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like\nthe ones shown in Fig. 5. These tasks involve objects and\nskills that are not present in the RT-2 dataset but occur in the\nBridge dataset [95] for a different robot (the WidowX robot).\nResults are shown in Table II, Emergent Skills Evaluation\ncolumn. Comparing rows (1) and (2), we find that RT-2-X\noutperforms RT-2 by ∼3×, suggesting that incorporating\ndata from other robots into the training improves the range\nof tasks that can be performed even by a robot that already\nhas large amounts of data available. Our results suggest that\nco-training with data from other platforms imbues the RT-2-\nX controller with additional skills for the platform that are\nnot present in that platform’s original dataset.\nOur next ablation involves removing the Bridge dataset\nfrom RT-2-X training: Row (3) shows the results for RT-2-\nX that includes all data used for RT-2-X except the Bridge\ndataset. This variation significantly reduces performance on\nthe hold-out tasks, suggesting that transfer from the WidowX\ndata may indeed be responsible for the additional skills that\ncan be performed by RT-2-X with the Google Robot.\nC. Design decisions\nLastly, we perform ablations to measure the influence of\ndifferent design decisions on the generalization capabilities\nof our most performant RT-2-X model, which are presented\nin Table II. We note that including a short history of im-\nages significantly improves generalization performance (row\n(4) vs row (5)). Similarly to the conclusions in the RT-2\npaper [9], Web-based pre-training of the model is critical\nto achieving a high performance for the large models (row\n(4) vs row (6)). We also note that the 55B model has\nsignificantly higher success rate in the Emergent Skills com-\npared to the 5B model (row (2) vs row (4)), demonstrating\nthat higher model capacity enables higher degree of transfer\nacross robotic datasets. Contrary to previous RT-2 findings,\nco-fine-tuning and fine-tuning have similar performance in\nboth the Emergent Skills and Generalization Evaluation (row\n(4) vs row (7)), which we attribute to the fact that the robotics\ndata used in RT-2-X is much more diverse than the previously\nused robotics datasets.\nFig. 5: To assess transfer between embodiments, we evaluate the\nRT-2-X model on out-of-distribution skills. These skills are in\nthe Bridge dataset, but not in the Google Robot dataset (the\nembodiment they are evaluated on).\nVI. DISCUSSION, FUTURE WORK, AND OPEN PROBLEMS\nWe presented a consolidated dataset that combines data\nfrom 22 robotic embodiments collected through a collab-\noration between 21 institutions, demonstrating 527 skills\n(160266 tasks). We also presented an experimental demon-\nstration that Transformer-based policies trained on this data\ncan exhibit significant positive transfer between the different'
 'dataset. This constitutes a reasonable baseline insofar as\nit can be expected that the model has been optimized to\nwork well with the associated data; we refer to this baseline\nmodel as the Original Method model. (2) An RT-1 model\ntrained on the dataset in isolation; this baseline allows us to\nassess whether the RT-X model architectures have enough\ncapacity to represent policies for multiple different robot\nplatforms simultaneously, and whether co-training on multi-\nembodiment data leads to higher performance.\nSmall-scale dataset domains (Fig. 4). RT-1-X outper-\nforms Original Method trained on each of the robot-specific\ndatasets on 4 of the 5 datasets, with a large average im-\nprovement, demonstrating domains with limited data benefit\nsubstantially from co-training on X-embodiment data.\nLarge-scale dataset domains (Table I). In the large-\ndataset setting, the RT-1-X model does not outperform\nthe RT-1 baseline trained on only the embodiment-specific\ndataset, which indicates underfitting for that model class.\nHowever, the larger RT-2-X model outperforms both the\nOriginal Method and RT-1 suggesting that X-robot training\ncan improve performance in the data-rich domains, but only\nwhen utilizing a sufficiently high-capacity architecture.\nB. Improved generalization to out-of-distribution settings\nWe now examine how X-embodiment training can enable\nbetter generalization to out-of-distribution settings and more\ncomplex and novel instructions. These experiments focus on\nthe high-data domains, and use the RT-2-X model.\nRow\nModel\nSize\nHistory Length\nDataset\nCo-Trained w/ Web\nInitial Checkpoint\nEmergent Skills Evaluation\nRT-2 Generalization Evaluation\n(1)\nRT-2\n55B\nnone\nGoogle Robot action\nYes\nWeb-pretrained\n27.3%\n62%\n(2)\nRT-2-X\n55B\nnone\nRobotics data\nYes\nWeb-pretrained\n75.8%\n61%\n(3)\nRT-2-X\n55B\nnone\nRobotics data except Bridge\nYes\nWeb-pretrained\n42.8%\n54%\n(4)\nRT-2-X\n5B\n2\nRobotics data\nYes\nWeb-pretrained\n44.4%\n52%\n(5)\nRT-2-X\n5B\nnone\nRobotics data\nYes\nWeb-pretrained\n14.5%\n30%\n(6)\nRT-2-X\n5B\n2\nRobotics data\nNo\nFrom scratch\n0%\n1%\n(7)\nRT-2-X\n5B\n2\nRobotics data\nNo\nWeb-pretrained\n48.7%\n47%\nTABLE II: Ablations to show the impact of design decisions on generalization (to unseen objects, backgrounds, and environments) and\nemergent skills (skills from other datasets on the Google Robot), showing the importance of Web-pretraining, model size, and history.\nUnseen objects, backgrounds and environments. We\nfirst conduct the same evaluation of generalization properties\nas proposed in [9], testing for the ability to manipulate\nunseen objects in unseen environments and against unseen\nbackgrounds. We find that RT-2 and RT-2-X perform roughly\non par (Table II, rows (1) and (2), last column). This is not\nunexpected, since RT-2 already generalizes well (see [9])\nalong these dimensions due to its VLM backbone.\nEmergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like']","RT-2-X outperforms RT-2 by ∼3× on Bridge dataset tasks, suggesting that incorporating data from other robots into the training improves the range of tasks that can be performed.",reasoning,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
"Can X-embodiment training help develop a universal robot policy, overcoming the need for separate models for each application or environment in robot learning?","['remains whether it is possible to train a model in the field of\nrobotics that can absorb knowledge from other fields. Could\nthe model demonstrate zero-shot generalization capabilities\nfor new tasks? Robotics Transformer 1 (RT-1) [10] was\nproposed to address the aforementioned question. RT-1 is\ncapable of encoding high-dimensional input and output data,\nincluding images and instructions, into compact tokens that\ncan be efficiently processed by Transformer [131]. It exhibits\nreal-time operation characteristics, making it suitable for ap-\nplications that require rapid processing and response times.\nIn experimental evaluations, RT-1 demonstrated strong gen-\neralization. The structure of RT-1 is composed of FiLM\n[96], conditioned EfficientNet [124], a TokenLearner [107],\nand Transformer [131]. However, RT-1 is not an end-to-end\nmodel.\nRobotics transformer 2. Can we pre-train a vision-\nlanguage model (VLM) [22, 34] that can be seamlessly inte-\ngrated into low-level robot control? Hereby enhancing VLM\ngeneralization capabilities? We can achieve this by training\nthe robot’s trajectory to be represented as a sequence of to-\nkens, effectively mapping natural language instructions into\na series of robot actions. To create an end-to-end model that\ncan directly map robot observations into actions, DeepMind\nemploys a collaborative fine-tuning approach. Combining\nstate-of-the-art VLMs with network-scale visual-language\ntasks on robot trajectory data, Robot Transformer 2 (RT-2)\n[9] is a model that leverages fine-tuning of a VLM. RT-2 is\ntrained on a web-scale dataset to achieve direct possession of\ngeneralization ability and semantic awareness for new tasks.\nThrough fine-tuning a VLM, it is adapted to generate actions\nbased on text encoding. Specifically, the model is trained\non a dataset that incorporates action-related text tokens.\nThis type of model can be called a visual-language-action\nmodel (VLA) [9]. RT-2 builds upon the policy trained by\nRobotic Transformer 1 (RT-1) [10], leveraging the same\ndataset and an expanded VLA to significantly enhance the\nmodel’s generalization capabilities for new tasks.\nRobotics transformer X. In robot learning, it is com-\nmon to train a separate large model for each application\nor environment. However, this approach can be limiting, as\nit may not allow for adaptability across different robots or\nenvironments. Can we develop a robot policy that is versatile\nand can be applied across various robots and environments?\nWith the advancements in large models, it is within the realm\nof possibility to train a versatile model that exhibits strong\ngeneralization capabilities for a specific task. Inspired by\nthese large models, X-embodiment training3 is proposed,\nwhich involves using robot data from diverse platforms for\ntraining. This approach enables the model to better adapt to\nchanges in both the robot and the environment, leading to\nimproved performance and versatility. Robotics Transformer']","X-embodiment training is proposed to develop a versatile model that exhibits strong generalization capabilities for a specific task. It involves using robot data from diverse platforms for training, enabling the model to better adapt to changes in both the robot and the environment. This approach helps overcome the need for separate models for each application or environment in robot learning.",reasoning,"[{'Authors': 'Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S. Yu', 'Published': '2023-11-13', 'Summary': 'The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.', 'Title': 'Large Language Models for Robotics: A Survey'}]",TRUE
What are the results of the RT-2-X model experiments on tasks involving objects and skills from both RT-2 and Bridge datasets?,"['Emergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like\nthe ones shown in Fig. 5. These tasks involve objects and\nskills that are not present in the RT-2 dataset but occur in the\nBridge dataset [95] for a different robot (the WidowX robot).\nResults are shown in Table II, Emergent Skills Evaluation\ncolumn. Comparing rows (1) and (2), we find that RT-2-X\noutperforms RT-2 by ∼3×, suggesting that incorporating\ndata from other robots into the training improves the range\nof tasks that can be performed even by a robot that already\nhas large amounts of data available. Our results suggest that\nco-training with data from other platforms imbues the RT-2-\nX controller with additional skills for the platform that are\nnot present in that platform’s original dataset.\nOur next ablation involves removing the Bridge dataset\nfrom RT-2-X training: Row (3) shows the results for RT-2-\nX that includes all data used for RT-2-X except the Bridge\ndataset. This variation significantly reduces performance on\nthe hold-out tasks, suggesting that transfer from the WidowX\ndata may indeed be responsible for the additional skills that\ncan be performed by RT-2-X with the Google Robot.\nC. Design decisions\nLastly, we perform ablations to measure the influence of\ndifferent design decisions on the generalization capabilities\nof our most performant RT-2-X model, which are presented\nin Table II. We note that including a short history of im-\nages significantly improves generalization performance (row\n(4) vs row (5)). Similarly to the conclusions in the RT-2\npaper [9], Web-based pre-training of the model is critical\nto achieving a high performance for the large models (row\n(4) vs row (6)). We also note that the 55B model has\nsignificantly higher success rate in the Emergent Skills com-\npared to the 5B model (row (2) vs row (4)), demonstrating\nthat higher model capacity enables higher degree of transfer\nacross robotic datasets. Contrary to previous RT-2 findings,\nco-fine-tuning and fine-tuning have similar performance in\nboth the Emergent Skills and Generalization Evaluation (row\n(4) vs row (7)), which we attribute to the fact that the robotics\ndata used in RT-2-X is much more diverse than the previously\nused robotics datasets.\nFig. 5: To assess transfer between embodiments, we evaluate the\nRT-2-X model on out-of-distribution skills. These skills are in\nthe Bridge dataset, but not in the Google Robot dataset (the\nembodiment they are evaluated on).\nVI. DISCUSSION, FUTURE WORK, AND OPEN PROBLEMS\nWe presented a consolidated dataset that combines data\nfrom 22 robotic embodiments collected through a collab-\noration between 21 institutions, demonstrating 527 skills\n(160266 tasks). We also presented an experimental demon-\nstration that Transformer-based policies trained on this data\ncan exhibit significant positive transfer between the different'
 'dataset. This constitutes a reasonable baseline insofar as\nit can be expected that the model has been optimized to\nwork well with the associated data; we refer to this baseline\nmodel as the Original Method model. (2) An RT-1 model\ntrained on the dataset in isolation; this baseline allows us to\nassess whether the RT-X model architectures have enough\ncapacity to represent policies for multiple different robot\nplatforms simultaneously, and whether co-training on multi-\nembodiment data leads to higher performance.\nSmall-scale dataset domains (Fig. 4). RT-1-X outper-\nforms Original Method trained on each of the robot-specific\ndatasets on 4 of the 5 datasets, with a large average im-\nprovement, demonstrating domains with limited data benefit\nsubstantially from co-training on X-embodiment data.\nLarge-scale dataset domains (Table I). In the large-\ndataset setting, the RT-1-X model does not outperform\nthe RT-1 baseline trained on only the embodiment-specific\ndataset, which indicates underfitting for that model class.\nHowever, the larger RT-2-X model outperforms both the\nOriginal Method and RT-1 suggesting that X-robot training\ncan improve performance in the data-rich domains, but only\nwhen utilizing a sufficiently high-capacity architecture.\nB. Improved generalization to out-of-distribution settings\nWe now examine how X-embodiment training can enable\nbetter generalization to out-of-distribution settings and more\ncomplex and novel instructions. These experiments focus on\nthe high-data domains, and use the RT-2-X model.\nRow\nModel\nSize\nHistory Length\nDataset\nCo-Trained w/ Web\nInitial Checkpoint\nEmergent Skills Evaluation\nRT-2 Generalization Evaluation\n(1)\nRT-2\n55B\nnone\nGoogle Robot action\nYes\nWeb-pretrained\n27.3%\n62%\n(2)\nRT-2-X\n55B\nnone\nRobotics data\nYes\nWeb-pretrained\n75.8%\n61%\n(3)\nRT-2-X\n55B\nnone\nRobotics data except Bridge\nYes\nWeb-pretrained\n42.8%\n54%\n(4)\nRT-2-X\n5B\n2\nRobotics data\nYes\nWeb-pretrained\n44.4%\n52%\n(5)\nRT-2-X\n5B\nnone\nRobotics data\nYes\nWeb-pretrained\n14.5%\n30%\n(6)\nRT-2-X\n5B\n2\nRobotics data\nNo\nFrom scratch\n0%\n1%\n(7)\nRT-2-X\n5B\n2\nRobotics data\nNo\nWeb-pretrained\n48.7%\n47%\nTABLE II: Ablations to show the impact of design decisions on generalization (to unseen objects, backgrounds, and environments) and\nemergent skills (skills from other datasets on the Google Robot), showing the importance of Web-pretraining, model size, and history.\nUnseen objects, backgrounds and environments. We\nfirst conduct the same evaluation of generalization properties\nas proposed in [9], testing for the ability to manipulate\nunseen objects in unseen environments and against unseen\nbackgrounds. We find that RT-2 and RT-2-X perform roughly\non par (Table II, rows (1) and (2), last column). This is not\nunexpected, since RT-2 already generalizes well (see [9])\nalong these dimensions due to its VLM backbone.\nEmergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like']","Results are shown in Table II, Emergent Skills Evaluation column. Comparing rows (1) and (2), we find that RT-2-X outperforms RT-2 by ∼3×, suggesting that incorporating data from other robots into the training improves the range of tasks that can be performed even by a robot that already has large amounts of data available.",multi_context,"[{'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}
 {'Authors': 'Open X-Embodiment Collaboration, Abby O\'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi ""Jim"" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick ""Tree"" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin', 'Published': '2024-04-02', 'Summary': 'Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.', 'Title': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models'}]",TRUE
How are objects' visibility determined for bounding box image annotation?,"['Choosing Annotation Images. There are multiple bounding\nbox images in EgoObjects for each object instance. To deter-\nmine which to present for annotating an object, we choose\nthe bounding box with the highest CLIP [22] similarity with\nthe object’s category label, as a heuristic for the object’s\nvisibility. We use the CLIP-ViT-H-14-laion2B-s32B-b79K\nmodel from OpenCLIP [39]. In Fig. 6, we show an example\nof randomly sampled bounding boxes for an instance of\nthe object category guitar, arranged from left-to-right in\ndecreasing order of CLIP similarity. The objects in bounding\nboxes with lower CLIP similarities tend to be less visible.\nFig. 6: Bounding boxes for an instance of guitar, in\ndecreasing order of CLIP similarity\nAttention Checks. We generate attention checks for crowd-\nworkers by randomly sampling from the automatic annota-\ntions, which have known labels. For the concepts contents,\ndensity, and liquid capacity, for which there are no automatic\nannotations, we manually label a small set of objects for\nattention checks.\nOther Details. Each annotation job on Prolific consisted\nof 250 annotations for a single concept, of which 25 are\nattention checks. Participants were paid an average of 15.50\nUS dollars per hour, and each annotation job took on average\n20-30 minutes to complete, depending on the concept.\nConcept\nHigh\nLow\nMass\ntelevision, microwave oven, table,\nnightstand, chest of drawers\npen, paper, spoon, fork, glasses,\nsunglasses, scissors, watch, neck-\nlace, house/car key, pencil, earrings,\nring, screwdriver, book, container,\nplate, bowl, pillow, remote control,\nclothing, mug, laptop, knife, mobile\nphone, toy, computer mouse, wa-\nter glass, towel, headphones, spatula,\nfrying pan, measuring cup, banana,\nwallet, blanket, candle, apple, wine\nglass, picture frame, computer key-\nboard, game controller/pad, tea cup,\ntin can, handbag, whisk, orange, belt,\nplastic bag, salt and pepper shak-\ners, cutting board, perfume, stapler,\nfootwear, tablet coputer, teddy bear,\ncookie, scarf, coffee cup, ball, mix-\ning bowl, pear, alarm clock, light\nswitch, bread, jacket, tennis ball, san-\ndal, saucer, laptop charger, camera,\nyoga mat, power plugs and sock-\nets, cream, shirt, baseball bat, sun\nhat, paper towel, kitchen knife, doll,\ncan opener, sock, facial tissue holder,\nboot, hair dryer\nFragility\nwater glass, television\nhouse/car\nkey,\ndumbbell,\nscrew-\ndriver, kitchen knife\nDeformability\npillow, clothing, towel, blanket, belt,\nplastic bag, scarf, jacket, yoga mat,\nshirt, paper towel, sock\nremote control, mug, mobile phone,\ncomputer mouse, water glass, fry-\ning pan, flowerpot, scissors, wine\nglass, house/car key, dumbbell, cut-\nting board, microwave oven, toaster,\nblender,\npressure\ncooker,\nkitchen\nknife, table, spoon, laptop, knife,\nfork, glasses, spatula, sunglasses,\nchair, measuring cup, pencil, picture\nframe,\ncomputer\nkeyboard,\ngame\ncontroller/pad, tea cup, tin can, salt\nand pepper shakers, television, cof-\nfeemaker, stapler, tablet computer,'
 'Choosing Annotation Images. There are multiple bounding\nbox images in EgoObjects for each object instance. To deter-\nmine which to present for annotating an object, we choose\nthe bounding box with the highest CLIP [22] similarity with\nthe object’s category label, as a heuristic for the object’s\nvisibility. We use the CLIP-ViT-H-14-laion2B-s32B-b79K\nmodel from OpenCLIP [39]. In Fig. 6, we show an example\nof randomly sampled bounding boxes for an instance of\nthe object category guitar, arranged from left-to-right in\ndecreasing order of CLIP similarity. The objects in bounding\nboxes with lower CLIP similarities tend to be less visible.\nFig. 6: Bounding boxes for an instance of guitar, in\ndecreasing order of CLIP similarity\nAttention Checks. We generate attention checks for crowd-\nworkers by randomly sampling from the automatic annota-\ntions, which have known labels. For the concepts contents,\ndensity, and liquid capacity, for which there are no automatic\nannotations, we manually label a small set of objects for\nattention checks.\nOther Details. Each annotation job on Prolific consisted\nof 250 annotations for a single concept, of which 25 are\nattention checks. Participants were paid an average of 15.50\nUS dollars per hour, and each annotation job took on average\n20-30 minutes to complete, depending on the concept.\nConcept\nHigh\nLow\nMass\ntelevision, microwave oven, table,\nnightstand, chest of drawers\npen, paper, spoon, fork, glasses,\nsunglasses, scissors, watch, neck-\nlace, house/car key, pencil, earrings,\nring, screwdriver, book, container,\nplate, bowl, pillow, remote control,\nclothing, mug, laptop, knife, mobile\nphone, toy, computer mouse, wa-\nter glass, towel, headphones, spatula,\nfrying pan, measuring cup, banana,\nwallet, blanket, candle, apple, wine\nglass, picture frame, computer key-\nboard, game controller/pad, tea cup,\ntin can, handbag, whisk, orange, belt,\nplastic bag, salt and pepper shak-\ners, cutting board, perfume, stapler,\nfootwear, tablet coputer, teddy bear,\ncookie, scarf, coffee cup, ball, mix-\ning bowl, pear, alarm clock, light\nswitch, bread, jacket, tennis ball, san-\ndal, saucer, laptop charger, camera,\nyoga mat, power plugs and sock-\nets, cream, shirt, baseball bat, sun\nhat, paper towel, kitchen knife, doll,\ncan opener, sock, facial tissue holder,\nboot, hair dryer\nFragility\nwater glass, television\nhouse/car\nkey,\ndumbbell,\nscrew-\ndriver, kitchen knife\nDeformability\npillow, clothing, towel, blanket, belt,\nplastic bag, scarf, jacket, yoga mat,\nshirt, paper towel, sock\nremote control, mug, mobile phone,\ncomputer mouse, water glass, fry-\ning pan, flowerpot, scissors, wine\nglass, house/car key, dumbbell, cut-\nting board, microwave oven, toaster,\nblender,\npressure\ncooker,\nkitchen\nknife, table, spoon, laptop, knife,\nfork, glasses, spatula, sunglasses,\nchair, measuring cup, pencil, picture\nframe,\ncomputer\nkeyboard,\ngame\ncontroller/pad, tea cup, tin can, salt\nand pepper shakers, television, cof-\nfeemaker, stapler, tablet computer,']","To determine which bounding box image to present for annotating an object, the one with the highest CLIP similarity with the object's category label is chosen as a heuristic for the object's visibility.",multi_context,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
"How does the safety margin affect the robot's path planning in risky areas, like the SubT Station platform?","['even though the actual terrain was too rough and rocky to overcome. Using just reachability checking, our\nsampler found individual valid poses on the slope and connected them, as shown in Figure 9(a). Here, risk\npruning identiﬁed that moving on this slope was too risky and removed these edges, as shown in Figure 9(b).\nWithout risk pruning the robot would have tried to scale this slope, which would in the best case have lead\nto lost time, and in the worst case to loss of this robot.\nFigure 7\n+\nshow that the cost function generally lead to safer paths, which kept a safe distance from\n(a) Without risk pruning\n(b) With risk pruning\nFigure 9: Sampled valid poses shown in red with blue graph edges connecting them. Using reachability checking,\nvalid robot poses are still found on a steep and rocky incline in the cave section. Pruning graph edges based on\nmotion risk prevents the planning graph from spanning this risky area.\n(a) Onboard Image\n(b) Exploration Path\n(c) No Safety Threshold\n(d) With Safety Threshold\nEdge\nEdge\nEdge\nEdge\nFigure 10: (a) The SubT Station platform had a sharp edge with a signiﬁcant drop. (b) GBPlanner2 was tuned to be\noptimistic and planned over the edge of the SubT Station platform. (c) Without a foothold safety margin the robot\nwould have stepped onto and possibly over the platform edge. (d) With foothold safety margin the ﬁnal path pose is\na safe distance from the platform edge. Yellow cubes represent our lidar map to indicate the actual location of the\nedge. The ANYmal model in (c)+(d) does not indicate the current robot pose, but rather is placed at the ﬁnal pose\nof the path to better show how close the robot would have stepped to the edge.\nobstacles. This can be attributed to the risk term cr in the cost function. The time cost ct had negligible\nimpact compared to the shortest path of the No Motion Cost planner, since the terrain present during the\nFinals was uniform enough such that the shortest path generally was also the fastest. However, ct was\nnecessary to condition the planning problem, since a pure risk cost would have lead to large detours to\nachieve minor risk improvements.\n3.2.2\nSafety Threshold Analysis\nThe safety threshold was introduced to handle negative obstacles. One carrier robot reached the Subway\nStation in autonomous exploration mode during the ﬁrst Preliminary Run of the Finals. The Subway Station\nhad a sharp drop with a wall a few meters behind, pictured in Figure 10(a). As discussed in Section 1.5, the\nexploration planner was tuned to be optimistic, and planned to explore into the free space above the train\ntracks (Figure 10(b)). With the wall visible behind the platform, both image inpainting and virtual surfaces\nwould have simply created ﬂat ground or a gentle slope such that we could not rely on motion cost for safety.\nReachability checking generally prevents the planner from planning over the edge, however, without a safety']","The safety margin affects the robot's path planning in risky areas by ensuring that the final path pose is a safe distance from the platform edge. It prevents the robot from stepping onto or possibly over the platform edge, providing a buffer for safety.",reasoning,"[{'Authors': 'Lorenz Wellhausen, Marco Hutter', 'Published': '2023-03-02', 'Summary': 'Due to the highly complex environment present during the DARPA Subterranean\nChallenge, all six funded teams relied on legged robots as part of their\nrobotic team. Their unique locomotion skills of being able to step over\nobstacles require special considerations for navigation planning. In this work,\nwe present and examine ArtPlanner, the navigation planner used by team CERBERUS\nduring the Finals. It is based on a sampling-based method that determines valid\nposes with a reachability abstraction and uses learned foothold scores to\nrestrict areas considered safe for stepping. The resulting planning graph is\nassigned learned motion costs by a neural network trained in simulation to\nminimize traversal time and limit the risk of failure. Our method achieves\nreal-time performance with a bounded computation time. We present extensive\nexperimental results gathered during the Finals event of the DARPA Subterranean\nChallenge, where this method contributed to team CERBERUS winning the\ncompetition. It powered navigation of four ANYmal quadrupeds for 90 minutes of\nautonomous operation without a single planning or locomotion failure.', 'Title': 'ArtPlanner: Robust Legged Robot Navigation in the Field'}]",TRUE
How do the challenges of prompt engineering impact the assessment of LLMs in the functionality and cybersecurity domains?,"['• LegalBench [23]: The study outlined challenges in\nprompt engineering, notably in crafting prompts that ac-\n24\ncurately assess LLMs’ legal reasoning capabilities with-\nout introducing bias, thus impacting benchmark integrity.\nThe study acknowledged the complexity of legal language\nand the ongoing effort to reﬁne evaluation techniques,\nindicating an attempt to address benchmark inadequacies\nbut also highlighting the unresolved nature of prompt\nengineering challenges.\n• FLUE [24]: The benchmark development focused on\ncreating assessments across various NLP tasks in the\nﬁnancial domain, but did not explicitly mention efforts\nto mitigate or acknowledge the intricacies involved in\ncrafting unbiased and effective prompts to accurately\ngauge LLM capabilities.\n• MultiMedQA [8]: The study acknowledged the difﬁculty\nin creating prompts that accurately assess LLMs without\nintroducing biases or misinterpretations, affecting both\nfunctionality and cybersecurity domains. It attempted\nto address this through instruction prompt tuning, aim-\ning to align LLMs more closely with medical domain\nrequirements, but the challenge remains complex and\nunresolved, indicating ongoing issues with prompt en-\ngineering adequacy in benchmark assessments.\n• M3KE [7]: The study utilized a uniﬁed prompt for all\nmodels across different settings without detailing efforts\nto mitigate biases or inaccuracies in prompt formulation,\nimpacting the models’ evaluation accuracy and poten-\ntially overlooking the complexity of assessing LLMs’ true\ncapabilities. The study did not explicitly acknowledge or\naddress this benchmark inadequacy.\n• T-Bench [25]: The study developed ToolBench, a bench-\nmark to evaluate open-source LLMs for tool manipula-\ntion tasks, employing techniques like model alignment,\ndemonstration retrieval, and generation regulation with\nsystem prompts. However, it did not explicitly address\nthe intricacies of crafting unbiased and effective prompts\nto accurately measure a model’s capabilities. The study\nacknowledged the importance of prompt engineering by\nincorporating system prompts designed to guide model\ngeneration, yet it did not detail efforts to address or\nmitigate the potential biases and limitations inherent in\nprompt design.\n• Chain-of-Thought Hub [26]: The study introduced the\nChain-of-Thought Hub to measure reasoning capabilities\nof LLMs using a suite of reasoning benchmarks without\naddressing the intricacy of crafting prompts that accu-\nrately assess these capabilities without bias, which was\ncrucial for fair evaluation. The study acknowledged the\nchallenge of evaluating complex reasoning capabilities\nin LLMs, but did not speciﬁcally mention attempts to\naddress the prompt engineering\n• ARB [28]: The study introduced a novel benchmark that\nincluded advanced reasoning problems and proposed a\nrubric-based self-evaluation method for assessing LLMs,\nacknowledging the difﬁculty in prompt engineering, but\ndid not provide a conclusive solution to this challenge.'
 'rubric-based self-evaluation method for assessing LLMs,\nacknowledging the difﬁculty in prompt engineering, but\ndid not provide a conclusive solution to this challenge.\n• BIG-Bench [4]: The study acknowledged the difﬁculty in\ncrafting prompts that accurately reﬂected a model’s capa-\nbilities without introducing biases or misinterpretations.\nHowever, it did not speciﬁcally address or propose solu-\ntions to overcome those challenges in prompt engineer-\ning, highlighting an ongoing inadequacy in effectively\nevaluating LLMs through benchmarks.\n• HELM [31]: The study utilized a standardized few-\nshot prompting adaptation for all models, indicating an\nawareness of prompt engineering challenges. However, it\nalso highlighted the sensitivity of model performance to\nprompt formatting and adaptation methods, revealing an\nongoing struggle with crafting prompts that accurately\nassess model capabilities without introducing biases or\ninaccuracies. The benchmark acknowledged this inade-\nquacy by discussing the variation in model performance\nbased on different prompting strategies.\n• AgentBench [33]: The study designed and implemented\nAgentBench to evaluate LLMs as agents across various\nenvironments, including code-grounded, game-grounded,\nand web-grounded scenarios, without explicitly address-\ning the challenges of crafting unbiased and representative\nprompts that accurately assess a model’s capabilities. This\nomission suggested that the study might not fully account\nfor the complexities of prompt engineering, potentially\naffecting the accuracy of its evaluations. The benchmark\nstudy acknowledged the need for systematic evaluation\nof LLMs as agents, but did not speciﬁcally address\nor attempt to mitigate the inadequacies associated with\nprompt engineering.\n• C-Eval [35]: The study implemented a comprehensive\nevaluation of LLMs on C-EVAL, including both answer-\nonly and chain-of-thought settings, without explicitly\naddressing or mitigating the intricacies of prompt engi-\nneering. Although the study detailed the creation and ap-\nplication of C-EVAL for evaluating LLMs across various\ndisciplines and difﬁculty levels, it did not discuss speciﬁc\nmeasures to ensure the prompts accurately and effectively\nelicit the models’ capabilities without introducing biases\nor misinterpretations. The benchmark study recognized\nthe importance of evaluating advanced abilities of LLMs\nin a Simpliﬁed Chinese context but did not explicitly\nmention efforts to address the prompt engineering chal-\nlenge.\n• BOLAA [36]: The study did not speciﬁcally address or\nattempt to mitigate the intricacies of prompt engineering\nfor accurately and effectively eliciting the capabilities\nof language models. Instead, it focused on evaluating\nthe performance of various LAA architectures and their\norchestration without delving into the effects of prompt\ndesign on benchmark outcomes. The study acknowledged\nthe importance of prompt engineering indirectly by ex-']","The challenges of prompt engineering impact the assessment of LLMs in the functionality and cybersecurity domains by introducing biases and misinterpretations, affecting both the accuracy and effectiveness of evaluating LLMs in these domains. The studies mentioned in the context acknowledge the difficulty in creating prompts that accurately assess LLMs without introducing biases or misinterpretations, highlighting the ongoing issues with prompt engineering adequacy in benchmark assessments.",simple,"[{'Authors': 'Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Malka N. Halgamuge', 'Published': '2024-02-15', 'Summary': ""The rapid rise in popularity of Large Language Models (LLMs) with emerging\ncapabilities has spurred public curiosity to evaluate and compare different\nLLMs, leading many researchers to propose their LLM benchmarks. Noticing\npreliminary inadequacies in those benchmarks, we embarked on a study to\ncritically assess 23 state-of-the-art LLM benchmarks, using our novel unified\nevaluation framework through the lenses of people, process, and technology,\nunder the pillars of functionality and security. Our research uncovered\nsignificant limitations, including biases, difficulties in measuring genuine\nreasoning, adaptability, implementation inconsistencies, prompt engineering\ncomplexity, evaluator diversity, and the overlooking of cultural and\nideological norms in one comprehensive assessment. Our discussions emphasized\nthe urgent need for standardized methodologies, regulatory certainties, and\nethical guidelines in light of Artificial Intelligence (AI) advancements,\nincluding advocating for an evolution from static benchmarks to dynamic\nbehavioral profiling to accurately capture LLMs' complex behaviors and\npotential risks. Our study highlighted the necessity for a paradigm shift in\nLLM evaluation methodologies, underlining the importance of collaborative\nefforts for the development of universally accepted benchmarks and the\nenhancement of AI systems' integration into society."", 'Title': 'Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence'}
 {'Authors': 'Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Malka N. Halgamuge', 'Published': '2024-02-15', 'Summary': ""The rapid rise in popularity of Large Language Models (LLMs) with emerging\ncapabilities has spurred public curiosity to evaluate and compare different\nLLMs, leading many researchers to propose their LLM benchmarks. Noticing\npreliminary inadequacies in those benchmarks, we embarked on a study to\ncritically assess 23 state-of-the-art LLM benchmarks, using our novel unified\nevaluation framework through the lenses of people, process, and technology,\nunder the pillars of functionality and security. Our research uncovered\nsignificant limitations, including biases, difficulties in measuring genuine\nreasoning, adaptability, implementation inconsistencies, prompt engineering\ncomplexity, evaluator diversity, and the overlooking of cultural and\nideological norms in one comprehensive assessment. Our discussions emphasized\nthe urgent need for standardized methodologies, regulatory certainties, and\nethical guidelines in light of Artificial Intelligence (AI) advancements,\nincluding advocating for an evolution from static benchmarks to dynamic\nbehavioral profiling to accurately capture LLMs' complex behaviors and\npotential risks. Our study highlighted the necessity for a paradigm shift in\nLLM evaluation methodologies, underlining the importance of collaborative\nefforts for the development of universally accepted benchmarks and the\nenhancement of AI systems' integration into society."", 'Title': 'Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence'}]",TRUE
What is the significance of large language models in the field of natural language processing and programming?,"['ciech Zaremba. Evaluating Large Language Models Trained on Code. ArXiv preprint, abs/2107.03374,\n2021b. URL https://arxiv.org/abs/2107.03374.\nChris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O’Boyle, and Hugh\nLeather. ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Op-\ntimizations. In ICLR, 2021.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau\nYih, Luke Zettlemoyer, and Mike Lewis. Incoder: A Generative Model for Code Infilling and Synthesis.\nArXiv preprint, abs/2204.05999, 2022. URL https://arxiv.org/abs/2204.05999.\nSpandan Garg, Roshanak Zilouchian Moghaddam, Colin B. Clement, Neel Sundaresan, and Chen Wu.\nDeepPERF: A Deep Learning-Based Approach For Improving Software Performance, 2022.\nURL\nhttps://arxiv.org/abs/2206.13619.\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P Bigham. Instructdial:\nimproving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pp. 505–525, 2022.\nPatrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to\nprogram better. arXiv preprint arXiv:2207.14502, 2022.\nYoussef Hamadi and Youssef Hamadi. Autonomous Search. Combinatorial Search: From Algorithms to\nSystems, 2013.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nChangwu Huang, Yuanxiang Li, and Xin Yao.\nA Survey of Automatic Parameter Tuning Methods for\nMetaheuristics. IEEE transactions on evolutionary computation, 2019.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data, 7(3):535–547, 2019.\nSam Kaufman, Phitchaya Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Roy, Amit Sabne, and Mike\nBurrows. A Learned Performance Model for Tensor Processing Units. Proceedings of Machine Learning\nand Systems, 2021.\nPascal Kerschke, Holger H Hoos, Frank Neumann, and Heike Trautmann. Automated Algorithm Selection:\nSurvey and Perspectives. Evolutionary computation, 2019.\n11\nPreprint. Under review.\nLars Kotthoff. Algorithm Selection for Combinatorial Search Problems: A Survey. Data mining and con-\nstraint programming: Foundations of a cross-disciplinary approach, 2016.\nCharles E Leiserson, Neil C Thompson, Joel S Emer, Bradley C Kuszmaul, Butler W Lampson, Daniel\nSanchez, and Tao B Schardl. There’s plenty of room at the top: What will drive computer performance\nafter moore’s law? Science, 368(6495):eaam9744, 2020.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-'
 'ciech Zaremba. Evaluating Large Language Models Trained on Code. ArXiv preprint, abs/2107.03374,\n2021b. URL https://arxiv.org/abs/2107.03374.\nChris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O’Boyle, and Hugh\nLeather. ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Op-\ntimizations. In ICLR, 2021.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau\nYih, Luke Zettlemoyer, and Mike Lewis. Incoder: A Generative Model for Code Infilling and Synthesis.\nArXiv preprint, abs/2204.05999, 2022. URL https://arxiv.org/abs/2204.05999.\nSpandan Garg, Roshanak Zilouchian Moghaddam, Colin B. Clement, Neel Sundaresan, and Chen Wu.\nDeepPERF: A Deep Learning-Based Approach For Improving Software Performance, 2022.\nURL\nhttps://arxiv.org/abs/2206.13619.\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P Bigham. Instructdial:\nimproving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pp. 505–525, 2022.\nPatrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to\nprogram better. arXiv preprint arXiv:2207.14502, 2022.\nYoussef Hamadi and Youssef Hamadi. Autonomous Search. Combinatorial Search: From Algorithms to\nSystems, 2013.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nChangwu Huang, Yuanxiang Li, and Xin Yao.\nA Survey of Automatic Parameter Tuning Methods for\nMetaheuristics. IEEE transactions on evolutionary computation, 2019.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transac-\ntions on Big Data, 7(3):535–547, 2019.\nSam Kaufman, Phitchaya Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Roy, Amit Sabne, and Mike\nBurrows. A Learned Performance Model for Tensor Processing Units. Proceedings of Machine Learning\nand Systems, 2021.\nPascal Kerschke, Holger H Hoos, Frank Neumann, and Heike Trautmann. Automated Algorithm Selection:\nSurvey and Perspectives. Evolutionary computation, 2019.\n11\nPreprint. Under review.\nLars Kotthoff. Algorithm Selection for Combinatorial Search Problems: A Survey. Data mining and con-\nstraint programming: Foundations of a cross-disciplinary approach, 2016.\nCharles E Leiserson, Neil C Thompson, Joel S Emer, Bradley C Kuszmaul, Butler W Lampson, Daniel\nSanchez, and Tao B Schardl. There’s plenty of room at the top: What will drive computer performance\nafter moore’s law? Science, 368(6495):eaam9744, 2020.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-']","Large language models have significant significance in the field of natural language processing and programming. They can be used for tasks such as code generation, code completion, and natural language understanding. These models have the ability to understand and generate human-like text, making them valuable for various applications in NLP and programming.",simple,"[{'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}
 {'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}]",TRUE
How does ChatGPT's performance and generalization compare to other models in code generation tasks?,"['LLMs struggle to generalize their abilities to new and unseen problems, i.e., the cold start problem:\nThe model cannot draw any inferences for problems about which it has not yet gathered sufficient\ninformation. Fortunately, when mitigating the impact of data leakage, the results demonstrate\nthat ChatGPT indeed improves the performance and generalization over the prior state of the arts.\nIt can solve most of the easy problems (33/40) and merely 2 hard problems. On the other hand,\nthe efficiency rank of code generated by ChatGPT are still in the top 50% for easy and medium\nproblems while not for hard problems.\nTable 3. Performance of ChatGPT (GPT), Codex (Dex), and CodeGen (Gen) on code generation for the\nproblems in LeetCode 2016-2020.\nLevel\nType\nCorrectness\nAverage Rank\nGPT\nDex\nGen\nGPT\nDex\nGen\nEasy\nArray\n10 (0.96)\n10 (0.28)\n2 (0.04)\n8%\n43%\n37%\nString\n10 (0.94)\n7 (0.24)\n2 (0.04)\n10%\n35%\n36%\nHash Table\n10 (1.00)\n7 (0.36)\n3 (0.14)\n12%\n21%\n23%\nSorting\n10 (0.92)\n8 (0.42)\n2 (0.06)\n7%\n20%\n10%\nTotal\n40 (0.95)\n32 (0.33)\n9 (0.07)\n9%\n31%\n26%\nMedium\nArray\n10 (0.76)\n5 (0.18)\n0 (0.00)\n8%\n34%\n-\nString\n9 (0.72)\n5 (0.16)\n1 (0.02)\n11%\n49%\n82%\nHash Table\n10 (0.76)\n5 (0.20)\n1 (0.02)\n13%\n25%\n89%\nSorting\n10 (0.94)\n5 (0.14)\n1 (0.02)\n11%\n36%\n27%\nTotal\n39 (0.80)\n20 (0.17)\n3 (0.02)\n11%\n36%\n66%\nHard\nArray\n6 (0.46)\n3 (0.08)\n0 (0.00)\n26%\n29%\n-\nString\n8 (0.48)\n2 (0.04)\n0 (0.00)\n29%\n56%\n-\nHash Table\n7 (0.42)\n3 (0.10)\n0 (0.00)\n37%\n42%\n-\nSorting\n5 (0.36)\n1 (0.02)\n0 (0.00)\n12%\n15%\n-\nTotal\n26 (0.43)\n9 (0.06)\n0 (0.00)\n27%\n38%\n-\n∗The integer values in column ‘Correctness’ indicate the number of problems\nfor which the approaches can generate correct solutions within five times\nattempts (TOP-5) given ten programming problems at each type. The decimal\nvalues in parentheses indicate the overall average success rate at five times\nattempts (AVG-5). See randomness in Section 3.4.\n∗The percentage values in column ‘Average Rank’ represent the average rank\npercentile of generated correct solutions. The lower, the better.\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nIs ChatGPT the Ultimate Programming Assistant - How far is it?\n13\nTable 4. Performance of ChatGPT (GPT), Codex (Dex), and CodeGen (Gen) on code generation for the\nproblems in LeetCode 2022.\nLevel\nType\nCorrectness\nAverage Rank\nGPT\nDex\nGen\nGPT\nDex\nGen\nEasy\nArray\n8 (0.74)\n4 (0.12)\n1 (0.02)\n7%\n18%\n18%\nString\n10 (0.50)\n2 (0.10)\n0 (0.00)\n19%\n27%\n-\nHash Table\n7 (0.58)\n3 (0.08)\n1 (0.02)\n7%\n42%\n45%\nSorting\n8 (0.52)\n2 (0.10)\n1 (0.04)\n16%\n21%\n19%']","The results demonstrate that ChatGPT improves the performance and generalization over the prior state of the arts. It can solve most of the easy problems and a few hard problems. However, the efficiency rank of code generated by ChatGPT is not in the top 50% for hard problems.",simple,"[{'Authors': 'Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, Tegawendé F. Bissyandé', 'Published': '2023-08-31', 'Summary': ""Recently, the ChatGPT LLM has received great attention: it can be used as a\nbot for discussing source code, prompting it to suggest changes, provide\ndescriptions or even generate code. Typical demonstrations generally focus on\nexisting benchmarks, which may have been used in model training (i.e., data\nleakage). To assess the feasibility of using an LLM as a useful assistant bot\nfor programmers, we must assess its realistic capabilities on unseen problems\nas well as its capabilities on various tasks. In this paper, we present an\nempirical study of ChatGPT's potential as a fully automated programming\nassistant, focusing on the tasks of code generation, program repair, and code\nsummariziation. The study investigates ChatGPT's performance on common\nprogramming problems and compares it with state-of-the-art approaches on two\nbenchmarks. Among several findings, our study shows that ChatGPT is effective\nin dealing with common programming problems. However, our experiments also\nreveal limitations in terms of its attention span: detailed descriptions will\nconstrain the focus of ChatGPT and prevent it from leveraging its vast\nknowledge to solve the actual problem. Surprisingly, we have identified the\nability of ChatGPT to reason the original intention of the code. We expect\nfuture work to build on this insight for dealing with the open question of the\noracle problem. Our findings contribute interesting insights to the development\nof LLMs for programming assistance, notably by demonstrating the importance of\nprompt engineering, and providing a better understanding of ChatGPT's practical\napplications for software engineering."", 'Title': 'Is ChatGPT the Ultimate Programming Assistant -- How far is it?'}]",TRUE
"What is the topic of the paper ""Rapid locomotion via reinforcement learning""?","['arXiv preprint arXiv:2108.10470, 2021.\n[25] G. B. Margolis, G. Yang, K. Paigwar, T. Chen, and P. Agrawal,\n“Rapid\nlocomotion\nvia\nreinforcement\nlearning,”\narXiv\npreprint\narXiv:2205.02824, 2022.\n[26] S. Mirchandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. G. Arenas,\nK. Rao, D. Sadigh, and A. Zeng, “Large language models as general\npattern machines,” arXiv preprint arXiv:2307.04721, 2023.\n[27] OpenAI, “Gpt-3.5 documentation,” 2023. [Online]. Available: https:\n//platform.openai.com/docs/models/gpt-3-5\n[28] ——, “Gpt-4 technical report,” arXiv, 2023.\n[29] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., “Training language\nmodels to follow instructions with human feedback,” Advances in\nNeural Information Processing Systems, vol. 35, pp. 27 730–27 744,\n2022.\n[30] X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine,\n“Learning agile robotic locomotion skills by imitating animals,” arXiv\npreprint arXiv:2004.00784, 2020.\n[31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable\nvisual models from natural language supervision,” in International\nconference on machine learning.\nPMLR, 2021, pp. 8748–8763.\n[32] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., “Improv-\ning language understanding by generative pre-training,” 2018.\n[33] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[34] U. Robotics, 2023. [Online]. Available: https://unitreerobotics.net/\n[35] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk\nin minutes using massively parallel deep reinforcement learning,” in\nConference on Robot Learning.\nPMLR, 2022, pp. 91–100.\n[36] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[37] D. Shah, B. Osi´\nnski, S. Levine, et al., “Lm-nav: Robotic navigation\nwith large pre-trained models of language, vision, and action,” in\nConference on Robot Learning.\nPMLR, 2023, pp. 492–504.\n[38] L. Smith, J. C. Kew, X. B. Peng, S. Ha, J. Tan, and S. Levine,\n“Legged robots that keep on learning: Fine-tuning locomotion policies\nin the real world,” in 2022 International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2022, pp. 1593–1599.\n[39] T. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths, “Cognitive\narchitectures for language agents,” arXiv preprint arXiv:2309.02427,\n2023.\n[40]']","The topic of the paper ""Rapid locomotion via reinforcement learning"" is rapid locomotion achieved through reinforcement learning.",simple,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What techniques are highly effective for optimizing pre-trained code LLMs based on the PIE benchmark evaluation?,"['LLMs for program optimization remains an open research question. We study whether such LLMs can be\nadapted for performance optimization. To this end, we introduce a novel benchmark for performance opti-\n∗Equal contribution.\n†Now at Google DeepMind\n1The project website can be found at www.pie4perf.com\n1\narXiv:2302.07867v4  [cs.SE]  8 Nov 2023\nPreprint. Under review.\nFigure 1: An example of a program that solves the problem of “compute the sum of the numbers from 1 to\nN”. The program on the left runs in O (N), whereas the program on the right runs in constant time. The\ngoal of PIE is to enable LLMs to perform these kinds of program optimizations.\nmization that addresses the key challenge of replicable performance measurement, and perform an extensive\nevaluation of a wide range of adaptation techniques based on it.\nFirst, we construct a dataset of Performance-Improving Edits (PIE). We collect C++ programs written to\nsolve competitive programming problems, where we track a single programmer’s submissions as they evolve\nover time, filtering for sequences of edits that correspond to performance improvements.\nNext, a major challenge is the significant variability of measuring performance on real hardware due to\nserver workload and configuration issues. Indeed, we find that benchmarking on real hardware can lead to\nlarge, phantom performance “improvements” due only to random chance. To address this challenge, we\nevaluate performance using the gem5 CPU simulator (Binkert et al., 2011), the gold standard CPU simulator\nin academia and industry, and models state-of-the-art general-purpose processors. This evaluation strategy\nis entirely deterministic, ensuring both reliability and reproducibility.\nBased on this benchmark, we evaluate a variety of techniques for adapting pre-trained code LLMs for per-\nformance optimization. First, we consider baseline prompting approaches, including techniques such as\nchain-of-thought (Wei et al., 2022b) (CoT). We find that LLMs are limited in the challenging task of code\noptimization. Without data-driven methods that leverage PIE, our strongest baseline COT only warrants\n1.61× average speedups vs. 4.06× human reference. Next we consider a retrieval-based prompting ap-\nproach where retrieval is used to select examples most similar to the current one (Liu et al., 2021; Poesia\net al., 2021). Lastly, we consider several finetuning strategies: these include using synthetic data generated\nvia self-play (Haluptzok et al., 2022), where synthetic training examples are generated by an LLM without\nthe need for direct human examples, as well as performance-conditioned generation, where we condition\ngeneration on the performance of the generated program.\nWe find that data-driven methods using PIE, like retrieval-based few-shot prompting and fine-tuning, are\nhighly effective at achieving strong optimization abilities in LLMs. When allowing a model to take 8']","Data-driven methods using PIE, such as retrieval-based few-shot prompting and fine-tuning, are highly effective for optimizing pre-trained code LLMs based on the PIE benchmark evaluation.",reasoning,"[{'Authors': 'Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh', 'Published': '2023-11-08', 'Summary': 'With the waning of Moore\'s law, optimizing program performance has become a\nmajor focus of software research. However, high-level optimizations such as API\nand algorithm changes remain elusive due to the difficulty of understanding the\nsemantics of code. Simultaneously, pretrained large language models (LLMs) have\ndemonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program\noptimization. First, we curate a dataset of performance-improving edits made by\nhuman programmers of over 77K competitive C++ programming submission pairs,\naccompanied by extensive unit tests. A major challenge is the significant\nvariability of measuring performance on commodity hardware, which can lead to\nspurious ""improvements"". To isolate and reliably evaluate the impact of program\noptimizations, we design an environment based on the gem5 full system\nsimulator, the de facto simulator used in academia and industry. Next, we\npropose a broad range of adaptation strategies for code optimization; for\nprompting, these include retrieval-based few-shot prompting and\nchain-of-thought, and for finetuning, these include performance-conditioned\ngeneration and synthetic data augmentation based on self-play. A combination of\nthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and\n6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our\nproposed performance-conditioned generation is particularly effective at\nimproving performance as well as increasing the fraction of optimized programs.', 'Title': 'Learning Performance-Improving Code Edits'}]",TRUE
"How do observation and action prompts, as well as normalization method, affect normalized walking time in LLMs?","['LLMs prompting a robot to walk.\nD. Observation and Action Prompt\nIn our subsequent investigation, we assess the influence\nof the observation and action prompt PHist on walking per-\nformance. Inspired by the RL-based walking control design,\nwe first study how historical observations and actions affect\nthe performance. We conduct a series of experiments, testing\nobservation and action lengths of 0, 10, 30, and 50, all while\nusing the description prompt. To clarify, a length of 0 means\nonly a description prompt. In our experiments, the LLM is\nqueried at 10 Hz, so a length of 50 means 5 seconds in\nwall time that covers several walking steps for a quadruped\nrobot. The experimental result is shown in Fig. 6. It is evident\nthat increased lengths of observations and actions correlate\nwith enhanced performance, both in terms of normalized\nwalking time and success rate. With lengths ranging from\n0 to 50, the LLM token consumptions are approximately\n348, 1738, 4518, and 7298, respectively. As we use the GPT-\n4 model with an 8k token length, we are not able to explore\nlonger lengths of observations and actions.\nIn addition to comparing various lengths for observation\nand action prompts, we also investigate the effect of dif-\nferent observation prompts. Our choices for observations\nare influenced by the RL policy, as we initialize our LLM\npolicy using a reinforcement learning-based approach. We\nevaluated five scenarios: (E1) no observation; (E2) only\nbase linear velocity and angular velocity; (E3) only joint\nE1\nE2\nE3\nE4\nE5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nValue\nNormalized Walking Time\nSuccess Rate\nFig. 7: Observation Choice Comparison. E1. No observa-\ntion. E2. Base linear velocity and angular velocity. E3. Joint\nposition and joint velocity. E4. Combine observations from\nexperiments 2 and 3. E5. Full observation.\nposition and joint velocity; (E4) a combination of base\nlinear velocity, angular velocity, joint position, and joint\nvelocity; (E5) full observation. The comparison result is\nshown in Fig. 7. The full observation listed in Fig. 3 achieves\nthe best performance. However, it remains unclear which\nspecific observation component is the most influential. It is\nnoteworthy that the observation in the LLM policy has a\ndimension of 33 while the observation space in the RL policy\nhas a dimension of 48, which indicates that the LLM policy\ncan use less information to make a robot walk compared to\nan RL policy.\nFurthermore, we study the effect of how we normalize\nthe observation and action prompt. We benchmark 5 dif-\nferent normalization methods: (E1) original values without\nany normalization; (E2) normalize to positive values; (E3)\nnormalize to integers; (E4) discard the decimal part and\nthen normalize the integer part to positive integer values;\n(E5) normalize to positive integer values. Due to the limited\ntoken size of GPT-4, we opt for a compact observation\nprompt consisting of base linear and angular velocities.'
 'LLMs prompting a robot to walk.\nD. Observation and Action Prompt\nIn our subsequent investigation, we assess the influence\nof the observation and action prompt PHist on walking per-\nformance. Inspired by the RL-based walking control design,\nwe first study how historical observations and actions affect\nthe performance. We conduct a series of experiments, testing\nobservation and action lengths of 0, 10, 30, and 50, all while\nusing the description prompt. To clarify, a length of 0 means\nonly a description prompt. In our experiments, the LLM is\nqueried at 10 Hz, so a length of 50 means 5 seconds in\nwall time that covers several walking steps for a quadruped\nrobot. The experimental result is shown in Fig. 6. It is evident\nthat increased lengths of observations and actions correlate\nwith enhanced performance, both in terms of normalized\nwalking time and success rate. With lengths ranging from\n0 to 50, the LLM token consumptions are approximately\n348, 1738, 4518, and 7298, respectively. As we use the GPT-\n4 model with an 8k token length, we are not able to explore\nlonger lengths of observations and actions.\nIn addition to comparing various lengths for observation\nand action prompts, we also investigate the effect of dif-\nferent observation prompts. Our choices for observations\nare influenced by the RL policy, as we initialize our LLM\npolicy using a reinforcement learning-based approach. We\nevaluated five scenarios: (E1) no observation; (E2) only\nbase linear velocity and angular velocity; (E3) only joint\nE1\nE2\nE3\nE4\nE5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nValue\nNormalized Walking Time\nSuccess Rate\nFig. 7: Observation Choice Comparison. E1. No observa-\ntion. E2. Base linear velocity and angular velocity. E3. Joint\nposition and joint velocity. E4. Combine observations from\nexperiments 2 and 3. E5. Full observation.\nposition and joint velocity; (E4) a combination of base\nlinear velocity, angular velocity, joint position, and joint\nvelocity; (E5) full observation. The comparison result is\nshown in Fig. 7. The full observation listed in Fig. 3 achieves\nthe best performance. However, it remains unclear which\nspecific observation component is the most influential. It is\nnoteworthy that the observation in the LLM policy has a\ndimension of 33 while the observation space in the RL policy\nhas a dimension of 48, which indicates that the LLM policy\ncan use less information to make a robot walk compared to\nan RL policy.\nFurthermore, we study the effect of how we normalize\nthe observation and action prompt. We benchmark 5 dif-\nferent normalization methods: (E1) original values without\nany normalization; (E2) normalize to positive values; (E3)\nnormalize to integers; (E4) discard the decimal part and\nthen normalize the integer part to positive integer values;\n(E5) normalize to positive integer values. Due to the limited\ntoken size of GPT-4, we opt for a compact observation\nprompt consisting of base linear and angular velocities.']","Increased lengths of observations and actions correlate with enhanced performance in terms of normalized walking time. Different observation prompts also have an effect on performance, with the full observation achieving the best results. It is unclear which specific observation component is the most influential. The normalization method used also affects performance, with different methods yielding different results.",multi_context,"[{'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}
 {'Authors': 'Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath', 'Published': '2023-11-17', 'Summary': 'Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .', 'Title': 'Prompt a Robot to Walk with Large Language Models'}]",TRUE
What determines an object's transparency?,"['Mass\n94.2\n58.8\nFragility\n93.6\n53.1\nDeformability\n90.5\n48.1\nMaterial\n93.7\n59.4\nTransparency\n97.0\n72.5\nContents\n90.4\n49.8\nCan Contain Liquid\n99.3\n64.2\nIs Sealed\n98.2\n74.7\nDensity (held-out)\n93.3\n50.7\nLiquid Capacity (held-out)\n89.1\n46.0\nTABLE XI: Agreement among crowd-workers per concept\ncrowd-sourced data is also the bounding box image presented\nfor annotation. For crowd-sourced validation data, we filter\nour data to only include examples with at least 2/3 majority\nagreement among annotators, and only use the majority label.\nWe do not apply this filtering for training data. For preference\npair annotations, we remove data annotated with unclear.\nDataset Balancing. We construct sub-datasets for dataset\nbalancing purposes. For the categorical concepts except is\nsealed, we combine the crowd-sourced and automatically\nannotated data for each concept into one sub-dataset per\nconcept. For the other concepts, we keep separate sub-\ndatasets for crowd-sourced and automatically annotated data.\nWe keep separate sub-datasets for is sealed because for its\ncrowd-sourced data, we only train using the bounding box\nimage for the object that was presented for annotation, rather\nthan randomly sampling one of its bounding box images\n(as described in the below sub-section), as values for this\nconcept may change for the same object instance. We keep\nseparate datasets for the continuous concepts because there\nis a large imbalance between the number of crowd-sourced\nand automatically annotated examples for these concepts. To\nbalance these sub-datasets, we sample from each of them\nduring training at a rate proportional to the square root of\nthe number of annotations in the sub-dataset, as proposed in\nInstructBLIP for instruction tuning.\nAdditional Training Details. For most objects, each time\nwe sample one for training, we randomly sample one of its\nbounding box images as input to the model, as a form of\ndata augmentation. We do not do this with crowd-sourced\ndata for the contents and is sealed concepts, because labels\nfor these concepts may vary across different images of the\nsame object. Instead, we only use the bounding box image\nthat was presented for annotation.\nTo promote robustness to different queries to the VLM, we\ninclude object category labels in the question prompt for half\nof the training examples (e.g., asking “Is this bottle heavy?”),\nHyperparameter\nValue\nMax fine-tuning steps\n10000\nWarmup steps\n1000\nLearning rate\n1e-5\nBatch size\n128\nAdamW β\n(0.9, 0.999)\nWeight decay\n0.05\nImage resolution\n224\nPrompt template\nQuestion: {} Respond unknown if you are not sure. Short answer:\nTABLE XII: Hyperparameters for fine-tuning InstructBLIP\nConcept\nQuestion Prompt\nMass\nIs this object heavy?\nFragility\nIs this object fragile?\nDeformability\nIs this object deformable?\nMaterial\nWhat material is this object made of?\nTransparency\nIs this object transparent, translucent, or opaque?\nContents\nWhat is inside this container?\nCan Contain Liquid'
 'Mass\n94.2\n58.8\nFragility\n93.6\n53.1\nDeformability\n90.5\n48.1\nMaterial\n93.7\n59.4\nTransparency\n97.0\n72.5\nContents\n90.4\n49.8\nCan Contain Liquid\n99.3\n64.2\nIs Sealed\n98.2\n74.7\nDensity (held-out)\n93.3\n50.7\nLiquid Capacity (held-out)\n89.1\n46.0\nTABLE XI: Agreement among crowd-workers per concept\ncrowd-sourced data is also the bounding box image presented\nfor annotation. For crowd-sourced validation data, we filter\nour data to only include examples with at least 2/3 majority\nagreement among annotators, and only use the majority label.\nWe do not apply this filtering for training data. For preference\npair annotations, we remove data annotated with unclear.\nDataset Balancing. We construct sub-datasets for dataset\nbalancing purposes. For the categorical concepts except is\nsealed, we combine the crowd-sourced and automatically\nannotated data for each concept into one sub-dataset per\nconcept. For the other concepts, we keep separate sub-\ndatasets for crowd-sourced and automatically annotated data.\nWe keep separate sub-datasets for is sealed because for its\ncrowd-sourced data, we only train using the bounding box\nimage for the object that was presented for annotation, rather\nthan randomly sampling one of its bounding box images\n(as described in the below sub-section), as values for this\nconcept may change for the same object instance. We keep\nseparate datasets for the continuous concepts because there\nis a large imbalance between the number of crowd-sourced\nand automatically annotated examples for these concepts. To\nbalance these sub-datasets, we sample from each of them\nduring training at a rate proportional to the square root of\nthe number of annotations in the sub-dataset, as proposed in\nInstructBLIP for instruction tuning.\nAdditional Training Details. For most objects, each time\nwe sample one for training, we randomly sample one of its\nbounding box images as input to the model, as a form of\ndata augmentation. We do not do this with crowd-sourced\ndata for the contents and is sealed concepts, because labels\nfor these concepts may vary across different images of the\nsame object. Instead, we only use the bounding box image\nthat was presented for annotation.\nTo promote robustness to different queries to the VLM, we\ninclude object category labels in the question prompt for half\nof the training examples (e.g., asking “Is this bottle heavy?”),\nHyperparameter\nValue\nMax fine-tuning steps\n10000\nWarmup steps\n1000\nLearning rate\n1e-5\nBatch size\n128\nAdamW β\n(0.9, 0.999)\nWeight decay\n0.05\nImage resolution\n224\nPrompt template\nQuestion: {} Respond unknown if you are not sure. Short answer:\nTABLE XII: Hyperparameters for fine-tuning InstructBLIP\nConcept\nQuestion Prompt\nMass\nIs this object heavy?\nFragility\nIs this object fragile?\nDeformability\nIs this object deformable?\nMaterial\nWhat material is this object made of?\nTransparency\nIs this object transparent, translucent, or opaque?\nContents\nWhat is inside this container?\nCan Contain Liquid']","Transparency is determined by whether an object is transparent, translucent, or opaque.",multi_context,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}
 {'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
How does fine-tuning a vision-language model using the PHYSOBJECTS dataset enhance its understanding of physical object concepts and its impact on planning performance in robotic manipulation tasks?,"['Physically Grounded Vision-Language Models for Robotic Manipulation\nJensen Gao1, Bidipta Sarkar1, Fei Xia2, Ted Xiao2, Jiajun Wu1,\nBrian Ichter2, Anirudha Majumdar2,3, Dorsa Sadigh1,2\nAbstract— Recent\nadvances\nin\nvision-language\nmodels\n(VLMs) have led to improved performance on tasks such as\nvisual question answering and image captioning. Consequently,\nthese models are now well-positioned to reason about the\nphysical world, particularly within domains such as robotic\nmanipulation. However, current VLMs are limited in their\nunderstanding of the physical concepts (e.g., material, fragility)\nof common objects, which restricts their usefulness for robotic\nmanipulation tasks that involve interaction and physical reason-\ning about such objects. To address this limitation, we propose\nPHYSOBJECTS, an object-centric dataset of 39.6K crowd-\nsourced and 417K automated physical concept annotations of\ncommon household objects. We demonstrate that fine-tuning a\nVLM on PHYSOBJECTS improves its understanding of physical\nobject concepts, including generalization to held-out concepts,\nby capturing human priors of these concepts from visual\nappearance. We incorporate this physically grounded VLM in\nan interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on\ntasks that require reasoning about physical object concepts,\ncompared to baselines that do not leverage physically grounded\nVLMs. We additionally illustrate the benefits of our physically\ngrounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and\nvisualizations of our results at https://iliad.stanford.\nedu/pg-vlm/.\nI. INTRODUCTION\nLarge language models (LLMs) have shown great promise\nfor converting language instructions into task plans for em-\nbodied agents [1], [2]. The fundamental challenge in apply-\ning LLMs for this is grounding them to the physical world,\nthrough sensory input such as vision. Prior work has made\nprogress towards grounding LLMs by using vision-language\nmodels (VLMs) to indicate the presence of objects in a\nscene, or to provide feedback about occurrences in a scene\n[3]–[7]. However, vision could be used to further improve\ngrounding by extracting more detailed scene information.\nFor robotic manipulation, understanding physical concepts of\nobjects, such as their material composition or their fragility,\nwould help planners identify relevant objects to interact with,\nand affordances based on physical or safety constraints. For\nexample, if a human wants a robot to get a cup of water,\nthe robot should be able to determine if a cup already has\nwater or something else in it. Also, the robot should handle\nthe cup with greater caution if it is more fragile.\nHow can we use vision to reason about physical object\nconcepts? Prior work has studied this problem using more\ntraditional vision techniques, such as self-supervised learning']",Fine-tuning a vision-language model using the PHYSOBJECTS dataset enhances its understanding of physical object concepts and improves planning performance in robotic manipulation tasks.,reasoning,"[{'Authors': 'Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh', 'Published': '2024-03-03', 'Summary': 'Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.', 'Title': 'Physically Grounded Vision-Language Models for Robotic Manipulation'}]",TRUE
